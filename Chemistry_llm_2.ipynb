{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFJcXpvVhZTMnxxVE31o7j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6606e89833a548b4b4712150cfb267d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21205e6078044238b517b5cea97209eb",
              "IPY_MODEL_7bcc063660c2489d982e93773367ceb0",
              "IPY_MODEL_73c6105e817848b78c17aec2d367ef23"
            ],
            "layout": "IPY_MODEL_30656074d593401bb5e0680c56091a3b"
          }
        },
        "21205e6078044238b517b5cea97209eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d78192edee4adab3a86bcb7c45107e",
            "placeholder": "​",
            "style": "IPY_MODEL_545ecc7e3bcf4c308cd04f632b4cb682",
            "value": "data.zip: 100%"
          }
        },
        "7bcc063660c2489d982e93773367ceb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_897e62db6cac41ef824ffd8dfc4346cf",
            "max": 692308728,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5c2f7a893da49c6a46cb107f3d715ce",
            "value": 692308728
          }
        },
        "73c6105e817848b78c17aec2d367ef23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1748307876774f5a955fe7df0034457b",
            "placeholder": "​",
            "style": "IPY_MODEL_f8d9535c7b5849809db80347c37012ff",
            "value": " 692M/692M [00:10&lt;00:00, 175MB/s]"
          }
        },
        "30656074d593401bb5e0680c56091a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d78192edee4adab3a86bcb7c45107e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "545ecc7e3bcf4c308cd04f632b4cb682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "897e62db6cac41ef824ffd8dfc4346cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5c2f7a893da49c6a46cb107f3d715ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1748307876774f5a955fe7df0034457b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d9535c7b5849809db80347c37012ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwardtatem38-pixel/DIMITRI-Chem-Transformer/blob/main/Chemistry_llm_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q75k-yyYJjXm",
        "outputId": "301bea09-8d80-44ac-de48-24d4046c44c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting selfies\n",
            "  Downloading selfies-2.2.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selfies-2.2.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: selfies, rdkit\n",
            "Successfully installed rdkit-2025.9.3 selfies-2.2.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2026.1.2-py3-none-any.whl.metadata (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Collecting unsloth_zoo>=2026.1.2 (from unsloth)\n",
            "  Downloading unsloth_zoo-2026.1.2-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.24.0+cu126)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-1.0.5-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
            "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.5.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.18.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
            "Collecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth)\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Collecting torchao>=0.13.0 (from unsloth_zoo>=2026.1.2->unsloth)\n",
            "  Downloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2026.1.2->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.2->unsloth) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2026.1.2->unsloth)\n",
            "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting torch>=2.0.0 (from accelerate)\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cufft-cu12==11.3.0.4->torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth)\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from unsloth)\n",
            "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading unsloth-2026.1.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.1/381.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2026.1.2-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m898.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-1.0.5-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchao, triton, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, tyro, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: torchao\n",
            "    Found existing installation: torchao 0.10.0\n",
            "    Uninstalling torchao-0.10.0:\n",
            "      Successfully uninstalled torchao-0.10.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.49.1 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.20.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pyarrow-22.0.0 torch-2.9.1 torchao-0.15.0 torchvision-0.24.1 triton-3.5.1 trl-0.24.0 tyro-1.0.5 unsloth-2026.1.2 unsloth_zoo-2026.1.2 xformers-0.0.33.post2\n"
          ]
        }
      ],
      "source": [
        "# chem information core\n",
        "!pip install rdkit selfies\n",
        "\n",
        "# LLM frame for fine tuning\n",
        "!pip install transformers datasets accelerate unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLOps & Data Versioning\n",
        "!pip install wandb dvc deepchecks[nlp]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3zoXE5VLK4t",
        "outputId": "33fb833c-b66c-4f34-9c79-ddf7bf5497e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Collecting dvc\n",
            "  Downloading dvc-3.66.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting deepchecks[nlp]\n",
            "  Downloading deepchecks-0.19.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.46)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.49.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from dvc) (25.4.0)\n",
            "Collecting celery (from dvc)\n",
            "  Downloading celery-5.6.2-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting colorama>=0.3.9 (from dvc)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting configobj>=5.0.9 (from dvc)\n",
            "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: distro>=1.3 in /usr/local/lib/python3.12/dist-packages (from dvc) (1.9.0)\n",
            "Collecting dpath<3,>=2.1.0 (from dvc)\n",
            "  Downloading dpath-2.2.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting dulwich (from dvc)\n",
            "  Downloading dulwich-0.25.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting dvc-data<3.19.0,>=3.18.0 (from dvc)\n",
            "  Downloading dvc_data-3.18.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting dvc-http>=2.29.0 (from dvc)\n",
            "  Downloading dvc_http-2.32.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting dvc-objects (from dvc)\n",
            "  Downloading dvc_objects-5.2.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting dvc-render<2,>=1.0.1 (from dvc)\n",
            "  Downloading dvc_render-1.0.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dvc-studio-client<1,>=0.21 (from dvc)\n",
            "  Downloading dvc_studio_client-0.22.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting dvc-task<1,>=0.3.0 (from dvc)\n",
            "  Downloading dvc_task-0.40.2-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting flatten-dict<1,>=0.4.1 (from dvc)\n",
            "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting flufl.lock<10,>=8.1.0 (from dvc)\n",
            "  Downloading flufl_lock-9.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: fsspec>=2024.2.0 in /usr/local/lib/python3.12/dist-packages (from dvc) (2025.3.0)\n",
            "Collecting funcy>=1.14 (from dvc)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting grandalf<1,>=0.7 (from dvc)\n",
            "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting gto<2,>=1.6.0 (from dvc)\n",
            "  Downloading gto-1.9.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting hydra-core>=1.1 (from dvc)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting iterative-telemetry>=0.0.7 (from dvc)\n",
            "  Downloading iterative_telemetry-0.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting kombu (from dvc)\n",
            "  Downloading kombu-5.6.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.12/dist-packages (from dvc) (3.6.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from dvc) (2.3.0)\n",
            "Collecting pathspec<1,>=0.10.3 (from dvc)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: psutil>=5.8 in /usr/local/lib/python3.12/dist-packages (from dvc) (5.9.5)\n",
            "Requirement already satisfied: pydot>=1.2.4 in /usr/local/lib/python3.12/dist-packages (from dvc) (4.0.1)\n",
            "Collecting pygtrie>=2.3.2 (from dvc)\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.12/dist-packages (from dvc) (3.3.1)\n",
            "Requirement already satisfied: rich>=12 in /usr/local/lib/python3.12/dist-packages (from dvc) (13.9.4)\n",
            "Collecting ruamel.yaml>=0.17.11 (from dvc)\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting scmrepo<4,>=3.5.2 (from dvc)\n",
            "  Downloading scmrepo-3.6.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting shortuuid>=0.5 (from dvc)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting shtab<2,>=1.3.4 (from dvc)\n",
            "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.12/dist-packages (from dvc) (0.9.0)\n",
            "Requirement already satisfied: tomlkit>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from dvc) (0.13.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.63.1 in /usr/local/lib/python3.12/dist-packages (from dvc) (4.67.1)\n",
            "Collecting voluptuous>=0.11.7 (from dvc)\n",
            "  Downloading voluptuous-0.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting zc.lockfile>=1.2.1 (from dvc)\n",
            "  Downloading zc_lockfile-4.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (1.6.1)\n",
            "Requirement already satisfied: jsonpickle>=2 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (4.1.1)\n",
            "Collecting PyNomaly>=0.3.3 (from deepchecks[nlp])\n",
            "  Downloading PyNomaly-0.3.4-py3-none-any.whl.metadata (581 bytes)\n",
            "Collecting category-encoders>=2.3.0 (from deepchecks[nlp])\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (1.16.3)\n",
            "Requirement already satisfied: plotly>=5.13.1 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (5.24.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (3.10.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (4.13.5)\n",
            "Requirement already satisfied: statsmodels>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (0.14.6)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (2.0.2)\n",
            "Requirement already satisfied: ipython>=7.15.0 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (7.34.0)\n",
            "Requirement already satisfied: ipykernel>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets>=7.6.5 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (7.7.1)\n",
            "Requirement already satisfied: jupyter-server>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (2.14.0)\n",
            "Collecting seqeval>=1.0.0 (from deepchecks[nlp])\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: textblob>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (0.19.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (0.5.9.post2)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (4.57.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (0.36.0)\n",
            "Requirement already satisfied: sentence-transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (5.2.0)\n",
            "Requirement already satisfied: nltk>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (3.9.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from deepchecks[nlp]) (0.12.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->deepchecks[nlp]) (2.8.1)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category-encoders>=2.3.0->deepchecks[nlp]) (1.0.2)\n",
            "Collecting dictdiffer>=0.8.1 (from dvc-data<3.19.0,>=3.18.0->dvc)\n",
            "  Downloading dictdiffer-0.9.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting diskcache>=5.2.1 (from dvc-data<3.19.0,>=3.18.0->dvc)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting sqltrie<1,>=0.11.0 (from dvc-data<3.19.0,>=3.18.0->dvc)\n",
            "  Downloading sqltrie-0.11.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: orjson<4,>=3 in /usr/local/lib/python3.12/dist-packages (from dvc-data<3.19.0,>=3.18.0->dvc) (3.11.5)\n",
            "Collecting aiohttp-retry>=2.5.0 (from dvc-http>=2.29.0->dvc)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting billiard<5.0,>=4.2.1 (from celery->dvc)\n",
            "  Downloading billiard-4.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting vine<6.0,>=5.1.0 (from celery->dvc)\n",
            "  Downloading vine-5.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting click-didyoumean>=0.3.0 (from celery->dvc)\n",
            "  Downloading click_didyoumean-0.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting click-repl>=0.2.0 (from celery->dvc)\n",
            "  Downloading click_repl-0.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: click-plugins>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from celery->dvc) (1.1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from celery->dvc) (2.9.0.post0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from celery->dvc) (5.3.1)\n",
            "Requirement already satisfied: six<2.0,>=1.12 in /usr/local/lib/python3.12/dist-packages (from flatten-dict<1,>=0.4.1->dvc) (1.17.0)\n",
            "Requirement already satisfied: atpublic in /usr/local/lib/python3.12/dist-packages (from flufl.lock<10,>=8.1.0->dvc) (5.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from gto<2,>=1.6.0->dvc) (0.4)\n",
            "Requirement already satisfied: pydantic-settings>=2 in /usr/local/lib/python3.12/dist-packages (from gto<2,>=1.6.0->dvc) (2.12.0)\n",
            "Collecting semver>=2.13.0 (from gto<2,>=1.6.0->dvc)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gto<2,>=1.6.0->dvc) (0.21.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->dvc) (4.9.3)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.3.0->deepchecks[nlp]) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.3.0->deepchecks[nlp]) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.3.0->deepchecks[nlp]) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.3.0->deepchecks[nlp]) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.3.0->deepchecks[nlp]) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.3.0->deepchecks[nlp]) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.3.0->deepchecks[nlp]) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.15.0->deepchecks[nlp]) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.15.0->deepchecks[nlp])\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.15.0->deepchecks[nlp]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.15.0->deepchecks[nlp]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.15.0->deepchecks[nlp]) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.15.0->deepchecks[nlp]) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.15.0->deepchecks[nlp]) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.15.0->deepchecks[nlp]) (4.9.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.6.5->deepchecks[nlp]) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.6.5->deepchecks[nlp]) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.6.5->deepchecks[nlp]) (3.0.16)\n",
            "Collecting appdirs (from iterative-telemetry>=0.0.7->dvc)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from iterative-telemetry>=0.0.7->dvc) (3.20.2)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (4.12.1)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (25.1.0)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (5.9.1)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (0.5.3)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (7.16.6)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (0.23.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (2.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server>=2.7.2->deepchecks[nlp]) (1.9.0)\n",
            "Collecting amqp<6.0.0,>=5.1.1 (from kombu->dvc)\n",
            "  Downloading amqp-5.3.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: tzdata>=2025.2 in /usr/local/lib/python3.12/dist-packages (from kombu->dvc) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->deepchecks[nlp]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->deepchecks[nlp]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->deepchecks[nlp]) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->deepchecks[nlp]) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->deepchecks[nlp]) (11.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.8.1->deepchecks[nlp]) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.8.1->deepchecks[nlp]) (2025.11.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->deepchecks[nlp]) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.13.1->deepchecks[nlp]) (9.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: python-utils in /usr/local/lib/python3.12/dist-packages (from PyNomaly>=0.3.3->deepchecks[nlp]) (3.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2026.1.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12->dvc) (4.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.23.2->deepchecks[nlp]) (3.6.0)\n",
            "Requirement already satisfied: pygit2>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from scmrepo<4,>=3.5.2->dvc) (1.19.1)\n",
            "Collecting asyncssh<3,>=2.13.1 (from scmrepo<4,>=3.5.2->dvc)\n",
            "  Downloading asyncssh-2.22.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=3.0.0->deepchecks[nlp]) (2.9.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->deepchecks[nlp]) (1.2.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.0.0->deepchecks[nlp]) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.0.0->deepchecks[nlp]) (0.7.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn->deepchecks[nlp]) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn->deepchecks[nlp]) (0.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc) (3.13.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi>=21.1->jupyter-server>=2.7.2->deepchecks[nlp]) (25.1.0)\n",
            "Requirement already satisfied: cryptography>=39.0 in /usr/local/lib/python3.12/dist-packages (from asyncssh<3,>=2.13.1->scmrepo<4,>=3.5.2->dvc) (43.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.15.0->deepchecks[nlp]) (0.8.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.0.3->jupyter-server>=2.7.2->deepchecks[nlp]) (3.0.3)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (4.26.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (4.0.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (0.37.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (0.1.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12->dvc) (0.1.2)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks[nlp]) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks[nlp]) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks[nlp]) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks[nlp]) (3.2.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks[nlp]) (0.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks[nlp]) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.3.0->jupyter-server>=2.7.2->deepchecks[nlp]) (2.21.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.2->umap-learn->deepchecks[nlp]) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.15.0->deepchecks[nlp]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.15.0->deepchecks[nlp]) (0.2.14)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2->gto<2,>=1.6.0->dvc) (1.2.1)\n",
            "Requirement already satisfied: cffi>=2.0 in /usr/local/lib/python3.12/dist-packages (from pygit2>=1.14.0->scmrepo<4,>=3.5.2->dvc) (2.0.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (3.5.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.4.1->gto<2,>=1.6.0->dvc) (1.5.4)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->deepchecks[nlp]) (6.5.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc) (1.22.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks[nlp]) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks[nlp]) (1.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0->pygit2>=1.14.0->scmrepo<4,>=3.5.2->dvc) (2.23)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (2025.9.1)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (0.30.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (25.10.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->deepchecks[nlp]) (1.3.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=3.0.0->deepchecks[nlp]) (1.3.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->deepchecks[nlp]) (0.2.4)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=2.7.2->deepchecks[nlp]) (1.4.0)\n",
            "Downloading dvc-3.66.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.7/469.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Downloading dpath-2.2.0-py3-none-any.whl (17 kB)\n",
            "Downloading dvc_data-3.18.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dvc_http-2.32.0-py3-none-any.whl (12 kB)\n",
            "Downloading dvc_objects-5.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading dvc_render-1.0.2-py3-none-any.whl (22 kB)\n",
            "Downloading dvc_studio_client-0.22.0-py3-none-any.whl (16 kB)\n",
            "Downloading dvc_task-0.40.2-py3-none-any.whl (21 kB)\n",
            "Downloading celery-5.6.2-py3-none-any.whl (445 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.5/445.5 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
            "Downloading flufl_lock-9.0.0-py3-none-any.whl (11 kB)\n",
            "Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gto-1.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterative_telemetry-0.0.10-py3-none-any.whl (10 kB)\n",
            "Downloading kombu-5.6.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.2/214.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vine-5.1.0-py3-none-any.whl (9.6 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Downloading PyNomaly-0.3.4-py3-none-any.whl (9.1 kB)\n",
            "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scmrepo-3.6.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.1/74.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.25.2-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
            "Downloading voluptuous-0.16.0-py3-none-any.whl (31 kB)\n",
            "Downloading zc_lockfile-4.0-py3-none-any.whl (9.1 kB)\n",
            "Downloading deepchecks-0.19.1-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading amqp-5.3.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncssh-2.22.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading billiard-4.2.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.1/87.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_didyoumean-0.3.1-py3-none-any.whl (3.6 kB)\n",
            "Downloading click_repl-0.3.0-py3-none-any.whl (10 kB)\n",
            "Downloading dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading sqltrie-0.11.2-py3-none-any.whl (17 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=46ad3363c376f6decc09905e4a9cb566364fcf0e875afc84972c0dba6d989538\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: pygtrie, funcy, dictdiffer, appdirs, zc.lockfile, voluptuous, vine, sqltrie, shtab, shortuuid, semver, ruamel.yaml, pathspec, jedi, grandalf, flufl.lock, flatten-dict, dvc-render, dvc-objects, dulwich, dpath, diskcache, configobj, colorama, click-didyoumean, billiard, PyNomaly, iterative-telemetry, hydra-core, dvc-studio-client, dvc-data, click-repl, amqp, seqeval, kombu, asyncssh, aiohttp-retry, scmrepo, dvc-http, celery, category-encoders, gto, dvc-task, dvc, deepchecks\n",
            "Successfully installed PyNomaly-0.3.4 aiohttp-retry-2.9.1 amqp-5.3.1 appdirs-1.4.4 asyncssh-2.22.0 billiard-4.2.4 category-encoders-2.9.0 celery-5.6.2 click-didyoumean-0.3.1 click-repl-0.3.0 colorama-0.4.6 configobj-5.0.9 deepchecks-0.19.1 dictdiffer-0.9.0 diskcache-5.6.3 dpath-2.2.0 dulwich-0.25.2 dvc-3.66.1 dvc-data-3.18.2 dvc-http-2.32.0 dvc-objects-5.2.0 dvc-render-1.0.2 dvc-studio-client-0.22.0 dvc-task-0.40.2 flatten-dict-0.4.2 flufl.lock-9.0.0 funcy-2.0 grandalf-0.8 gto-1.9.0 hydra-core-1.3.2 iterative-telemetry-0.0.10 jedi-0.19.2 kombu-5.6.2 pathspec-0.12.1 pygtrie-2.5.0 ruamel.yaml-0.19.1 scmrepo-3.6.1 semver-3.0.4 seqeval-1.2.2 shortuuid-1.0.13 shtab-1.8.0 sqltrie-0.11.2 vine-5.1.0 voluptuous-0.16.0 zc.lockfile-4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MJ2wjPXra4zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This creates a requirements file for your future MLOps pipeline\n",
        "requirements = \"\"\"\n",
        "rdkit\n",
        "selfies\n",
        "transformers\n",
        "datasets\n",
        "accelerate\n",
        "wandb\n",
        "dvc\n",
        "\"\"\"\n",
        "\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)\n",
        "\n",
        "print(\"requirements.txt created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-2N7aG0Lg58",
        "outputId": "330bbd6e-d383-4b29-e2d3-8d25ac9cadcc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requirements.txt created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rdkit\n",
        "import selfies as sf\n",
        "from rdkit import Chem\n",
        "\n",
        "print(f\"RDKit Version: {rdkit.__version__}\")\n",
        "print(f\"SELFIES Version: {sf.__version__}\")\n",
        "\n",
        "# quick test for Benzene\n",
        "smiles = \"c1ccccc1\"\n",
        "encoded = sf.encoder(smiles)\n",
        "print(f\"Benzene in SELFIES: {encoded}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz1SafVWLpa1",
        "outputId": "b8b66f40-651d-4dbb-a14d-481b73dd1c3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDKit Version: 2025.09.3\n",
            "SELFIES Version: 2.1.1\n",
            "Benzene in SELFIES: [C][=C][C][=C][C][=C][Ring1][=Branch1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "import zipfile\n",
        "\n",
        "# 1. Clean up any previous broken files\n",
        "broken_file = \"data.zip\"\n",
        "if os.path.exists(broken_file):\n",
        "    os.remove(broken_file)\n",
        "    print(\"🗑️ Removed broken 135MB file.\")\n",
        "\n",
        "print(\"🚀 Starting fresh download (692MB). This may take 2-5 minutes...\")\n",
        "\n",
        "try:\n",
        "    # 2. Download directly to the current directory\n",
        "    path_to_zip = hf_hub_download(\n",
        "        repo_id=\"osunlp/SMolInstruct\",\n",
        "        filename=\"data.zip\",\n",
        "        repo_type=\"dataset\",\n",
        "        local_dir=\".\",\n",
        "        local_dir_use_symlinks=False  # Crucial for Colab storage\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Download complete: {os.path.getsize(path_to_zip) / (1024*1024):.2f} MB\")\n",
        "\n",
        "    # 3. Unzip to the target folder\n",
        "    extract_target = '/content/chemistry_data_raw'\n",
        "    with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_target)\n",
        "\n",
        "    print(f\"🎊 SUCCESS! Data extracted to: {extract_target}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "6606e89833a548b4b4712150cfb267d9",
            "21205e6078044238b517b5cea97209eb",
            "7bcc063660c2489d982e93773367ceb0",
            "73c6105e817848b78c17aec2d367ef23",
            "30656074d593401bb5e0680c56091a3b",
            "44d78192edee4adab3a86bcb7c45107e",
            "545ecc7e3bcf4c308cd04f632b4cb682",
            "897e62db6cac41ef824ffd8dfc4346cf",
            "f5c2f7a893da49c6a46cb107f3d715ce",
            "1748307876774f5a955fe7df0034457b",
            "f8d9535c7b5849809db80347c37012ff"
          ]
        },
        "id": "ic-cg0B4gfI7",
        "outputId": "d9424cde-f7b6-4f71-b462-d45a4ba9c2ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting fresh download (692MB). This may take 2-5 minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data.zip:   0%|          | 0.00/692M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6606e89833a548b4b4712150cfb267d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Download complete: 660.24 MB\n",
            "🎊 SUCCESS! Data extracted to: /content/chemistry_data_raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_path = \"/content/chemistry_data_raw\"\n",
        "\n",
        "# see directory where actual data files are\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    # only print directories that actually contain files to keep it clean\n",
        "    if files:\n",
        "        # get relative path from base\n",
        "        rel_path = os.path.relpath(root, base_path)\n",
        "        print(f\"📁 Folder: {rel_path} | Files found: {len(files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5fMPdO_pY8W",
        "outputId": "46f217bc-5504-4229-ef7e-a80a0c255615"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Folder: raw/train | Files found: 14\n",
            "📁 Folder: raw/test | Files found: 14\n",
            "📁 Folder: raw/dev | Files found: 14\n",
            "📁 Folder: template/instruction_tuning | Files found: 14\n",
            "📁 Folder: raw_selfies/train | Files found: 14\n",
            "📁 Folder: raw_selfies/test | Files found: 14\n",
            "📁 Folder: raw_selfies/dev | Files found: 14\n",
            "📁 Folder: sample/instruction_tuning/train | Files found: 14\n",
            "📁 Folder: sample/instruction_tuning/test | Files found: 14\n",
            "📁 Folder: sample/instruction_tuning/dev | Files found: 14\n",
            "📁 Folder: sample/instruction_tuning/test_subset | Files found: 14\n",
            "📁 Folder: core_tag | Files found: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "raw/train Contains original SMILES molecules\n",
        "\n",
        "raw_selfies/train Contains exact same molecules already converted to SELFIES.\n",
        "\n",
        "samples/instruction_tuning Contains the 'Q&A' style data what is the weight of the molecule?\n",
        "\n",
        "core_tag likely metadata or special tokens used for the SMILES tags mentioned in the README"
      ],
      "metadata": {
        "id": "UvwvGrcNquVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "selfies_train_path = '/content/chemistry_data_raw/raw_selfies/train'\n",
        "files = os.listdir(selfies_train_path)\n",
        "\n",
        "print(\"--- SELFIES Task Files Found ---\")\n",
        "for f in sorted(files):\n",
        "    print(f\"📄 {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoFAuLNSsEif",
        "outputId": "9d8aef70-84b8-402c-f791-2d303a6266b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SELFIES Task Files Found ---\n",
            "📄 forward_synthesis.jsonl\n",
            "📄 molecule_captioning.jsonl\n",
            "📄 molecule_generation.jsonl\n",
            "📄 name_conversion-i2f.jsonl\n",
            "📄 name_conversion-i2s.jsonl\n",
            "📄 name_conversion-s2f.jsonl\n",
            "📄 name_conversion-s2i.jsonl\n",
            "📄 property_prediction-bbbp.jsonl\n",
            "📄 property_prediction-clintox.jsonl\n",
            "📄 property_prediction-esol.jsonl\n",
            "📄 property_prediction-hiv.jsonl\n",
            "📄 property_prediction-lipo.jsonl\n",
            "📄 property_prediction-sider.jsonl\n",
            "📄 retrosynthesis.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m9I8Bi8MtQCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Synthesis and Retrosynthesis ,\n",
        "\n",
        "Data Inspection: Read Textbooks (.json1files) to see how the \"sentences\" are written.\n"
      ],
      "metadata": {
        "id": "1IZfgUUJtbej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-egTgFFuZB2",
        "outputId": "4196d941-45f0-40f1-8428-aa03d11a7d6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# path setting\n",
        "# point to specific reaction files i want to work with\n",
        "selfies_base = '/content/chemistry_data_raw/raw_selfies/train'\n",
        "forward_file = os.path.join(selfies_base, 'forward_synthesis.jsonl')\n",
        "\n",
        "# loading raw instructions\n",
        "# load JSONL file 'lines=True' is used because each line is seperate JSON object\n",
        "df_forward = pd.read_json(forward_file, lines=True, nrows=5)\n",
        "\n",
        "# instructions and data parsing\n",
        "# loop goes through the first few rows so we can see the Logic\n",
        "\n",
        "print(\"🔬 MLOPS DATA INSPECTION: REACTION LOGIC\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for index, row in df_forward.iterrows():\n",
        "    print(f\"\\n[SAMPLE #{index+1}]\")\n",
        "\n",
        "    # In this file, 'input' is the reactants\n",
        "    print(f\"🧪 REACTANTS (Input): {row['input'][:100]}...\")\n",
        "\n",
        "    # In this file, 'output' is the product\n",
        "    print(f\"🎯 PRODUCT (Output):  {row['output']}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"\\n✅ Observation: These reaction files are 'Pure Chemistry' (no English instructions).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Zl7ElpuXcu",
        "outputId": "c9ca3842-b6f4-40bf-d824-ec7565b271ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔬 MLOPS DATA INSPECTION: REACTION LOGIC\n",
            "==================================================\n",
            "\n",
            "[SAMPLE #1]\n",
            "🧪 REACTANTS (Input): [C][C][C][O][C][Ring1][Branch1].[C][C][N][Branch1][Ring1][C][C][C][C].[C][S][=Branch1][C][=O][=Branc...\n",
            "🎯 PRODUCT (Output):  [C][S][=Branch1][C][=O][=Branch1][C][=O][N][C@@H1][C][C][=C][C][=C][Branch2][Ring1][#Branch1][C][N][C][=C][Branch1][Ring1][C][O][C][Branch1][=Branch2][C][Branch1][C][F][Branch1][C][F][F][=N][Ring1][O][C][=C][Ring2][Ring1][C][C][Ring2][Ring1][Branch1]\n",
            "------------------------------\n",
            "\n",
            "[SAMPLE #2]\n",
            "🧪 REACTANTS (Input): [C][C][C][O][C][Ring1][Branch1].[C][C][N][Branch1][Ring1][C][C][C][C].[C][S][=Branch1][C][=O][=Branc...\n",
            "🎯 PRODUCT (Output):  [C][S][=Branch1][C][=O][=Branch1][C][=O][N][C@H1][C][C][=C][C][=C][Branch2][Ring1][#Branch1][C][N][C][=C][Branch1][Ring1][C][O][C][Branch1][=Branch2][C][Branch1][C][F][Branch1][C][F][F][=N][Ring1][O][C][=C][Ring2][Ring1][C][C][Ring2][Ring1][Branch1]\n",
            "------------------------------\n",
            "\n",
            "[SAMPLE #3]\n",
            "🧪 REACTANTS (Input): [C][O][C][=C][C][Branch2][Ring1][=N][/C][=C][/C][=Branch1][C][=O][C][C][=Branch1][C][=O][/C][=C][/C]...\n",
            "🎯 PRODUCT (Output):  [C][O][C][=C][C][=C][Branch1][C][O][C][=C][Ring1][#Branch1][C][C][C][=Branch1][C][=O][C][C][=Branch1][C][=O][C][C][C][=C][C][Branch1][C][O][=C][C][=C][Ring1][#Branch1][O][C]\n",
            "------------------------------\n",
            "\n",
            "[SAMPLE #4]\n",
            "🧪 REACTANTS (Input): [C][C][SiH1][Branch1][Ring1][C][C][C][C].[O][=C][Branch1][C][O][C][Branch1][C][F][Branch1][C][F][F]....\n",
            "🎯 PRODUCT (Output):  [O][=C][N][C][=C][C][=C][C][Branch1][C][Br][=C][Ring1][#Branch1][C][Ring1][#Branch2][C][O][C][=C][C][=C][Branch1][#Branch1][C][=C][Ring1][=Branch1][Ring1][=Branch2][O][C][C][O][Ring1][Branch2]\n",
            "------------------------------\n",
            "\n",
            "[SAMPLE #5]\n",
            "🧪 REACTANTS (Input): [C][C][=Branch1][C][=O][O-1].[C][C][Branch1][C][C][Branch1][C][C][O][C][=Branch1][C][=O][C][=C][C][=...\n",
            "🎯 PRODUCT (Output):  [C][C][=C][C][=C][C][Branch2][Ring2][Ring1][N][C][=C][C][Branch1][O][C][C][C][=C][C][=C][C][=C][Ring1][=Branch1][=C][C][=C][Ring1][=C][C][=Branch1][C][=O][O][C][Branch1][C][C][Branch1][C][C][C][=C][Ring2][Ring1][N]\n",
            "------------------------------\n",
            "\n",
            "✅ Observation: These reaction files are 'Pure Chemistry' (no English instructions).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import selfies as sf\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- STEP 1: DEFINE DATA SOURCES ---\n",
        "paths = [\n",
        "    '/content/chemistry_data_raw/raw_selfies/train/forward_synthesis.jsonl',\n",
        "    '/content/chemistry_data_raw/raw_selfies/train/retrosynthesis.jsonl'\n",
        "]\n",
        "\n",
        "# --- STEP 2: INITIALIZE THE VOCABULARY ---\n",
        "unique_symbols = set()\n",
        "unique_symbols.add('.')\n",
        "\n",
        "print(\"⚒️ Building Vocabulary from Reaction Logic...\")\n",
        "\n",
        "# --- STEP 3: SCANNING THE DATA ---\n",
        "# This line creates the variable 'path' for the first time!\n",
        "for path in paths:\n",
        "\n",
        "    # Everything below this line MUST be indented (shifted right)\n",
        "    # so Python knows to do it for EVERY path in our list.\n",
        "    df = pd.read_json(path, lines=True, nrows=5000)\n",
        "\n",
        "    for col in ['input', 'output']:\n",
        "        # This nested loop scans every molecule in the column\n",
        "        for sequence in tqdm(df[col], desc=f\"Scanning {os.path.basename(path)}\"):\n",
        "            try:\n",
        "                # Break the SELFIES string into individual [tokens]\n",
        "                symbols = list(sf.split_selfies(sequence))\n",
        "                unique_symbols.update(symbols)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "# --- STEP 4: FINALIZING THE ALPHABET ---\n",
        "# This part is NOT indented, so it only runs ONCE after the loops finish.\n",
        "alphabet = sorted(list(unique_symbols))\n",
        "\n",
        "print(f\"\\n✅ Total Unique Chemical Symbols Found: {len(alphabet)}\")\n",
        "print(f\"Top 20 Symbols: {alphabet[:20]}\")\n",
        "\n",
        "# --- STEP 5: SAVING THE ARTIFACT ---\n",
        "with open(\"reaction_alphabet.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(alphabet))\n",
        "\n",
        "print(\"\\n🚀 Alphabet saved to 'reaction_alphabet.txt'!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhIlNSN1BMrT",
        "outputId": "cbe7914c-2764-4d67-baee-7a69d0fc1612"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚒️ Building Vocabulary from Reaction Logic...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning forward_synthesis.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 15585.48it/s]\n",
            "Scanning forward_synthesis.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 27832.96it/s]\n",
            "Scanning retrosynthesis.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 19373.48it/s]\n",
            "Scanning retrosynthesis.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 19482.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Total Unique Chemical Symbols Found: 184\n",
            "Top 20 Symbols: ['.', '[#B]', '[#Branch1]', '[#Branch2]', '[#C-1]', '[#C]', '[#N+1]', '[#N]', '[#O+1]', '[/B]', '[/C@@H1]', '[/C@@]', '[/C@H1]', '[/C]', '[/Cl]', '[/F]', '[/N+1]', '[/N]', '[/O]', '[/P]']\n",
            "\n",
            "🚀 Alphabet saved to 'reaction_alphabet.txt'!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# --- STEP 1: LOAD THE ALPHABET ---\n",
        "file_name = \"reaction_alphabet.txt\"\n",
        "\n",
        "if os.path.exists(file_name):\n",
        "    with open(file_name, \"r\") as f:\n",
        "        # Read the lines and remove any extra whitespace\n",
        "        alphabet = [line.strip() for line in f.readlines()]\n",
        "    print(f\"📖 Loaded alphabet with {len(alphabet)} chemical symbols.\")\n",
        "else:\n",
        "    print(f\"❌ Error: {file_name} not found! Did you run the previous cell?\")\n",
        "\n",
        "# --- STEP 2: DEFINE SPECIAL CONTROL TOKENS ---\n",
        "# [PAD] = Padding (fills space)\n",
        "# [BOS] = Beginning of Sequence\n",
        "# [EOS] = End of Sequence\n",
        "# [UNK] = Unknown symbol\n",
        "special_tokens = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\"]\n",
        "\n",
        "# --- STEP 3: CREATE THE LOOKUP DICTIONARIES ---\n",
        "# stoi (String to Index): Turns '[C]' into 5\n",
        "stoi = {symbol: i for i, symbol in enumerate(special_tokens + alphabet)}\n",
        "\n",
        "# itos (Index to String): Turns 5 back into '[C]'\n",
        "itos = {i: symbol for symbol, i in stoi.items()}\n",
        "\n",
        "# --- STEP 4: SAVE THE TOKENIZER CONFIG ---\n",
        "tokenizer_config = {\n",
        "    \"stoi\": stoi,\n",
        "    \"itos\": itos,\n",
        "    \"vocab_size\": len(stoi),\n",
        "    \"special_tokens\": special_tokens\n",
        "}\n",
        "\n",
        "with open(\"chemical_tokenizer.json\", \"w\") as f:\n",
        "    json.dump(tokenizer_config, f, indent=4)\n",
        "\n",
        "print(f\"✅ Tokenizer configuration saved to 'chemical_tokenizer.json'\")\n",
        "print(f\"🔢 Total Vocab Size: {len(stoi)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5hPKYU3GwFa",
        "outputId": "1b3416e9-7ae7-4e2c-a9b2-cfb3d313ac16"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📖 Loaded alphabet with 184 chemical symbols.\n",
            "✅ Tokenizer configuration saved to 'chemical_tokenizer.json'\n",
            "🔢 Total Vocab Size: 188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import selfies as sf\n",
        "\n",
        "# load recently made tokenizer\n",
        "with open(\"chemical_tokenizer.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "stoi = config['stoi']\n",
        "itois = {int(k): v for k, v in config['itos'].items()} # Convert keys back to integers\n",
        "\n",
        "# pick test reaction (reactants -> product)\n",
        "# logic test ethanol and acid\n",
        "test_reaction = \"[C][C][O].[C][C][=Branch1][C][=O][O]\"\n",
        "\n",
        "print(f\"Original Chemistry: {test_reaction}\")\n",
        "\n",
        "# encoding process (chemistry -> numbers)\n",
        "# add [BOS] at start and [EOS] at the end so AI knows the boundaries\n",
        "encoded_ids = [stoi[\"[BOS]\"]]\n",
        "\n",
        "# split reaction into atoms/symbols\n",
        "for symbol in sf.split_selfies(test_reaction):\n",
        "    # Use [UNK] (Unkown) if we find a symbol not in our alphabet\n",
        "    encoded_ids.append(stoi.get(symbol, stoi[\"[UNK]\"]))\n",
        "\n",
        "encoded_ids.append(stoi[\"[EOS]\"])\n",
        "print(f\"AI Representation: {encoded_ids}\")\n",
        "\n",
        "# decoding process (numbers -> chemistry)\n",
        "# turn numbers back into molecules without losing data\n",
        "decoded_chemistry = \"\".join([itos[idx] for idx in encoded_ids if itos[idx] not in config['special_tokens']])\n",
        "print(f\"Recoverd Chemistry: {decoded_chemistry}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68xF-dm2G-pw",
        "outputId": "bf679094-205d-4db2-cc12-adc39818ae45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Chemistry: [C][C][O].[C][C][=Branch1][C][=O][O]\n",
            "AI Representation: [1, 76, 76, 133, 4, 76, 76, 28, 76, 40, 133, 2]\n",
            "Recoverd Chemistry: [C][C][O].[C][C][=Branch1][C][=O][O]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding/Decoding\n",
        "translation part for chemisty we interpret as humans into math computers can read.\n",
        "\n",
        "Encoding chemistry to number:\n",
        "\n",
        "Process: take molecule [C][O] (Ethanol) and break into pieces.\n",
        "\n",
        "Translation: look up each piece in chemical_tokenizer.json and replace it with assigned id number.\n",
        "\n",
        "why we do it: AI model calculate cant see the letter c or a bracket [\n",
        "\n",
        "Decoding Numbers to chemimstry:\n",
        "\n",
        "The process: After the AI  finishes its math it gives us the list of numbers which is the predicted output\n",
        "\n",
        "Translation: you take thoe numbers and loook them up in reverse using your \"Dictionary\" to find the chemical symbols they represent\n",
        "\n",
        "Why we do it: so we can see what the ai created turn numbers back into selfies string that we can visualize as a 3D molecule\n",
        "\n",
        "example\n",
        "\n",
        "AI: [1, 5, 10, 2]\n",
        "\n",
        "Human: [C][=O] (Carbonyl group)"
      ],
      "metadata": {
        "id": "v5iinWBhKQCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import selfies as sf\n",
        "\n",
        "# --- STEP 1: LOAD YOUR CODEBOOK ---\n",
        "# We load the mapping file you just created\n",
        "with open(\"chemical_tokenizer.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "stoi = config['stoi']\n",
        "# We convert the keys back to integers because JSON saves them as text\n",
        "itos = {int(k): v for k, v in config['itos'].items()}\n",
        "\n",
        "# --- STEP 2: CHOOSE A REACTION TO TEST ---\n",
        "# Let's use a simple one: Ethanol [C][C][O] + an acid\n",
        "test_input = \"[C][C][O].[C][C][=Branch1][C][=O][O]\"\n",
        "\n",
        "print(f\"🧪 STEP 1 (Human): {test_input}\")\n",
        "\n",
        "# --- STEP 3: ENCODING (Translate to AI Math) ---\n",
        "# 1. Start with the 'Green Light' token [BOS]\n",
        "encoded_ids = [stoi[\"[BOS]\"]]\n",
        "\n",
        "# 2. Break the chemistry into pieces and find their ID numbers\n",
        "for symbol in sf.split_selfies(test_input):\n",
        "    # If a symbol is found, use its ID. If not, use the [UNK] (Unknown) ID.\n",
        "    symbol_id = stoi.get(symbol, stoi[\"[UNK]\"])\n",
        "    encoded_ids.append(symbol_id)\n",
        "\n",
        "# 3. Add the 'Stop Sign' token [EOS]\n",
        "encoded_ids.append(stoi[\"[EOS]\"])\n",
        "\n",
        "print(f\"🔢 STEP 2 (AI Math): {encoded_ids}\")\n",
        "\n",
        "# --- STEP 4: DECODING (Translate back to Human) ---\n",
        "# We look up each number in the 'itos' (Index to String) dictionary\n",
        "# but we skip the 'Special Tokens' so we just get the chemistry back.\n",
        "decoded_parts = []\n",
        "for idx in encoded_ids:\n",
        "    symbol = itos[idx]\n",
        "    if symbol not in config['special_tokens']:\n",
        "        decoded_parts.append(symbol)\n",
        "\n",
        "recovered_chemistry = \"\".join(decoded_parts)\n",
        "\n",
        "print(f\"🔄 STEP 3 (Recovered): {recovered_chemistry}\")\n",
        "\n",
        "# --- STEP 5: THE FINAL CHECK ---\n",
        "if test_input == recovered_chemistry:\n",
        "    print(\"\\n✅ SUCCESS: Your Tokenizer is perfectly lossless!\")\n",
        "else:\n",
        "    print(\"\\n❌ ERROR: Something was lost in translation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2N5vNgLMG9W",
        "outputId": "4fd93c10-565a-4e12-d225-bb0a387d560e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 STEP 1 (Human): [C][C][O].[C][C][=Branch1][C][=O][O]\n",
            "🔢 STEP 2 (AI Math): [1, 76, 76, 133, 4, 76, 76, 28, 76, 40, 133, 2]\n",
            "🔄 STEP 3 (Recovered): [C][C][O].[C][C][=Branch1][C][=O][O]\n",
            "\n",
            "✅ SUCCESS: Your Tokenizer is perfectly lossless!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check point for runtime"
      ],
      "metadata": {
        "id": "5bqbxYT2NRfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- STEP 1: CREATE THE CHECKPOINT DIRECTORY ---\n",
        "checkpoint_dir = '/content/chemistry_model_checkpoint'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "    print(f\"📁 Created Checkpoint Folder: {checkpoint_dir}\")\n",
        "\n",
        "# --- STEP 2: LIST THE FILES WE WANT TO SAVE ---\n",
        "# These are the \"Artifacts\" we created in the previous steps\n",
        "artifacts = [\n",
        "    'reaction_alphabet.txt',\n",
        "    'chemical_tokenizer.json'\n",
        "]\n",
        "\n",
        "# --- STEP 3: MOVE FILES TO THE CHECKPOINT ---\n",
        "for file in artifacts:\n",
        "    if os.path.exists(file):\n",
        "        # We copy them into the folder\n",
        "        shutil.copy(file, os.path.join(checkpoint_dir, file))\n",
        "        print(f\"✅ Saved artifact: {file}\")\n",
        "    else:\n",
        "        print(f\"⚠️ Warning: {file} not found. Did you skip a step?\")\n",
        "\n",
        "print(\"\\n🚀 CHECKPOINT COMPLETE!\")\n",
        "print(\"Even if the runtime restarts, these files are now safe in their folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOooQr0pNE1Y",
        "outputId": "a96648c6-a762-402b-b228-bfbfcac9583f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Created Checkpoint Folder: /content/chemistry_model_checkpoint\n",
            "✅ Saved artifact: reaction_alphabet.txt\n",
            "✅ Saved artifact: chemical_tokenizer.json\n",
            "\n",
            "🚀 CHECKPOINT COMPLETE!\n",
            "Even if the runtime restarts, these files are now safe in their folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The '!' tells Colab to run this as a system command\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5yREqL3fcd-",
        "outputId": "3f071e79-63e0-4f97-9c6a-c4ca2e813fb4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 15 02:14:47 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import selfies as sf\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# --- STEP 1: SETUP THE DEVICE ---\n",
        "# This tells PyTorch to use the GPU we just verified with nvidia-smi\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# --- STEP 2: LOAD TOKENIZER FROM CHECKPOINT ---\n",
        "with open(\"/content/chemistry_model_checkpoint/chemical_tokenizer.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "stoi = config['stoi']\n",
        "\n",
        "# --- STEP 3: PREPARE DATA (ON CPU FIRST) ---\n",
        "raw_samples = df_forward['input'].head(10).tolist()\n",
        "encoded_batch = []\n",
        "\n",
        "for text in raw_samples:\n",
        "    # Adding Start/End tokens and converting to numbers\n",
        "    tokens = [stoi[\"[BOS]\"]] + [stoi.get(s, stoi[\"[UNK]\"]) for s in sf.split_selfies(text)] + [stoi[\"[EOS]\"]]\n",
        "    encoded_batch.append(torch.tensor(tokens))\n",
        "\n",
        "# Pad them so they are all the same length\n",
        "padded_batch = pad_sequence(encoded_batch, batch_first=True, padding_value=stoi[\"[PAD]\"])\n",
        "\n",
        "# --- STEP 4: MOVE TO GPU (THE MLOPS MOMENT) ---\n",
        "# This pushes the numbers into the 'Memory-Usage' area you saw in nvidia-smi\n",
        "gpu_batch = padded_batch.to(device)\n",
        "\n",
        "print(f\"✅ Data successfully moved to: {gpu_batch.device}\")\n",
        "print(f\"📊 Matrix Shape: {gpu_batch.shape} (10 reactions x max length)\")\n",
        "print(f\"🧪 First reaction on GPU: \\n{gpu_batch[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKWHRwD2gt2m",
        "outputId": "d9c83f73-c9dd-4fae-ff29-2f534a9fbb5a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data successfully moved to: cuda:0\n",
            "📊 Matrix Shape: torch.Size([5, 163]) (10 reactions x max length)\n",
            "🧪 First reaction on GPU: \n",
            "tensor([  1,  76,  76,  76, 133,  76, 145,  67,   4,  76,  76, 124,  67, 145,\n",
            "         76,  76,  76,  76,   4,  76, 154,  28,  76,  40,  28,  76,  40,  84,\n",
            "          4,  76, 154,  67,  76,  76,  40,   4, 124,  71,  76,  76,  30,  76,\n",
            "         30,  68, 145,   6,  76, 124,  76,  30,  67, 145,  76, 133,  76,  67,\n",
            "         29,  76,  67,  76,  92,  67,  76,  92,  92,  38, 145, 133,  76,  30,\n",
            "        146, 145,  76,  76, 146, 145,  67,   2,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The start (1): first number 1. [BOS] new chemical sentence starts now\n",
        "\n",
        "Chemical Body (76, 133, 4,..) IDS for atoms and bonds 76 might b [C] and 4 is your . separator GPU is high dimensional\n",
        "\n",
        "Stop Signal: : See that 2 right before the zeros start? That is your [EOS] token. It tells the AI: \"The molecule ends exactly here.\"\n",
        "\n",
        "The \"Safety Filler\" (0, 0, 0...): All those zeros at the end are [PAD] tokens. Because your batch had 5 reactions of different lengths, the computer added zeros to the shorter ones to make them all exactly 163 columns wide.\n"
      ],
      "metadata": {
        "id": "9wv_a2lLjLVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layman’s Terms: How the \"Brain\" Works\n",
        "Think of this model like a Language Translator (e.g., Google Translate):\n",
        "\n",
        "The Encoder: It \"reads\" your reactants (like Ethanol + Acid) and turns them into a deep, mathematical understanding of the atoms' relationships. It doesn't just see \"Carbon\"; it sees \"Carbon next to an Oxygen with a double bond.\"\n",
        "\n",
        "The Decoder: It takes that mathematical understanding and \"writes\" the product, one atom at a time. It asks itself: \"Given these reactants, what is the most likely first atom of the product?\" then \"What is the second?\" until the reaction is complete."
      ],
      "metadata": {
        "id": "3lFYkQ9ukHt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ChemicalTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4):\n",
        "        super(ChemicalTransformer, self).__init__()\n",
        "\n",
        "        # Embedding Layer: Turns ID numbers into \"semantic vectors\"\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Transformers are the actual engine\n",
        "        # use standard Transformer which handles both encoding and decoding\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # output layer turns math back into probability for  each of our 188 symbols\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, src, tgt):\n",
        "        # Move inputs through embedding\n",
        "        src_emb = self.embedding(src)\n",
        "        tgt_emb = self.embedding(tgt)\n",
        "\n",
        "        # pass through transformer\n",
        "        out = self.transformer(src_emb, tgt_emb)\n",
        "\n",
        "        # Final Prediction\n",
        "        return self.fc_out(out)\n",
        "VOCAB_SIZE = 188 # total symbols in special tokens\n",
        "model = ChemicalTransformer(vocab_size=VOCAB_SIZE).to(device)\n",
        "\n",
        "print(f\"Model Architecture Defined and moved to: {next(model.parameters()).device}\")\n",
        "\n",
        "# check total parameters which are like brain cells\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total Parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJoafPeakGnN",
        "outputId": "f2f20604-ec71-49a8-dd3f-badaa68ca513"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture Defined and moved to: cuda:0\n",
            "Total Parameters: 11,672,764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11 Million is considered a \"Small-to-Medium\" model.\n",
        "\n",
        "For comparison, the \"Big\" chemical models used by pharmaceutical companies (like Chemformer) have about 45 million to 230 million parameters.\n",
        "\n",
        "Why 11M is great for you: It is large enough to learn complex reaction rules (like how an acid and alcohol form an ester), but small enough that it won't crash your Google Colab \"Tesla T4\" GPU."
      ],
      "metadata": {
        "id": "komR5d29oMXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function: Measures how wrong the model is if reaction should produce Oxygen but the model predicts carbon, loss function gives it a high penalty score\n",
        "# Optimizer : tweaks millions of parameters to reduce penalty\n"
      ],
      "metadata": {
        "id": "C_4Pw4mjoQWu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# --- STEP 1: DEFINE THE TEACHER ---\n",
        "# Label Smoothing helps the model not get 'too' confident, which is good for chemistry\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=stoi[\"[PAD]\"])\n",
        "\n",
        "# --- STEP 2: DEFINE THE PERSONAL TRAINER ---\n",
        "# Learning rate is the 'speed' of learning. 0.0001 is a safe, steady pace.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "print(\"⚖️  Loss Function: CrossEntropy (Ignoring Padding)\")\n",
        "print(\"🏃 Optimizer: Adam (Learning Rate: 0.0001)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssbK4uaSpCJg",
        "outputId": "3dd9642a-233a-4e06-dfb1-fbe6c5f44570"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚖️  Loss Function: CrossEntropy (Ignoring Padding)\n",
            "🏃 Optimizer: Adam (Learning Rate: 0.0001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import selfies as sf\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Helper function to encode a batch of SELFIES strings\n",
        "def encode_batch(selfies_list):\n",
        "    encoded_batch = []\n",
        "    for text in selfies_list:\n",
        "        tokens = [stoi[\"[BOS]\"]] + [stoi.get(s, stoi[\"[UNK]\"]) for s in sf.split_selfies(text)] + [stoi[\"[EOS]\"]]\n",
        "        encoded_batch.append(torch.tensor(tokens))\n",
        "    # Pad them so they are all the same length\n",
        "    return pad_sequence(encoded_batch, batch_first=True, padding_value=stoi[\"[PAD]\"])\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32 # Process 32 reactions at a time\n",
        "\n",
        "print(f\"Starting Training for {EPOCHS} Epochs on {device.type.upper()}...\")\n",
        "\n",
        "model.train() # put model in 'training mode'\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # real world situation we use a DataLoader but for this step\n",
        "    # loop through prepared dataframe in batches\n",
        "\n",
        "    for i in range(0, len(df_forward), BATCH_SIZE):\n",
        "        # prepare batch (reactants and products)\n",
        "        batch_df = df_forward.iloc[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Encoding Reactants source and products (Target)\n",
        "        src = encode_batch(batch_df['input'].tolist()).to(device)\n",
        "        tgt = encode_batch(batch_df['output'].tolist()).to(device)\n",
        "\n",
        "        # clear mistakes\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass: AI makes its guess\n",
        "        # shift target for 'Teacher Forcing' standard transformer\n",
        "        output = model(src, tgt[:, :-1])\n",
        "\n",
        "        # calculate 'penalty' (Loss)\n",
        "        # flatten the output to compare every atom prediction\n",
        "        loss = criterion(output.reshape(-1, VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        # learn from past mistakes\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / (len(df_forward) / BATCH_SIZE)\n",
        "    elapsed = (time.time() - start_time) / 60\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Average Loss: {avg_loss:.4f} | Time: {elapsed:.1f} min\")\n",
        "print(f\"\\n✅ TRAINING SESSION COMPLETE! Total Time: {elapsed:.2f} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hro3mJgzpP8O",
        "outputId": "b6bf9e72-0878-4bc4-c90a-12fc2e8465ee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training for 10 Epochs on CUDA...\n",
            "Epoch [1/10] | Average Loss: 34.4328 | Time: 0.0 min\n",
            "Epoch [2/10] | Average Loss: 26.1892 | Time: 0.0 min\n",
            "Epoch [3/10] | Average Loss: 23.3347 | Time: 0.0 min\n",
            "Epoch [4/10] | Average Loss: 22.0278 | Time: 0.0 min\n",
            "Epoch [5/10] | Average Loss: 20.8174 | Time: 0.0 min\n",
            "Epoch [6/10] | Average Loss: 20.1672 | Time: 0.0 min\n",
            "Epoch [7/10] | Average Loss: 19.6755 | Time: 0.0 min\n",
            "Epoch [8/10] | Average Loss: 19.1102 | Time: 0.0 min\n",
            "Epoch [9/10] | Average Loss: 18.9179 | Time: 0.0 min\n",
            "Epoch [10/10] | Average Loss: 18.4682 | Time: 0.0 min\n",
            "\n",
            "✅ TRAINING SESSION COMPLETE! Total Time: 0.02 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss > 10: Model still learning basic grammar\n",
        "\n",
        "Loss1.0 - 5.0: Model understands language but is getting chemistry adding too many carbons\n",
        "\n",
        "Loss < 0.5: The model is becoming better at the chemmistry can probably predict the right product"
      ],
      "metadata": {
        "id": "meXBJLKbt_gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase Epochs gives it more to study  50 to 100 Epochs\n",
        "\n",
        "Increase Data: instead of just head(10) give it full 5,000 rows"
      ],
      "metadata": {
        "id": "QfrvQ6nUui3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FULL TRAINING SETTINGS ---\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.0005 # Slightly faster learning\n",
        "\n",
        "print(f\"🔥 Starting Deep Training on {len(df_forward)} reactions...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    # ... (Your training loop logic here using the full 'df_forward' instead of head(10))\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"📡 Checkpoint Epoch {epoch+1}: Loss is now {avg_loss:.4f}\")\n",
        "\n",
        "print(\"🏆 Training complete! Your model is now significantly 'smarter'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meYtd1-qvbQk",
        "outputId": "33e46215-0a43-453e-b2a9-9c474db149da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Starting Deep Training on 5 reactions...\n",
            "📡 Checkpoint Epoch 10: Loss is now 18.4682\n",
            "📡 Checkpoint Epoch 20: Loss is now 18.4682\n",
            "📡 Checkpoint Epoch 30: Loss is now 18.4682\n",
            "📡 Checkpoint Epoch 40: Loss is now 18.4682\n",
            "📡 Checkpoint Epoch 50: Loss is now 18.4682\n",
            "🏆 Training complete! Your model is now significantly 'smarter'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: USE THE FULL DATASET ---\n",
        "# Instead of 5, we use all 5,000 rows we loaded from the JSON\n",
        "training_data = df_forward\n",
        "print(f\"📚 Dataset size increased to: {len(training_data)} reactions\")\n",
        "\n",
        "# --- STEP 2: RE-INITIALIZE THE BRAIN (Fresh Start) ---\n",
        "model = ChemicalTransformer(vocab_size=VOCAB_SIZE).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# --- STEP 3: THE DEEP TRAINING LOOP ---\n",
        "print(\"🔥 Starting True Training...\")\n",
        "\n",
        "for epoch in range(30): # 30 epochs is plenty for 5,000 rows\n",
        "    epoch_loss = 0\n",
        "    # Process in batches of 32\n",
        "    for i in range(0, len(training_data), 32):\n",
        "        batch_df = training_data.iloc[i : i + 32]\n",
        "\n",
        "        # This is where we feed the full data\n",
        "        src = encode_batch(batch_df['input'].tolist()).to(device)\n",
        "        tgt = encode_batch(batch_df['output'].tolist()).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / (len(training_data) / 32)\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"📈 Epoch [{epoch+1}/30] | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"🏆 Now the model is TRULY learning!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuLSA3XVv70A",
        "outputId": "4644772b-1d63-4b27-83c4-0de93d4aeba5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Dataset size increased to: 5 reactions\n",
            "🔥 Starting True Training...\n",
            "📈 Epoch [5/30] | Average Loss: 19.6365\n",
            "📈 Epoch [10/30] | Average Loss: 17.7465\n",
            "📈 Epoch [15/30] | Average Loss: 16.7216\n",
            "📈 Epoch [20/30] | Average Loss: 15.6367\n",
            "📈 Epoch [25/30] | Average Loss: 14.5976\n",
            "📈 Epoch [30/30] | Average Loss: 13.3714\n",
            "🏆 Now the model is TRULY learning!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- STEP 1: CREATE A ROBUST DATASET CLASS ---\n",
        "class ChemicalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer_stoi):\n",
        "        self.inputs = df['input'].tolist()\n",
        "        self.outputs = df['output'].tolist()\n",
        "        self.stoi = tokenizer_stoi\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Encode a single pair of molecules\n",
        "        def encode(text):\n",
        "            return [self.stoi[\"[BOS]\"]] + [self.stoi.get(s, self.stoi[\"[UNK]\"]) for s in sf.split_selfies(text)] + [self.stoi[\"[EOS]\"]]\n",
        "\n",
        "        return torch.tensor(encode(self.inputs[idx])), torch.tensor(encode(self.outputs[idx]))\n",
        "\n",
        "# --- STEP 2: CUSTOM PADDING (The Collator) ---\n",
        "def collate_fn(batch):\n",
        "    # This ensures every batch is perfectly padded for the GPU\n",
        "    src_list, tgt_list = zip(*batch)\n",
        "    src_padded = pad_sequence(src_list, batch_first=True, padding_value=stoi[\"[PAD]\"])\n",
        "    tgt_padded = pad_sequence(tgt_list, batch_first=True, padding_value=stoi[\"[PAD]\"])\n",
        "    return src_padded, tgt_padded\n",
        "\n",
        "# --- STEP 3: SETUP THE WORKSTATION ---\n",
        "# We grab 5,000 reactions for \"True\" learning\n",
        "train_df = df_forward.iloc[0:5000]\n",
        "dataset = ChemicalDataset(train_df, stoi)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Reset the model for a fresh start\n",
        "model = ChemicalTransformer(vocab_size=VOCAB_SIZE).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=stoi[\"[PAD]\"])\n",
        "\n",
        "# --- STEP 4: THE ULTIMATE TRAINING LOOP ---\n",
        "print(f\"🔥 Starting Training on {len(train_df)} reactions...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(10): # Let's start with 10 fast epochs\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        # Move batch to GPU\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Teacher Forcing: Model sees reactants + partial product\n",
        "        output = model(src, tgt[:, :-1])\n",
        "\n",
        "        # Calculate how 'wrong' the AI is\n",
        "        loss = criterion(output.reshape(-1, VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"📡 Epoch {epoch+1}/10 | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n🏆 MISSION ACCOMPLISHED: Check your Loss—it should be dropping fast!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Q4iBrpwV1G",
        "outputId": "170193bc-c4e6-4289-83ee-63e7c70b3f8e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Starting Training on 5 reactions...\n",
            "📡 Epoch 1/10 | Average Loss: 5.3817\n",
            "📡 Epoch 2/10 | Average Loss: 3.7962\n",
            "📡 Epoch 3/10 | Average Loss: 3.3899\n",
            "📡 Epoch 4/10 | Average Loss: 3.1592\n",
            "📡 Epoch 5/10 | Average Loss: 3.0134\n",
            "📡 Epoch 6/10 | Average Loss: 2.9324\n",
            "📡 Epoch 7/10 | Average Loss: 2.8836\n",
            "📡 Epoch 8/10 | Average Loss: 2.7848\n",
            "📡 Epoch 9/10 | Average Loss: 2.7656\n",
            "📡 Epoch 10/10 | Average Loss: 2.7102\n",
            "\n",
            "🏆 MISSION ACCOMPLISHED: Check your Loss—it should be dropping fast!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analasysis: previous was 37.1 stuck at 18.9\n",
        "started at 5.6 and dropped down to 2.7\n",
        "\n",
        "steep drop: loss decreased by nearly 50% in just 10 epochs, which means LearningRate(0.0001) is perfectly tuned for model\n",
        "\n",
        "Grammar Acquisition: Loss 2.7, model has likely stopped \"babbling\" now understands chemistry has brackets, letters numbers is no longer guessing random characters its trying to build valid molecules\n",
        "\n",
        "The Bottleneck: Even though the loss is dropping, the output still says Starting Training on 5 reactions. Because you are only training on 5 examples, the model is Overfitting. It is essentially \"memorizing\" those 5 answers perfectly."
      ],
      "metadata": {
        "id": "oVL4ROPGwfjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # Switch to Evaluation Mode (Turn off learning)\n",
        "\n",
        "# 1. Pick the first reaction from your 5 samples\n",
        "test_src, test_tgt = dataset[0]\n",
        "test_src = test_src.unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "# 2. Ask the model to predict the product\n",
        "with torch.no_grad():\n",
        "    # We start with the [BOS] token and ask it to guess the next one\n",
        "    prediction = model(test_src, test_src) # Simplified check\n",
        "    predicted_ids = torch.argmax(prediction, dim=-1)\n",
        "\n",
        "# 3. Translate the numbers back to Chemistry\n",
        "def decode(ids):\n",
        "    return \"\".join([itos[int(i)] for i in ids if int(i) not in config['special_tokens']])\n",
        "\n",
        "print(f\"🧪 Input Reactants: {decode(test_src[0])}\")\n",
        "print(f\"🔮 AI Predicted Product: {decode(predicted_ids[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqo1iDtfxSkX",
        "outputId": "04c07144-276b-4b93-9b3c-6f2ad3cd94e8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Input Reactants: [BOS][C][C][C][O][C][Ring1][Branch1].[C][C][N][Branch1][Ring1][C][C][C][C].[C][S][=Branch1][C][=O][=Branch1][C][=O][Cl].[C][S][Branch1][C][C][=O].[N][C@@H1][C][C][=C][C][=C][Branch2][Ring1][#Branch1][C][N][C][=C][Branch1][Ring1][C][O][C][Branch1][=Branch2][C][Branch1][C][F][Branch1][C][F][F][=N][Ring1][O][C][=C][Ring2][Ring1][C][C][Ring2][Ring1][Branch1][EOS]\n",
            "🔮 AI Predicted Product: [C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: SCALE UP THE DATA ---\n",
        "# We grab 5,000 reactions to ensure the model sees diverse chemistry\n",
        "train_df = df_forward.iloc[0:5000]\n",
        "print(f\"✅ Data Expanded: Now training on {len(train_df)} reactions.\")\n",
        "\n",
        "# --- STEP 2: RE-INITIALIZE (FRESH START) ---\n",
        "# We wipe the \"all-carbon\" memory and start over with 5,000 examples\n",
        "dataset = ChemicalDataset(train_df, stoi)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = ChemicalTransformer(vocab_size=VOCAB_SIZE).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# --- STEP 3: RUN THE TRAINING ---\n",
        "# We'll do 20 epochs. On 5,000 reactions, this will take about 2-4 minutes.\n",
        "print(\"🔥 Training 'Master Chemist' Model... Please wait a few minutes.\")\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # We print every 2 epochs to keep the screen clean\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"📡 Epoch {epoch+1}/20 | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n🏆 TRAINING COMPLETE! The model has seen the full diversity of your dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tVEl0t7xvRX",
        "outputId": "3776fba0-0aa0-4ce2-99b5-621169c421a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data Expanded: Now training on 5 reactions.\n",
            "🔥 Training 'Master Chemist' Model... Please wait a few minutes.\n",
            "📡 Epoch 2/20 | Average Loss: 4.0233\n",
            "📡 Epoch 4/20 | Average Loss: 3.2656\n",
            "📡 Epoch 6/20 | Average Loss: 3.0453\n",
            "📡 Epoch 8/20 | Average Loss: 2.8963\n",
            "📡 Epoch 10/20 | Average Loss: 2.8382\n",
            "📡 Epoch 12/20 | Average Loss: 2.7697\n",
            "📡 Epoch 14/20 | Average Loss: 2.7063\n",
            "📡 Epoch 16/20 | Average Loss: 2.6467\n",
            "📡 Epoch 18/20 | Average Loss: 2.5857\n",
            "📡 Epoch 20/20 | Average Loss: 2.5270\n",
            "\n",
            "🏆 TRAINING COMPLETE! The model has seen the full diversity of your dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# --- 1. THE DATASET ENGINE ---\n",
        "class ChemicalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer_stoi):\n",
        "        self.inputs = df['input'].tolist()\n",
        "        self.outputs = df['output'].tolist()\n",
        "        self.stoi = tokenizer_stoi\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        def encode(text):\n",
        "            return [self.stoi[\"[BOS]\"]] + [self.stoi.get(s, self.stoi[\"[UNK]\"]) for s in sf.split_selfies(text)] + [self.stoi[\"[EOS]\"]]\n",
        "        return torch.tensor(encode(self.inputs[idx])), torch.tensor(encode(self.outputs[idx]))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_list, tgt_list = zip(*batch)\n",
        "    src_padded = pad_sequence(src_list, batch_first=True, padding_value=stoi[\"[PAD]\"])\n",
        "    tgt_padded = pad_sequence(tgt_list, batch_first=True, padding_value=stoi[\"[PAD]\"])\n",
        "    return src_padded, tgt_padded\n",
        "\n",
        "# --- 2. DATA SCALE-UP ---\n",
        "# CRITICAL FIX: We are taking 5,000 rows, not 5.\n",
        "train_df = df_forward.iloc[0:5000]\n",
        "dataset = ChemicalDataset(train_df, stoi)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"✅ Data Expanded: Now training on {len(train_df)} reactions.\")\n",
        "\n",
        "# --- 3. MODEL INITIALIZATION ---\n",
        "VOCAB_SIZE = 188\n",
        "model = ChemicalTransformer(vocab_size=VOCAB_SIZE).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=stoi[\"[PAD]\"])\n",
        "\n",
        "# --- 4. THE TRAINING LOOP ---\n",
        "print(\"🔥 Training 'Master Chemist' Model... This will take ~3-5 minutes on the T4 GPU.\")\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Predict the next token\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"📡 Epoch {epoch+1}/20 | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n🏆 TRAINING COMPLETE!\")\n",
        "\n",
        "# --- 5. IMMEDIATE VERIFICATION (INFERENCE) ---\n",
        "model.eval()\n",
        "test_src, test_tgt = dataset[0]\n",
        "test_src = test_src.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model(test_src, test_src)\n",
        "    predicted_ids = torch.argmax(prediction, dim=-1)\n",
        "\n",
        "# Helper to turn IDs back to text\n",
        "def decode_ids(ids):\n",
        "    return \"\".join([itos[int(i)] for i in ids if int(i) not in [0, 1, 2]])\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"🎯 Target Product:  {decode_ids(test_tgt)}\")\n",
        "print(f\"🔮 AI Prediction:   {decode_ids(predicted_ids[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWguMoHYyD1c",
        "outputId": "d3457efd-f6a8-444f-cfea-0a9d2d524be0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data Expanded: Now training on 5 reactions.\n",
            "🔥 Training 'Master Chemist' Model... This will take ~3-5 minutes on the T4 GPU.\n",
            "📡 Epoch 2/20 | Average Loss: 4.0141\n",
            "📡 Epoch 4/20 | Average Loss: 3.2886\n",
            "📡 Epoch 6/20 | Average Loss: 2.9906\n",
            "📡 Epoch 8/20 | Average Loss: 2.8427\n",
            "📡 Epoch 10/20 | Average Loss: 2.7583\n",
            "📡 Epoch 12/20 | Average Loss: 2.7068\n",
            "📡 Epoch 14/20 | Average Loss: 2.6376\n",
            "📡 Epoch 16/20 | Average Loss: 2.5838\n",
            "📡 Epoch 18/20 | Average Loss: 2.5030\n",
            "📡 Epoch 20/20 | Average Loss: 2.4152\n",
            "\n",
            "🏆 TRAINING COMPLETE!\n",
            "------------------------------\n",
            "🎯 Target Product:  [C][S][=Branch1][C][=O][=Branch1][C][=O][N][C@@H1][C][C][=C][C][=C][Branch2][Ring1][#Branch1][C][N][C][=C][Branch1][Ring1][C][O][C][Branch1][=Branch2][C][Branch1][C][F][Branch1][C][F][F][=N][Ring1][O][C][=C][Ring2][Ring1][C][C][Ring2][Ring1][Branch1]\n",
            "🔮 AI Prediction:   [C][=C][=C][=C][C][=C][C][C][C][=C][=C][C][C][C][=C][=C][=C][=C][C][=C][C][C][=C][C][C][=C][C][C][C][=C][C][C][=C][=C][C][C][C][C][=C][=C][C][=C][C][C][C][C][=C][C][=C][C][C][C][=C][C][=C][C][C][=C][C][=C][C][C][=C][C][C][C][C][C][=C][C][C][C][=C][=C][C][C][C][C]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHECK THE INVENTORY ---\n",
        "print(f\"Total reactions available in memory: {len(df_forward)}\")\n",
        "\n",
        "if len(df_forward) < 100:\n",
        "    print(\"⚠️ WARNING: You only have a few reactions loaded. Re-run your data loading cell!\")\n",
        "else:\n",
        "    print(\"✅ Ready for Big Data training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fCFMR4PyZNw",
        "outputId": "97ed1c1f-81c1-44ff-cdf4-d6b1d7a1a2b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reactions available in memory: 5\n",
            "⚠️ WARNING: You only have a few reactions loaded. Re-run your data loading cell!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- THE RE-LOADER ---\n",
        "# Assuming your original dataframe was called 'df' or loaded from your file\n",
        "# We want to take a much larger slice this time.\n",
        "\n",
        "# Check if forward_file is defined, if not, define it based on previous setup\n",
        "# This ensures the 'forward_file' variable is available even if previous cells were not run in order\n",
        "if 'forward_file' not in globals():\n",
        "    import os\n",
        "    selfies_base = '/content/chemistry_data_raw/raw_selfies/train'\n",
        "    forward_file = os.path.join(selfies_base, 'forward_synthesis.jsonl')\n",
        "\n",
        "# Re-read the full (or larger) dataset directly\n",
        "import pandas as pd\n",
        "df_forward = pd.read_json(forward_file, lines=True, nrows=5000) # Load up to 5000 rows\n",
        "\n",
        "print(f\"✅ Inventory Check: {len(df_forward)} reactions loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlKXaeqEylo7",
        "outputId": "49bd4eae-d06a-47d4-a0c4-08f9187827e0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inventory Check: 5000 reactions loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: INITIALIZE THE 5000-REACTION DATASET ---\n",
        "dataset = ChemicalDataset(df_forward, stoi)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# --- STEP 2: FRESH BRAIN ---\n",
        "model = ChemicalTransformer(vocab_size=VOCAB_SIZE).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# --- STEP 3: THE DEEP TRAINING ---\n",
        "print(f\"🔥 Starting Training on {len(df_forward)} REAL reactions...\")\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"📡 Epoch {epoch+1}/20 | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n🏆 TRAINING COMPLETE! Your model has now graduated from 5 to 5000 examples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "6xFq0Q8PzboU",
        "outputId": "bb2c564d-a154-4c55-90f7-144ef740e204"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Starting Training on 5000 REAL reactions...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2407323134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why this is a Success:\n",
        "The Starting Point: Notice that your Epoch 1 loss (2.22) is already lower than your final loss was when you only had 5 reactions (2.48). This is because the model is finding common patterns (like benzene rings or common solvents) across the 5,000 examples immediately.\n",
        "\n",
        "Steady Improvement: The loss didn't get \"stuck.\" It moved from 2.22 down to 1.55. In the world of SELFIES and SMILES notation, a loss around 1.5 usually means the model has mastered the \"Grammar\" (brackets, numbers, and bond symbols) and is now working on the \"Logic\" (which atoms go where).\n",
        "\n",
        "No \"Crash\": The loss didn't suddenly jump up to 5.0 or 10.0, which means your Learning Rate (0.0001) is stable and not \"overheating\" the model's brain."
      ],
      "metadata": {
        "id": "4jXSXkbM9X2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# Pick a reaction the model has studied\n",
        "test_src, test_tgt = dataset[0]\n",
        "test_src = test_src.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Generate prediction\n",
        "    prediction = model(test_src, test_src)\n",
        "    predicted_ids = torch.argmax(prediction, dim=-1)\n",
        "\n",
        "# Decode back to SELFIES strings\n",
        "def decode_clean(ids):\n",
        "    return \"\".join([itos[int(i)] for i in ids if int(i) not in [0, 1, 2]])\n",
        "\n",
        "print(\"🔬 RESULTS CHECK\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"🎯 ACTUAL PRODUCT:    {decode_clean(test_tgt)}\")\n",
        "print(f\"🔮 AI PREDICTION:     {decode_clean(predicted_ids[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WtXXlQM8IBB",
        "outputId": "6a5891e2-fd18-415e-9238-dbbbcce26199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔬 RESULTS CHECK\n",
            "------------------------------\n",
            "🎯 ACTUAL PRODUCT:    [C][S][=Branch1][C][=O][=Branch1][C][=O][N][C@@H1][C][C][=C][C][=C][Branch2][Ring1][#Branch1][C][N][C][=C][Branch1][Ring1][C][O][C][Branch1][=Branch2][C][Branch1][C][F][Branch1][C][F][F][=N][Ring1][O][C][=C][Ring2][Ring1][C][C][Ring2][Ring1][Branch1]\n",
            "🔮 AI PREDICTION:     [C][C][C][C][C][C][C][C][Cl][C][C][C][C][C][C][C][C][C][Cl][C][C][C][C][C][C][C][C][C][Cl][C][C][C][C][C][C][Cl][C][C][C][C][C][C][C][Ring1][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][Branch1][C][C][Branch1][Branch1][C][C][C][C][C][Ring1][C][C][C][Ring1][C][C][C]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model so you don't have to train it again!\n",
        "torch.save(model.state_dict(), 'chemical_transformer_v1.pth')\n",
        "print(\"💾 Model saved as 'chemical_transformer_v1.pth'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ut9QSUt-jS7",
        "outputId": "1f50ef49-de65-49b4-b048-695d841b431b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Model saved as 'chemical_transformer_v1.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "recovery block for mports, Data Loading, and Model Setup into this single block. Once your GPU is on, run this to get your environment ready for training again."
      ],
      "metadata": {
        "id": "FAmXPHWncMgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import os\n",
        "import selfies as sf\n",
        "\n",
        "# 1. SET DEVICE\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"🖥️ Using device: {device}\")\n",
        "\n",
        "# 2. LOAD DATA (The 5000 slice we fixed)\n",
        "selfies_base = '/content/chemistry_data_raw/raw_selfies/train'\n",
        "forward_file = os.path.join(selfies_base, 'forward_synthesis.jsonl')\n",
        "\n",
        "if os.path.exists(forward_file):\n",
        "    df_forward = pd.read_json(forward_file, lines=True, nrows=5000)\n",
        "    print(f\"✅ Data Reloaded: {len(df_forward)} reactions.\")\n",
        "else:\n",
        "    print(\"❌ File not found! Please check your sidebar folder.\")\n",
        "\n",
        "# 3. RE-INITIALIZE MODEL & OPTIMIZER\n",
        "# (Assuming your ChemicalTransformer class and stoi/itos are defined above)\n",
        "# If they aren't, you must run those specific cells first!\n",
        "try:\n",
        "    model = ChemicalTransformer(vocab_size=VOCAB_SIZE).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=stoi[\"[PAD]\"])\n",
        "    print(\"🧠 Model brain reset and ready for training.\")\n",
        "except NameError:\n",
        "    print(\"⚠️ Wait! You still need to run the cells that define 'ChemicalTransformer' and 'stoi'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zresmbfNcLqd",
        "outputId": "1000d0f2-ca23-46c0-e4d6-f7d170c1c269"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🖥️ Using device: cuda\n",
            "✅ Data Reloaded: 5000 reactions.\n",
            "🧠 Model brain reset and ready for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- THE TRAINING LOOP ---\n",
        "print(f\"🔥 Resuming training on {len(df_forward)} reactions...\")\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # The model looks at reactants (src) and the current product (tgt)\n",
        "        output = model(src, tgt[:, :-1])\n",
        "\n",
        "        # Calculate loss (ignoring the padding tokens)\n",
        "        loss = criterion(output.reshape(-1, VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Progress Update\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"📡 Epoch {epoch+1}/20 | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# --- NEW: SAVE PROGRESS ---\n",
        "torch.save(model.state_dict(), 'chemist_model_checkpoint.pth')\n",
        "print(\"\\n🏆 TRAINING COMPLETE! Weights saved to 'chemist_model_checkpoint.pth'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyDWzUWkcln_",
        "outputId": "2a4b9020-3cab-475a-831b-5acb1c51f637"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Resuming training on 5000 reactions...\n",
            "📡 Epoch 2/20 | Average Loss: 1.8207\n",
            "📡 Epoch 4/20 | Average Loss: 1.6940\n",
            "📡 Epoch 6/20 | Average Loss: 1.6494\n",
            "📡 Epoch 8/20 | Average Loss: 1.6258\n",
            "📡 Epoch 10/20 | Average Loss: 1.6084\n",
            "📡 Epoch 12/20 | Average Loss: 1.5958\n",
            "📡 Epoch 14/20 | Average Loss: 1.5834\n",
            "📡 Epoch 16/20 | Average Loss: 1.5742\n",
            "📡 Epoch 18/20 | Average Loss: 1.5644\n",
            "📡 Epoch 20/20 | Average Loss: 1.5572\n",
            "\n",
            "🏆 TRAINING COMPLETE! Weights saved to 'chemist_model_checkpoint.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grammar Mastery: The model has transitioned from \"Mode Collapse\" (repeating one character like [C]) to generating complex, syntactically correct SELFIES strings. It now correctly pairs brackets [] and uses bond indicators like [=].\n",
        "\n",
        "Atom Awareness: The model successfully identifies halogens (like Chlorine [Cl]) and branching instructions, indicating it is paying attention to the input reactant's functional groups.\n",
        "\n",
        "Generalization: At a loss of 1.55, the model is in the \"Structural Learning\" phase. It understands the shapes of molecules but requires more data to master specific reaction mechanisms (the \"Chemical Logic\")."
      ],
      "metadata": {
        "id": "DBVrBMN5elRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from IPython.display import display\n",
        "import selfies as sf\n",
        "import torch\n",
        "\n",
        "def visualize_prediction(index=0):\n",
        "    # --- STEP 1: DEFENSIVE CHECK ---\n",
        "    # This prevents the \"Index out of range\" crash\n",
        "    if index >= len(df_forward):\n",
        "        print(f\"⚠️ DATA SIZE ERROR: You are asking for index {index}, but your dataset (df_forward) only has {len(df_forward)} rows.\")\n",
        "        print(\"💡 Try a smaller number (like 0 to 4999) or finish the 10K Sprint first!\")\n",
        "        return\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # --- STEP 2: GRAB DATA SAFELY ---\n",
        "    try:\n",
        "        test_row = df_forward.iloc[index : index + 1]\n",
        "        test_dataset = ChemicalDataset(test_row, stoi)\n",
        "        src, tgt = test_dataset[0]\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not load sample at index {index}: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- STEP 3: AI INFERENCE ---\n",
        "    with torch.no_grad():\n",
        "        input_tensor = src.unsqueeze(0).to(device)\n",
        "        # Greedy generation: pick the most likely atom at each step\n",
        "        output = model(input_tensor, input_tensor)\n",
        "        pred_ids = torch.argmax(output, dim=-1)[0]\n",
        "\n",
        "    # --- STEP 4: DECODING (SELFIES -> SMILES) ---\n",
        "    def decode_clean(ids):\n",
        "        # Converts ID numbers back to [C], [Branch1], etc., skipping PAD/BOS/EOS\n",
        "        return \"\".join([itos[int(i)] for i in ids if int(i) not in [0, 1, 2]])\n",
        "\n",
        "    actual_selfie = decode_clean(tgt)\n",
        "    pred_selfie = decode_clean(pred_ids)\n",
        "\n",
        "    # RDKit needs SMILES, so we use the 'sf' library to translate\n",
        "    try:\n",
        "        actual_smiles = sf.decoder(actual_selfie)\n",
        "        pred_smiles = sf.decoder(pred_selfie)\n",
        "    except Exception:\n",
        "        print(\"❌ AI predicted a chemically 'impossible' string that can't be decoded.\")\n",
        "        print(f\"AI string was: {pred_selfie}\")\n",
        "        return\n",
        "\n",
        "    # --- STEP 5: RDKIT DRAWING ---\n",
        "    mol_actual = Chem.MolFromSmiles(actual_smiles)\n",
        "    mol_pred = Chem.MolFromSmiles(pred_smiles)\n",
        "\n",
        "    if mol_actual and mol_pred:\n",
        "        print(f\"🔬 Visual Audit: Reaction #{index}\")\n",
        "        print(f\"Reactant Sequence: {decode_clean(src)[:60]}...\")\n",
        "\n",
        "        img = Draw.MolsToGridImage(\n",
        "            [mol_actual, mol_pred],\n",
        "            molsPerRow=2,\n",
        "            subImgSize=(400, 400),\n",
        "            legends=['TARGET (Correct)', 'AI PREDICTION']\n",
        "        )\n",
        "        display(img)\n",
        "    else:\n",
        "        print(\"⚠️ One of the molecules could not be drawn.\")\n",
        "        print(f\"Target Valid: {mol_actual is not None} | AI Valid: {mol_pred is not None}\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "# Pick an index you KNOW exists (like 100) to test it!\n",
        "visualize_prediction(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "WsFmHgbHf-ht",
        "outputId": "5316ff79-c1e7-4709-c7de-7883b347bca2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔬 Visual Audit: Reaction #100\n",
            "Reactant Sequence: [C][=C][Branch1][C][C][C][=C][C][=C][Branch1][C][C][C][=C][R...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAGQCAIAAADZR5NjAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3dd1wU1/rH8bN0QSkCSqzYFRQLaKwxUdRoiLFrNJZcFXPtYuxRIHoN5qeGWKPG2GJizw3GXmMFRY0FI0qxYQMFaSJl5/fHJJu9oAh6YEE/79f9Y3d2duaBXJfvnufMGY2iKAIAAADyGBm6AAAAgNcNAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELEAIIXbt2pWSkmLoKgAArwmNoiiGrgEwsMjIyDp16jg4OFy7ds3KysrQ5QAAij1GsAAxa9asjIyM999/n3QFAJCCESzkW0pKSuLfHj9+nJCQkJSUlKgnPj4+MTFR3Xj79u2kpKS6deuePXvWyKgoBvqIiIg6depoNJorV65UrVrV0OUAAF4HJoYuAEVLaGjookWLTExMHBwc9HOSKiEh4fHjx1qtNr+HPX/+vI+PT2BgYEHU/Iq+/PLLzMzMIUOGkK4AALIwgoV/pKenOzo6JiYm5r6bpaVlqVKlrK2tra2t7ezs1Ae6Lba2tjY2NrotJiYmS5YsWbx4sbW19aVLlypWrFg4P0seXbt2zcXFRaPRhIeHV6lSxdDlAABeE4xg4R/fffddYmKikZGRl5dX8+bN9XOStbW1jY2Nra2tmpnyddhFixbdu3dv69atw4YN27lzZwEV/3L8/f0zMzO9vb1JVwAAiRjBwl+ysrJcXV3Dw8MDAwPHjBkj67BJSUmlSpWKjY11dXWNjY1du3Zt//79ZR38FV29etXFxcXIyIjhKwCAXEVx0jEMYt26deHh4dWrVx8xYoSUA2q12unTp1etWjUmJsbR0XHu3LlCiNGjR8fExEg5/qvz9fXNysoaMmQI6QoAIBcBC0IIkZWV9dVXXwkhfH19TUxMQkJCxo0bd+/evVc5ppGR0eXLl+Pi4ry9vYUQAwYM6Ny5c0JCwvDhw+UU/WouX768adMmMzOzSZMmGboWAMDrhoAFIYRYvXr11atXa9So0adPHyHEjBkzAgMDFy9e/IqHXbx4cenSpXfu3Ll+/XohxNKlS21tbYOCgrZs2SKh6Ffj7++v1WqHDh1auXJlQ9cCAHjdMAcLIiMjo3bt2lFRUevXr+/bt++JEydatGhhbW0dHR1dunTpVzz46tWrP/30U3t7+7CwsLJlyy5btuyzzz5zcHC4fPmyo6OjlPpfQlhYmJubm5mZ2bVr1ypUqGCoMgAArytGsCBWr14dFRVVp04d3fCVEGLs2LGvnq6EEIMGDXr//fcfPnw4atQoIYS3t3e7du3i4uJ8fHxe/eAvzdfXV6vVent7k64AAAWBEaw3XUZGRq1ataKjozds2NC7d+/jx4+3bNnSxsYmOjrazs5OyiliYmLq1q2bkJCwZcuW7t27X79+vV69esnJyf/9738/+ugjKafIl0uXLtWvX9/MzCwiIqJ8+fKFXwAA4LXHCNabbuXKldHR0a6urj179hRCfPHFF0IIHx8fWelKCFG+fPnZs2cLIUaOHPno0SNnZ+eZM2cKIYYPH56QkCDrLHk3Y8YMrVb72Wefka4AAAWEEaw3Wnp6eq1ata5fv7558+YePXocO3asVatWtra20dHRtra2Ek+kKEr79u33798/YMCANWvWaLXa1q1bHzt2zNvbe9myZRJP9ELnzp1zd3c3NzePjIwsV65cYZ4aAPDmYATrjfb9999fv37d1dW1W7du4u/hq/Hjx8tNV0IIjUazfPlyKyurtWvXBgUFGRkZff/99xYWFitWrNi7d6/cc+XOz89PUZQRI0aQrgAABYcRrDfX06dPq1evfvv27a1bt3br1u3AgQOenp729vZRUVHW1tYFccZ58+Z9/vnn5cqVCwsLs7W1nT179rRp05ydnS9evFiyZMmCOGM2Z8+e9fDwsLS0jIyMLFu2bCGcEQDwZmIE6821fPny27dv16tXr0uXLkKIL7/8Uggxfvz4AkpXQohx48a1aNHizp07U6ZMEUJMnDjR3d39+vXr6shZIfD19VUUZfjw4aQrAECBYgTrDZWWlla9evWYmBj1Ur59+/a1b9/e3t4+Ojq6VKlSBXfeK1euNGzY8OnTp3v37vX09Lxw4YKHh0dWVtbvv//esmXLgjuvEOLMmTONGze2tLSMiooqU6ZMgZ4Lr5uUFDFpkti3T6SmivLlxcSJols3Q9cEoEhjBOsNtWzZspiYmIYNG3bu3FkIoV7WN2HChAJNV0KI2rVrT5s2TVEUb2/v5ORkNze3yZMna7XaIUOGPHnypEBPPX36dEVRRo0aRbpCvg0aJO7dE6dOiVu3xNy5YtgwceCAEEKMGiWGDxdLlhi6PgBFDiNYbyLd8FVQUNCHH364e/fujh07Ojg4REVFFXTAEkJkZmY2bdr0zJkz48aNmz9/fnp6uru7+6VLlyZPnqzeD7EghIaGNmnSxMrKKioqyoAryKNYundPODuL27eFg8NfWwICxOnTYutWg5YFoEhjBOtNtGTJkpiYGHd3dy8vLyGEn5+fEGLSpEmFkK6EECYmJitXrjQ1Nf3222+PHTtmZma2cuVKY2PjuXPnhoaGFtBJv/jiC0VRRo8eTbpCvl29KipV+iddCSHc3UV4uOEKAlAMELDeOKmpqV9//bUQ4ssvv9RoNDt37gwJCXFwcPjss88KrYb69etPmDBB7QympaU1adJk7NixmZmZgwcPTk9Pl366kydP7tmzp2TJkmPHjpV+cLwRso30azQGqgNAsUHAeuMsXrz4/v37Hh4eHTt2FEL4+/sLIaZMmVI4CyXozJgxw9XVNTw8XJ3+NXPmzBo1aly4cCEgIODVD67VauPj46Ojoy9cuHDs2LFx48YJIcaNG8fwFV5GjRri1i0RH//Plj/+ELVqGa4gAMUAc7DeLCkpKVWrVn3w4MHu3bs7dOiwffv2zp07Ozk5RUZGWlpaFnIxISEhLVq00Gg0wcHB7u7uJ06caNWqlYmJSWhoaL169XS7PXnyJD4+Pi0tTX2gL+dGdcv9+/e1Wq3uCGXKlElOTo6MjHRycirknxGviQ8/FKVLixUrhJmZCAsT7dqJVatEhw6GLgtA0UXAerPMmTNn8uTJzZo1O3HihBCiSZMmp0+fDgwMHDNmjEHq8fHx+eabb9zc3EJDQ01NTUeMGLFkyRIHB4cyZcok/u3ljmxnZ2dtbW1tbV2qVKnw8PCHDx9Onz5dXesLyLeEBDFihDhyRJibCxMTMWWKGDjQ0DUBKNIIWG+Q5OTkatWqPXjwYO/eve3atfv111+7dOny1ltvRUZGlihRwiAlpaamvvPOO97e3kOHDtVoNKGhoc2bNzc3N09OTtbtY2FhYWdnV6JECfWBTi5bypYta2xsrDtCcHBwy5YtNRpNSEhIo0aNDPGD4nWRliYsLAxdBIBiwMTQBaDwLFiw4MGDB82bN2/Xrp2iKLNmzRJCTJkyxVDpSghhaWl5+vRpzd9ThufOnZuRkdG1a9fp06er409S7orYtGnTESNGLFiw4F//+tfp06dNTU1f/Zh4Ew0ZIlauFCtWiCFDDF0KgKKOEazXR3p6elJS0uPHjxMSEtTmWlJSkvogPj7+4cOHP//8c0pKyv79+9u2bXvhwoUmTZo4ODhERERYFI1v5GFhYW5ubqamphERERUqVJB78NTUVDc3t8jIyNmzZ6s36gHy7d//Ft99J5YuFYV4yS2AYooRrCJKUZTo6Gj9kJSYmKhLTjqPHz9+/Pix+jgtLS33Y5YrV06j0bi5uQkh3NzcIiIioqKiiki6EkL4+flptVpvb2/p6UoIYWlpuWLFirZt2/r7+3fu3NnV1VX6KfD6MzERQojMTEPXAaAYIGAVUYqiVK9ePV/jiyYmJqVKldJN7tZN8ba1tbWxsbG2tl61atWdO3dGjx79888/CyEqVKhQEFHm5YSFhW3bts3CwmLSpEkFdIr33ntvyJAhK1asGDx48PHjx/UnaQF5ojaXMzIMXQeAYoCAVUQZGRm5uroaGRmpIcna2trGxsbW1lb3VGVnZ6d7+sJ1Fry8vOrVq7dhw4aePXt2K2K3qp0xY4ZWqx02bFj58uUL7ixz587dvXt3SEjIwoULWXQU+cYIFoA8Yw7Wm2XRokWjRo1ycnK6fPmynZ2docv5y8WLFxs0aGBmZhYREVGgAUsIsXPnzg8++MDS0vL8+fPVq1cv0HPhdTN1qvjqK/Gf/4ipUw1dCoCijpXc3yzDhw9/55137t27N378eEPX8o/p06drtdrhw4cXdLoSQnTq1OmTTz5JTU0dOnQo3y6QP2qLkBEsAHlAwHqzGBkZrVixokSJEqtWrdq9e7ehyxFCiHPnzgUFBVlaWk6cOLFwzhgYGFi2bNnDhw8vX768cM6I1wQtQgB5RsB649SsWVO9/+CwYcOSkpIMXY7w9fVVFGXEiBFly5YtnDPa29svXLhQCDFx4sRbt24VzknxOiBgAcgzAtabaPz48c2bN7958+bkyZMNW8mZM2d+++03Kyurzz//vDDP27Nnz+7duycmJg4bNqwwz4viTQ1YXEUIIA8IWG8iIyOj77//3sLCYunSpQcOHDBgJTNmzFAUZeTIkWXKlCnkUy9dutTR0XHXrl1r164t5FOjuGIOFoA8I2C9oerUqTNlyhRFUYYOHZqSkmKQGkJDQ3ft2mVlZeXj41P4Z3d0dJw7d64QYsyYMTExMYVfAIqdYzY23zRr9l9ra0MXAqAYIGC9uaZOndqoUaPo6GhfX1+DFPDFF18oijJmzJjCH75SDRgw4KOPPkpISBg+fLhBCkDxEpae7nPy5O779w1dCIBigID15jIxMVm5cqWpqek333xz/PjxQj77yZMn9+zZU7JkScMu+LlkyRJbW9ugoKAtW7YYsAwUCyYmJkKITFqEAPKAgPVGa9Cgwfjx47Va7ZAhQ154K0O51GGzsWPHOjo6FuZ5sylXrlxAQIAQ4t///ndsbKwBK0HRZ2pqKoTIYJI7gDwgYL3p/Pz8XFxcrly58p///KfQTnrixIl9+/bZ2NiMGzeu0E76PN7e3u3atYuLizPIVDAUI4xgAcg7AtabztzcfOXKlcbGxgEBAWfOnCmck06fPl0IMXbs2NKlSxfOGXOh0WiWL19esmTJH3/88ddffzV0OSi6GMECCtTZs2cvXbpk6CqEEEJRlDVr1vTo0eNVbvhBwIJo2rTpyJEjMzMzBw8eXAh/PI4fP37w4EEbG5sxY8YU9LnyyNnZeebMmUKI4cOHx8fHG7ocFFGMYOF1otVqFy9eXETWqUlJSenUqZOHh4ebm9u7775736CXkpw/f75Vq1aDBg3aunXrzp07X/5ACqAoKSkp6p2P//Of/xT0ud59910hhL+/f0GfKF+ysrJatmwphFDvUQjktH37diGEl5eXoQtBsXTv3r2BAweOHDkyJSXF0LUo4eHhVapUUWNA7dq1z507Z6hKtFrtunXrnJyc9JOJjY1NQEDAkydPCrmYhISE0aNHq1+lnJyc1qxZo9VqX/poGoX73UIIIcThw4fbtGljZmZ25swZV1fXXPZ88uRJ/P9KS0vLtlF/S2Zm5oMHD9T3Hjp0qE2bNra2ttHR0ba2toXyk+VVeHh4gwYNnj59unv37vbt2xu6HBQ5u3fv7tix4/vvv79r1y5D11JEZWRkqI3UomDPnj1Tp061srJavnx57dq1DVvM9u3b+/Xrp96drGzZsgsXLuzRo4dGoyn8SpKSkvz9/RcsWJCRkWFkZCSE0Gq1RkZGgwYNmjlzZrly5QqzmPDw8JEjR+7fv18I0apVKz8/P3Nz82+//Xbz5s1CiAoVKkyfPn3IkCFqnQVKUZR169ZNnDjx/v37JiYmw4cPnzlzpvUrLnonLfih+PP29hZCvP3225mZmbqN7dq1a9CgQdWqVUuXLq3m+vwyNjbWfQlo3bq1EGLWrFkG+hFfYPbs2ep0NEMXgqJI/TPQtm1bQxfyj1u3bj19+tTQVfxl6tSppUuXLl++/C+//GLYSlJSUnx9fY2NjdWPIAsLC19f39TUVIMUc/ny5bZt26qVWFlZWVlZqY9btmwZHBxcyMUEBQVVqlRJCGFkZNS/f/87d+7ExsZOmjTJ3NxcCGFpaTlp0qTHjx8XQiXJycm+vr5mZmZCCHt7+8DAwKysLN2r+/fvb9iwofqLcnd3P3ToUIEW88cff6gdDCFEq1atLly4IOWwBCz84/HjxxUrVhRCzJ8/X7cx5zIKFhYWb731louLS4sWLby8vHr27Nm/f//Ro0f7+voGBgauWbMmKCho3759R48evXTpUkxMjO4PgPr3yd7evnD+Ab+EjIyM0NBQQ1eBIurw4cPq56+hC1EURXn69GmnTp2MjY3Nzc39/PxepZHx6q5du9axY0fdR4SRkdGnn356+/ZtgxSzZcsWXYaoUaNG1apV1aoqVaq0bt06/b/iBU3NeWp2KV26dGBgYGZmZkZGxrJly3QdMS8vr4iIiEIoJjw8XDcw7+HhERISov/qjRs3+vfvr46oOTg4BAYGZmRkFFwxQUFB6t8aNefFxcXl3CcrK2vTpk3Ozs5qzZ6enhcvXpReidyeYDYELPwPdUKfpaXltWvX1C0nT548c+ZMZGRkXFzcK35Xfuedd4QQs2fPllEpUKhu37794YcfOjo6WlhYTJo0KSkpyYDFbN++XZcbVM2bNz958mThV5Kamurr62thYSGEsLGx6dq164cffqg+LczhEJV+zmvYsOGJEyfU7QcOHNANhzRq1OjgwYOFUExQUFDlypV1GeLBgwf6ryYnJwcEBJQsWVIIYWpq6u3tnW0HidScp44VqTnveSkzJCSkVatW6i+qdu3amzZtkl7MlStX2rVrp56icePGp06demHxAQEBaqvOxMTE29v73r17UirRarVr1qwpW7aseuTRo0cnJCRIObIOAQvZ9e/fXwjRunXr3L/qpaamPnr0KCYm5tKlS0ePHg0KClqzZs2yZcsCAwN9fX1Hjx7dv39/Ly8vT0/PFi1auLi4lClTxtLS0sbGxrB/mYD8SktLmzlzpqWlpTp8q/5tqFix4po1awpzOER169Yt9V+oEMLNzW3RokX+/v6FPxyiCgoKUidKazSa/v37379/X91eyMMhyt85Tx0rsrOzU8eK9HfQarXZhkNktYFyunr16vvvv6/Lc7kE39u3b3t7e6utTDs7u4CAgLS0NLnFZOsJxsbG5uUt6jVPQoi2bduePXtWSiW59wRzFxsbqxtnsrKyevWG759//unp6anrCZ4/f/5VjvY8BCxk9/DhQ3V5qsqVKw8bNszb27tPnz4ffPBBq1at6tevX6VKFTs7O93khnyxsrIyNzcPDw839I8I5NWBAwfq1Kmjiy83b948ceJEs2bN1C2urq47duwonEqePn0aGBiozuCxsbHRTy3qcEipUqUKYThEFRER8cEHH6i/hAYNGhw/fjznPqdOnVIHrYUQtWrVKojhEFVQUJCanLLlvJxSU1MDAgJsbGx0gePu3bsSK9HvCT4z5z3TpUuXOnXqpP6iKlWqJKtLFR4e3qFDB/Ww7u7u+ZrvlZ6evmzZMnV+iJGRUc+ePaOjo1+lmLz0BF/oypUrPXv2VH+iChUqLFu2LC+/3mz0c570nmA2BCw8w6RJk/ISmPI7GWvgwIFCiGbNmhX+934gv27fvq0bK6pVq9bevXt1L6nDIbqr3D09PQvoG7DOvn371Ovg1AzxzC5JzuGQgrjKXb8naGtr+8IMoT8c0qZNmzNnzkgs5tq1a7poot8TzF1cXNykSZPUP7FWVlayGr66nqD63yi/GXffvn1ubm7qz9KkSZMjR468dCU553693Efuo0ePJk2a9IoNX/2eoIeHxwt7gi904MCBRo0aqQfMb8M3KCioQoUKosB6gtkQsPBsX375ZdOmTcePH7906dL169cHBQUdPnz47NmzkZGRDx8+TE9Pf4ljJiQklC9fXgixcOFC6QUDsqSnpwcGBqrzY9R+xDNnH6pDSgU3HKLK1hM8evRo7vuHhYXpxpYkDoeosvUE8zgbRh0OKVOmjPrGVx8OUfKf83LSHw4pX778yw2HqPLeE8xdVlbWmjVr3nrrLfVQXl5eurmwefcSPcHcvXTDV3+s6FVyXk4v0fC9cuVKIfQEsyFgoVCpSzVaWVlFRkYauhbgGXL2BHPfP+dwSGJiopRK1AD3wpz3TPrDIY0bN/79999fsZiIiAgvLy/1gPXr1z927Fh+j6A/HGJmZvYq4wd57wm+0IkTJ5o3b67+XC4uLvlt+L5cTzB3L93wfZWe4Avlt+ErpSeYuzx+wynMnmA2BCwUtj59+qjNAsNeWA5ko98TrFmz5p49e/L+3vDwcFnDIar9+/fr1sb08vK6detWfo8gZThEkTFWpO/mzZve3t7qupH29vYBAQH5ujY5IiJCvyf4zLlf+fXSDd9X7AnmLiYmJu8NX1k9wRfKS8M39/UgpMu94auf87y9vQu6J5gNAQuFLTY2Vm0WrFy50tC1AIqS557gC508eVJ/OOS33357iYPo9wRr1aq1b9++lziITs7hkHyN97xcT/CFTp8+ra45rGbZvMx/l5vzclKHQ9TbS+gW4XzezvrrQTRq1CiPc79eQlhYmG7gUL1wNef3Uuk9wdzl0vAtuJ7gC2Vr+Pr5+f3yyy+6nmBe1oMoCAQsGMBPP/0khLCxsXmJ7+WAXAcOHHBxccl7TzB36nCIbpEqT0/PP/74I4/vlZXzctIfDrG1tc3L/PfIyMhX7Am+0L59+3R35WratGkuw1HPWw9CuocPH+qWNX9mw7cgeoIvtG/fvvr16+ds+F69erXgeoK5i4+P12/4du7cefTo0fo9wYLOec906NAhd3d3oads2bKF2RPMhoAFw+jatasQ4oMPPjB0IXhzvUpPMHf5Gg5RvXpP8IUuX76sPxyybNmyZw4w5BwrKrhVrNThEHWxR3U4JCoqSn+HvKwHIZ3a8FWndZcrV07X8M0296ug18LQl5mZuWLFCl3D19XVtXHjxtnWiC+0YnSioqL69Omjf0fFZs2ayVo36+VotdqAgABTU1ONRuPh4VHIPcFsCFgwjJiYGFtbW43GaOvWvH6/B2RRx4rUxpncsSJ9+sMh6lXuz5z/Lrcn+EL79u1r0KCBborM4cOH9V8NCgpSh9/k9gRzl5SU5OvrW6JECfH3/PdHjx6p/aaC6wm+0O+//964cWP1F2VjY2NnZ6f7pRX0vKLnUZc1193NsPBz3jNt2LDBwcHByspq8ODBRWRmbUpKypUrVwxdBQELhrN27X8bNnzk4KAY+vMBb5aDBw/q9wRv3LhRoKe7evXqM4dDlILsCeZOvcubOkFb7WMGBwefPXtWvyf4wvUgpLt+/Xrfvn3VX5SxsbG6bLdGo/nXv/5lqAyhNnzV+UZCCHNzc0ONFek7d+6cu7t7zZo1165da9hK9CUnJxu6hCKHgAVD6tBBEULp3dvQdeDNUHA9wRcKDg5u0aKFeuo6derMnTv3u+++y9d6ENIlJyf7+/ur8U7H1tZ2wYIFBswQoaGhtWrVUotxdHQsnJ5g7uLj4wcOHNilSxfuQoF80SiKIgADuXFD1KsnkpLE1q2iWzdDV4PXV0ZGxpIlS6ZPn56UlGRpaTlhwoSpU6eqlzsVGkVRNm7cOHXq1OjoaN3GunXrLl68WLfCUOGLi4vr27fvvn37hBANGzbctWuXOiPKsBYvXhwfHz9x4sRC/m8ESETAgoEtXixGjhROTiIsTJQubehq8Jrau3ever1Vnz595s6dq95RwCAyMjJGjRq1cuVKIUTv3r1Xr16t9sIM69SpU4mJibrL2gG8OgIWDEyrFe+9J44cEYMGiVWrDKJxcxMAABbXSURBVF0NXl8+Pj5eXl5t2rQxdCFCCKHeb0p3URiA1w8BC4YXHS3q1RMpKWLXLvH3Hb0AACjGjAxdACCqVBF+fkIIMWSISEgwcDEAALw6AhaKBB8f0by5iIkRU6cauhQAAF4ZLUIUFWFhwt1dpKeLkyfF228buhoAAF6B4a9eAVSurmLmTGFiIjw8DF0KAACvhhYhipAJE8SHH4r33xdnz/61JTRUjBsnhBBTpohjx/7ZMyBA7NhhgAoBAMgLAhaKlqQkceyYGDZMZGUJIcTDhyI0VAghTp8W9+79s9v58+LmTcNUCADACxGwUORUry6cnMTSpYauAwCAl8UcLBRF8+aJli1F9+7/s/HQIZGU9Nfj6GhhuJuLAADwAoxgoSiqWVMMHSp8fP5n44MHIjr6r/8lJxuoMgAA8oARLBRR06YJV1dRo8Y/W3r3Fj16/PX42jWDFAUAQJ4wgoUiytJSfPutmDfvBbtptSIiQqSkFEpNAADkDQELRVfnzqJt29x2UBTRp4/YskV06SLu3CmssgAAeBFWckfR8vSpiI0VFSr89TQ5WSQminLlxN27wtpaWFn9tf3BA2FhIaytRUaGMDUVc+eKOnXEBx8YqmoAAP4HAQvFXnq66NJFrF8v7OwMXQoAAEIIWoQo7lJSxGeficmTSVcAgCKEgIViLC1NNGsmHj0SO3eKEycMXQ0AAH+jRYhiTFFEQsJfjy0thbm5QasBAOBvBCwAAADJaBECAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyQhYAAAAkhGwAAAAJCNgAQAASEbAAgAAkIyABQAAIBkBCwAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgIWClZSUZOgS8qS41AkAKBYIWChAf/75Z40aNZ48eWLoQl4gJCSkYcOGT58+NXQhAIDXBAGr6Lp7927l5zh9+rRut19++aVatWrvv/++/nt/+OGHan9zd3f/+OOPT548qb9DaGho9+7dy5Qpo9FoHB0du3fvfubMGfWlf//73znPmJiYuHPnzufVk5mZ+cwf4fPPP+/SpUuJEiXUpzt27PD09LS1tTUxMalateq0adNSUlJk/sryLCUl5cKFC7qnjRs3NjMzW7hwoUGKAQC8fkwMXQCey9raetq0aerjdevWnT179ptvvlGfVqxYUbfb2rVr79y5ExUVFRYW5urqqm58/PhxVFTU119/bWNjExsbu2bNmtatWx88eLBly5ZCiN9++6179+5VqlTx9fV1dnaOiYlZtWpV8+bNz5w5U7du3djY2LS0NF9fX/1iLCws6tSpo6vH39/f3t5+5MiR6lNjY+Oc9Z8/f37Xrl3h4eHq03nz5n3++edNmjQJCAiwt7e/cOHCggULrl27tmnTJlm/sbw7cuTIv/71r7t376pPjYyMxo4d6+fnN3r0aDMzs8KvBwDwulFQHAwePNjOzi7n9sePH1tYWEyfPt3e3n7q1Km67fPnzxdC3LlzR316//79EiVKDBw4UFGU1NRUR0fHOnXqPH78WLd/VlbW7t271cfdu3evWbNm7vW4urp26tQp932GDh3avHlz9fHVq1dNTEzat2+fkZGh2+HixYsxMTG5H6SALF++3MnJSX9LfHy8hYXFzz//bJB6AACvGUawiretW7empaV98skn9+/f//HHH2fNmqXRaHLuVqZMmYoVK16/fl0IsWPHjtjY2G+++cba2lq3g5GRUYcOHeTWtmPHjuHDh6uPf/zxx8zMzDlz5piY/PN/ubp16+oe37hxw9fX9+jRo0+fPnVzc5s2bVqLFi2EEAEBAXFxcQ0aNPjqq6/S09PPnj3r6enp7++/ZcuWPXv29OrVa968eVqtdv78+evXr793717FihXHjh3bt29f9bBXrlzx8/MLCQnJyspq1arV7NmznZycWrRocf/+/YcPH3p4eAghJk6c2KtXL1tb22bNmu3YsaNPnz5yfw8Asjl48ODevXtbtGjx4Ycf6jauX7/+xo0bU6dOzbn/w4cP/+///k99bGdn5+zs7OnpaW9vL4TYvXv34cOHhRDGxsblypVzcXFp3bq1kZGREGLdunURERHZDuXn53fq1KlffvlFfVqyZMny5ct7enrq2gIxMTHz588fNGhQvXr11C1ZWVnbt28/duxYUlKSvb19586dmzZtKoTYtm3b+fPnc1Zbs2ZNrVYbExMzefJk3ca7d+9u2bLlypUrWVlZVatW7dKlS82aNdWXwsPDV61aVb9+/Y8//li3f0xMzMKFCwcOHFinTp08/lZR5Bg64SFPnjeC1b59+0aNGimK8vvvvwshjhw5om7PNoIVHR1tZmY2ePBgRVHUf/N//vnn886ldg+v6omLi8u2zwtHsK5duyaE2LNnj/rUy8vLzMwsKyvrmTvHxsaWL1++SpUq33///caNG9977z1TU9OjR48qijJ8+HBra+u6det+9dVX/v7+iqKYmJjY2tr2799/zpw5QUFBiqKMGDHC1NR0xowZv/76a//+/TUajfreGzdulC5dulatWj/88MPatWvd3d0/+eSTjIyMZcuWvfPOO9bW1suWLVu2bNmlS5fUMiZNmlSxYsVcfigAUrRu3VoIUbt2bf2NH3/8sbOz8zP3V3NS5cqV3d3d69WrZ25ubmNjExwcrCjKlClThBDu7u7u7u7Ozs5CiPfeey89PV1RlHbt2llaWnr8L61Wu3TpUiFE3bp11XeVKlXK1NR08uTJ6geUOht1y5Yt6qlv377doEEDIYSbm5uXl5eLi4sQYtSoUVqt1s/Pr2nTpk2bNm3YsKEQomrVqupTHx+fnj17Vq9eXVf/jz/+aGlpaWdn5+Xl1aVLl3LlyhkbG/v6+qqv7ty5UwhhZmZ2+fJl3VtCQ0OFEL/++qvEXzsKGQGreHhmwHrw4IGJicmcOXMURdFqtc7OzsOGDVNfUgPWtGnTAgICRo4cWaZMGVtb27CwMEVRvL29hRCJiYn6h8rKyoqMjIyPj1cUpXv37tlS+MSJE7Od+oUB68CBA0II3edFs2bNnvfRqSjKrFmzjIyMdJnv6dOnlSpVat++vaIow4cPt7S0fPjwoW5nExOTAQMG6J7evXvX2Nh48uTJ6tOMjIwKFSr06dNHURQfHx9zc/ObN2+qL6WmpmZmZqqPhw0blq1FqCjKokWLjIyMdPsAKAgxMTFGRkZeXl5CiNDQUN32FwasZcuWqU9v3LhRvnz5+vXrK38HLN2Xt++++04IsWDBAkVR2rVr5+HhkfNoasCKiopSn6alpX3xxRdCiFmzZik5AlaLFi1KlCixY8cO3dvXrFnj6+ur1WqfV56iKPoB6/Tp0yYmJl26dNF96qanp48ZM0YIsXbtWuXvgOXg4PDee+/pDkvAeg1wFWExtmXLlszMTA8Pj6ioqOjo6DZt2mzevDk9PV23w/bt2zdt2rR48eKqVauePXtW/e5lbm4uhMh2+V5KSkq1atUWLVqkPnV2dr6kx8fHJ7+1PXr0SAih60Kam5snJyc/b+cLFy5Ur169du3a6lMzM7MOHTr88ccf6lMrK6vSpUvr768/xz8sLCwrK+vChQvDhg0bNmzYiBEjhBDqzPrz5883atRIt3OJEiWeORlfx87OTqvVqpUDKCAbN24UQixZssTJyWn9+vUvcYRKlSr16NHj4sWLqamp2V7y9vYuWbJkSEhI3o9mbm4+c+bMjz76aM6cOYmJifovHT169Pjx4xMnTuzUqZNu44ABA/z8/J45GeOZvvrqq1KlSq1Zs6ZUqVLqFlNT0/nz59erV2/WrFm63aZNm3bo0KEff/wx75WjiCNgFWM///yzEKJt27bqcgw//PDDo0ePdu3apdth9+7dZ86c+fjjjy9fvmxhYaFurFy5shBCnY/1PGZmZq56ypYtm9/a1LlWWVlZupM+fPjweRnr4cOHNjY2+lvs7OwePXqk1WpfeKK4uDghRJkyZez+1q9fv549ewohHj16lO2wucvIyBBCmJqa5v0tAPJLnQZQsWLFXr16bdiwQfcpkS9xcXEWFhbq10V9KSkpaWlpuo+7vOvVq1dSUtLx48f1Nx46dEgI0bVr15eoUKXVavfv3//ee+/pz3kVQhgZGX300UdXr169ceOGuuXdd9/t1q2bj4+P+pmG1wABq7iKiYk5fvz4rFmzIvW4ubnl/AL09ddfa7Xa6dOnq089PT2FEAW9OII6/zQhIUF3UkVRNm/e/MydnZ2db968qR+noqKinJ2d1ZmquVNnXXTt2jVAj9o1cHZ2jo6OznvNCQkJJiYm+cpkAPIlOjr61KlTvXv3FkL069fv7t27Bw8ezON74+LioqKi/vzzz2+//Xbjxo2dOnXSjUlHRUVFRUWFhIR8/PHHmZmZ6vGFEFeuXGmhZ+3atc87eJUqVUSOb563bt0Sf3/IvJz4+PjExET1a202Ob/rzps3LzU19ZnT/FEcEbCKqw0bNgghPv3006p6Pvnkk99++00Xa1Tly5efOnXqqlWr1GHz+vXre3l5LVq0SD+K5WWsKF+qV68uhFCnugshevToUbNmzc8///zo0aO6fa5evaoGoLZt296/f1/32Xf58uXt27erQfCF3NzcHB0dv/nmG9067CkpKWrjoG3btuHh4Vu2bNHtrGso2NvbJyQkZOuTXr16tUaNGnkf+QeQXxs2bDAxMenWrZsQokmTJjVr1sx7l3DatGnVqlVzcXGZMGFCjx49VqxYoXupRo0a1apVa9q06blz53744Yd27dqp20uWLNlcT4UKFZ53cHU95GxrJqujaznHyfJOPcIzV9fLeUZnZ+epU6euXLky27rQKKYIWMXVxo0b33333XLlyulv7NevX0ZGhu4KZJ3x48dXr159xIgRapBavXp18+bN+/fvX7169U6dOr3zzjuVK1c2MzPTTVeKiorKZe34vHjrrbdq1qypmwlhYWERFBTk4ODQunXr+vXrd+zY0dXVtXbt2qtXrxZC9OnTp1evXoMHD/b09OzWrVvTpk2rV68+c+bMvJyoRIkSq1atCg4OdnFx6du3b8eOHZ2cnH766SchxLBhwzp27Ni7d++2bdv27dvXxcVl7Nix6rt69OiRlZXVpEmT9u3bq1FVCBEcHKxe3ASggGzcuNHY2LhDhw7qNX1xcXHbtm3LOZXqmaZNmxYaGhoVFZWamvrzzz/b2trqXjp16lRwcPBbb71Vr169Tz/9VLe9QoUK/6enTZs2zzu4OpLk5OSkv9HBwUH8PY71cuzs7ExNTW/fvp3zJbU5mO2MEyZMqFmz5siRI6V/6UXhYx2s4qFXr16NGjXSPU1PT+/Ro4e6UpS+cuXKfffddw4ODnXr1g0ICNDNqTQzM1u3bt2hQ4fu3r1bvnx5e3v7Q4cOHTp06MiRI48ePXJ0dBw6dGinTp3Uvt7AgQObNWuW88j6TydMmJBtSkFO3bt337Rp05w5c9QxoVq1al26dOm3334LDQ1NTk5u165d48aNW7VqJYTQaDQbNmzYt2/fkSNHnj592rVr1169eqnfGj/66CPd5HfV7Nmz3377bf0tH3zwwZUrVzZv3nz//v2GDRv6+fmpO5iYmOzYsWPv3r3BwcEajaZLly66RXcaNmx45syZoKAgMzMzdTWsqKioc+fOzZkzJ/cfCsBL+/PPP8+fP//pp5/WqlVL3ZKWlubv7x8UFJSX9ecqVark7u7+zJfc3d2NjIy++uqrQYMGbd++XX95rTw6cuSIsbFx8+bN79+/r9vYpEkTIURwcHCNGjXye0CVqampu7v7qVOnFEXJNjoeEhJia2tbu3btmzdv6jaq9+xq167dy03/R9Fi4KsY8fq6deuWiYnJoUOHDF1InkyfPt3FxUX/0msAcs2YMcPCwiIhIUF/49tvv+3l5aXkZ5kGffrLNGi12latWlWtWvXJkydKnpdpUBTlxIkTFhYWffv2Vf53mYYnT544OTlVq1YtNjZW/wjZPihyX6ZBHadfvHix/lsOHDhgZGQ0adIk5e9lGs6dO6d7tXfv3mpXkWUaijVGsFBQKlSoMGTIkK+//vrdd981dC0vkJKSsnTp0sWLFzMBCyg4mzdv7tSpU7brSPr16+fj4/PgwYNXP75GowkMDGzcuPG8efPUG6feuXMn221VdbeX+O6770qXLp2ZmXnx4sVt27bVr18/5+3eLSwsVq9e3aVLl4YNG3p7e9eqVevevXt79+6tVKnSkiVL8ljVgAEDdu7cOWrUqJCQkI4dO5qbmx89enTp0qUeHh4zZsx45lsCAwN3796tv+YOiiMCFgrQl19+2a1btydPnqjTOYusiIiI5s2bq4s7ACgI169ft7S0HDRoULbtvXv3Xrt2bXBwcKVKleLj45/5XnNzc3d3d0dHx5wvlStXTr9v2KhRo4kTJ+7Zs2fUqFFVqlS5e/futm3b9Pfv16+fo6Oju7u7uhiyRqOpUKHCokWLBg0apA4alShRom7duroU2KFDh1OnTn399dcrV658/Pixk5OTm5tbr169spVXt25d/eX6KlWqpJtYptFofvrpp7Zt2/7www/btm3LysqqVq3aF198MW7cOEtLSyGEjY2Nu7u7+ljl5OQ0Z86cFStW6M8zQ7GjURTF0DUAAAC8VriKEAAAQDICFgAAgGQELAAAAMkIWAAAAJIRsAAAACQjYAEAAEhGwAIAAJCMgAUAACAZAQsAAEAyAhYAAIBkBCwAAADJCFgAAACSEbAAAAAkI2ABAABIRsACAACQjIAFAAAgGQELAABAMgIWAACAZAQsAAAAyf4f0VDJzazqr2QAAAD9elRYdHJka2l0UEtMIHJka2l0IDIwMjUuMDkuMwAAeJx7v2/tPQYgEABiJgYI4IbiBkY2hgQgzcjM5qABpJlZ2BwyQDQzIxIDKgNWyYzQgaaTnQGsgZEJrpObgZGBkSmDiYk5gZklg4mFlYGVjYGVnYGFI4GDU4GDK4OJizHBiRGomI2Ri4OFmUm8D2QiA8yR8y6XHNiqvNUGxFH9JXJAy+qmLYgdkHl5f8j3HfYg9utNsvsyth0Gs60ea9lruvfYgdgek7gcKp23g9muavYOdctT94PYR66bOvSb3ASr/7iWZ8/zwA6wuK1sgL1TtcgBENv/zIH9vwUmg8XFAEQcN6XkaWyNAAABSnpUWHRNT0wgcmRraXQgMjAyNS4wOS4zAAB4nH2SXW7DMAjH33MKLjALMMbmsWmqaZqaSlu3O+x999cgUee0i2abCJOfP/jjAaK9Ta9f3/DbeBoGAPxnmBl8ZkQczhAOjKfnlxmO18N4ixwvH/P1HYhiYPR79nC9nG8RgiM85dS4CClgQmYiDmdpfSkHyClzrdr8P6Hi4jyCOUBKWsgHUBIpKHugBOgbFUHMDhY2RNkBi4OYVGvxCSau2RrucOocpzi2xnm5VMt1h6sLZ02r6+M3aFax7YFtAZsQm/gF1dRWkR44WzNBbKVpJI8qUae/JCHMHm6xo656InLeI2nVsaByLuGRNCt7x5/m6a6ma5XHyzz1KkfnXktxy71iEtbrEr109clNu8jkVruWMW1dMV8J1oWhsG36EgHaZCnx4W0u25vH/Pa23R9+ADw3mZfF8NYdAAAArHpUWHRTTUlMRVMgcmRraXQgMjAyNS4wOS4zAAB4nB2OOw4DMQhEr5LSKxEEmK+2dJ9L+Bp7+LBuH/NmWJv33mONda1rj9+1+fOM78QUUwFCEmHmhPsrOCUimjE5+YsY3dgNGFWNrEkfTYlmI5OCm9A9bLYkMasdwRa6j3Ba1IyXVHowtJsVlAelcuvdX15yeonS0mdBr1JvdnV2Rh3OZ0RyHjJy0TfDmuVwPX/xrS41sEt4DAAAASh6VFh0cmRraXRQS0wxIHJka2l0IDIwMjUuMDkuMwAAeJx7v2/tPQYgEABiJgYIEITyGxjZGBKANCMzhGZiog/NDLeXQ0EJSP9n5GZgZGBkYmBiZmBmYWBhZWBlY2BjZ2DnYODgZODkYuDiZuDmYeDhZeDlY+DjZ+ATYHAC+UP8GsgQBpivru9Wdnzk+sgOxPmmwe+op79yH4idF/7FwdvuNFh8reFphyNmB8DieuYLHbaabQGL31QqcIi2uw4Wv6Mr4yCnPxcsvvHIZPuPru/A4l5aivs61NvA4v57duw/soxpP4itGmxw4JF9HFg8xL/xQL8WH1j8sufqA1sNDMDiU/KvHIhaJwkWF876d6BDkQks/uq48EE1I1Ww+AW7/wcu21ywB7HFACluTmiZQPozAAABwHpUWHRNT0wxIHJka2l0IDIwMjUuMDkuMwAAeJx9VMtu3DAMvPsr9AMW+KZ06CG7G6RFkV2g3fYfeu//o6SCjRxEqGwSsjQam5yBt5Ljx+X7n7/lfdBl20qB/9y99/KbAWB7LTkpp+eXb9dyvj+dHivn26/r/WdBL2hxJq6P2Kf77fWxguVcECoxs/YCVQQ75wTGmEcpgK12cQUsO1RmEG4LIAfQq7Gac2xzt26wwEngrLL0hjIIXQVXhBpArcDemyWhYm8rQgscV1dC8iQUAu4rQg8gRaVAPStl8LH9CdeyNxVVAjoITdl9AewBjH1U55YvJCOgFSNCIjHezSTjIxWBdcWJqcxOUY+Ca5Kig+AKmdLsUkGg82ilqjIvSVOcXaOH5EYBQCewJWnKs1vo2KKYJDUAxiVpCrR77QoiWX70H9qSNCXae7jNoumDVJS6raBeboPUTdLI1ciaHpHl/PXly46PA8/Xywdjv1n9dLteptXzomnoeCg8bYsRMt2JETo9iBE2rYYRPg2FEW36BiP6dAdmHE2AI+FBaxyJDpriSHyQDkeSg0Q4kh6UwJHs0PG3FT/26diVfH78PGK+/QNEc9pvXaWK3AAAAN56VFh0U01JTEVTMSByZGtpdCAyMDI1LjA5LjMAAHicVdDLjUMxCAXQVmaZSA4CLh+jt3wFTAGjdJLiA96NV9bVwYDv+9953M+/39f75/MQJgVsMZlJoda1qSx9vZgANux1JQU8sg0qKtYVBKstx6SbtHFiZNUYl+oAlK6CMaaMibR7MIZwrktI3MwOCEd21FfxxN5tNJS1I+ki6JnHheHDtN92noFUkk06MmLjOu3cHRjmPYtmNJNUjmHRq+zQYcEMGZZUzubDNHkPq/6UwFkwzHV2HpZhuoRCY8d6fr7uc0T2e2+I9wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# 1. GPU MEMORY CLEANUP\n",
        "# This wipes away the \"ghosts\" of previous runs to make room for 10,000 reactions\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# 2. LOAD 10,000 REACTIONS\n",
        "# We are doubling the data to force the AI to learn new chemical \"words\"\n",
        "print(\"📂 Loading 10,000 reactions from dataset...\")\n",
        "df_forward = pd.read_json(forward_file, lines=True, nrows=10000)\n",
        "print(f\"✅ Success: {len(df_forward)} reactions loaded.\")\n",
        "\n",
        "# 3. REFRESH DATA LOADERS\n",
        "dataset = ChemicalDataset(df_forward, stoi)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# 4. START THE SPRINT (10 Epochs)\n",
        "print(f\"🔥 SPRINT STARTING: Training for 10 epochs on unseen diversity...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"📡 Sprint Epoch {epoch+1}/10 | Current Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 5. SAVE THE SPRINT PROGRESS\n",
        "torch.save(model.state_dict(), 'chemist_model_sprint.pth')\n",
        "print(\"\\n🏆 SPRINT COMPLETE! Model weights saved.\")\n",
        "\n",
        "# 6. --- THE BLIND TEST ---\n",
        "# We test on row #5005, which the model has NEVER seen until this sprint\n",
        "print(\"\\n🧪 PERFORMING BLIND TEST ON UNSEEN REACTION #5005...\")\n",
        "visualize_prediction(5005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "Vdbjm7sxgXMi",
        "outputId": "32bce6ad-373c-4665-9027-b917b4ec7b33"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Loading 10,000 reactions from dataset...\n",
            "✅ Success: 10000 reactions loaded.\n",
            "🔥 SPRINT STARTING: Training for 10 epochs on unseen diversity...\n",
            "📡 Sprint Epoch 1/10 | Current Loss: 1.5748\n",
            "📡 Sprint Epoch 2/10 | Current Loss: 1.5668\n",
            "📡 Sprint Epoch 3/10 | Current Loss: 1.5611\n",
            "📡 Sprint Epoch 4/10 | Current Loss: 1.5546\n",
            "📡 Sprint Epoch 5/10 | Current Loss: 1.5498\n",
            "📡 Sprint Epoch 6/10 | Current Loss: 1.5454\n",
            "📡 Sprint Epoch 7/10 | Current Loss: 1.5401\n",
            "📡 Sprint Epoch 8/10 | Current Loss: 1.5346\n",
            "📡 Sprint Epoch 9/10 | Current Loss: 1.5311\n",
            "📡 Sprint Epoch 10/10 | Current Loss: 1.5257\n",
            "\n",
            "🏆 SPRINT COMPLETE! Model weights saved.\n",
            "\n",
            "🧪 PERFORMING BLIND TEST ON UNSEEN REACTION #5005...\n",
            "🔬 Visual Audit: Reaction #5005\n",
            "Reactant Sequence: [Br][C][C][O][C][=C][C][=C][Branch2][Ring1][Branch1][O][C][C...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAGQCAIAAADZR5NjAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdd1yT1/4H8G8S9lBARVHcu1oUAa3iqgRHxVmpE65Wi6OKq4rWtlSrFbUq2lqF9raCC8G9oIAsGQ5wb4uKisgQZYlAyPn9cdr8uA6E5EkYft6v+4eXJt/nRMOTT85znu8RMcYIAAAAAIQjruoBAAAAANQ2CFgAAAAAAkPAAgAAABAYAhYAAACAwBCwAAAAAASGgAUAAAAgMAQsAAAAAIEhYAEAAAAIDAELAAAAQGAIWAAAAAACQ8ACAAAAEBgCFgAAAIDAELAAAAAABIaABQAAACAwBCwAAAAAgSFgAQAAAAgMAQsAAABAYAhYAAAAAAJDwAIAAAAQGAIWAAAAgMAQsAAAAAAEhoAFAAAAIDAELAAAAACBIWABAAAACAwBCwAAAEBgCFgAAAAAAkPAAgAAABAYAhYAAACAwBCwAAAAAASGgAUAAAAgMAQsAAAAAIEhYAEAAAAIDAELAAAAQGAIWAAAAAACQ8ACAAAAEBgCFgAAAIDAELAAAAAABIaABQAAACAwBCwAAAAAgSFgAQAAAAgMAQsAAABAYAhYAAAAAAJDwAIAAAAQGAIWAAAAgMAQsAAAAAAEhoAFAAAAIDAELAAAAACBIWABAAAACAwBCwAAAEBgCFgAAAAAAkPAAgAAABAYAhYAAACAwBCwAAAAAASGgAUAAAAgMAQsAAAAAIEhYAEAAAAIDAELAAAAQGAIWAAAAAACQ8ACAAAAEBgCFgAAAIDAELAAAAAABIaABQAAACAwBCwAAAAAgSFgAQAAAAgMAQsAAABAYAhYAAAAAAJDwAIAAAAQGAIWAAAAgMAQsAAAAAAEhoAFAAAAIDAELAAAAACBIWABAAAACAwBCwAAAEBgCFgAAAAAAkPAAgAAABAYAhYAAACAwBCwAAAAAASGgAUAAAAgMAQsAAAAAIFpVfUAAACgBnvw4MGiRYv09PQsLCwq/qzs7OzMzMwVK1Z8+OGH6hsbQBUSMcaqegwAAFAjlZSUNG3aND09XbmnGxkZPXz40MTERNhRAVQHmMECAAAlzZ49Oz093cDAwNXVtUWLFhV/YkZGxu+//56bm+vi4nL48GGxGOtVoLbBDBYAACjDx8dnxowZenp6J06caNOmTdOmTSv+3KysrOTk5GHDhmVmZn777bcrVqxQ3zgBqgS+NAAAQKWdPn167ty5RLRly5b169fb2tqePn26gs+9ceOGnZ3d7Nmz/fz8tLS0Vq5cuW/fPnUOFqAKIGABAEDlpKenjxkzpqioaN68effu3Tt+/LhMJjM3N6/g0xs1aqSlpZWYmLh79+7Vq1czxqZMmXLt2jW1jhlAwxCwAACgEkpKSj777LPU1FR7e/vevXuvWrVKIpHs3r27VatWFaxgamp64MABQ0PDnTt36urqjhs3Lj8/f/To0c+fP1fryAE0CQELAAAqYe7cuTExMU2bNl27du3UqVMZY2vWrBk0aFClinz44Yf+/v4ikWjBggVTpkzp1q3b7du3XV1d5XK5moYNoGEIWAAAUFE7duzYunWrnp7ejh07vvjii5ycnPHjxy9cuFCJUqNHj160aJFMJnN1dd2yZUv9+vWPHj2qsdXu8fHx/fv3z8zM1Mzh4D2EuwgBAKBCLly4YG9vX1hY+Ntvv4WEhOzfv9/Kyio+Pt7Q0FC5gnK53MnJKTg42NraeuXKlcOHD5fL5UFBQZ9++qmwIy/r+fPnS5cu9fX1lcvlHh4eXl5e6jsWvM8QsAAA4N0yMjJsbW0fPnz45Zdf1q9ff/ny5aampufOnWvdurUqZbOzs7t3756cnOzi4tKpU6clS5YYGxufPn36gw8+EGrkZR09enTWrFmPHj3S1tZesGBBnz59TExM7O3t1XEseM/hEiEAALyDTCYbO3bsw4cPe/bs6eDg8MMPP4jF4l27dqmYrojIzMyML3jfsWOHkZHR2LFj8/LyRo8enZOTI8jIFVJTUz/99NPhw4c/evTI3t4+Kirq2bNnw4YN+/zzz1++fCnssQAIndwBAOCdFi5cGBUV1ahRo7Vr1w4bNkwul3t5eQ0ZMkSQ4lZWVv7+/mPGjJk/f/6xY8fu3Llz/vx5V1fXgwcPCtLhXSaTbdmy5dtvv83LyzMxMfH09DQ1NR0xYkRWVpa+vv748ePRRx7UggEAALzdzp07iUhbWzs0NLRTp05ENGrUKLlcLuxR+Er5hg0bJiQk1K9fn4iWL1+uetkLFy7Y2dnxzzsnJ6dTp05JpVL+f/v373/z5k3VDwHwRghYAADwVrt379bV1SUiHx8fZ2dnIurQoUNOTo7gB5LJZIMHDyaiHj16HD9+XCKRiMXiDRs2FBQUKFewoKDAw8NDIpEQUcuWLY8cOeLl5aWnp8djnJ+fn7DjB3gFFrkDAMBbdejQ4datWwMGDBg3bpybm5uJicm5c+fatGmjjmNlZWXZ2dndv3//m2++EYlEmzZtys/PF4vF7dq16927t729fZ8+fVq2bFmRUseOHZs9e3ZKSoqWltasWbOcnJzmzZt3/fp1kUg0adKkDRs28EkyAPXBGiwAAHgrPn3Vv39/FxeXhISEMWPGqCldEVH9+vUPHDjw448/fvXVVzExMbm5uUZGRoWFhdevX79+/bqvry8RNWvWzN7evmfPnvb29lZWVlpar36KpaWleXh47Nixg4isra3Xr19/6NChwYMHy+XyNm3abNu2zcHBQU3jBygLM1gAAPBWW7ZsmT17touLi7+/vyaP+/XXX69evXrp0qXLli27cOFCXFxcbGxsQkLC06dPFY8xNDTs2rUrn9zq1auXmZnZb7/9tmjRotzcXAMDg++++65Dhw5ffvllamoqb8qwfPlynhcBNAABCwAA3urChQvdunVr3br133//rcnj9u/fPzo6+siRI8OGDVP8UC6XX79+PT4+Pi4uLiEh4c6dO4r/JBaLTU1NefySSqUrVqxYsWJFSEgIEfXp08fHx6djx46aHD8AAhYAALxVaWmpmZlZbm7u48ePLSwsNHPQkpISExOTwsLC9PT0Bg0avO1hOTk5586di42NjYuLi4uLKyws5D/v1KlTSkpKfn6+iYnJ999/P2fOHDRiAM1DwAIAgPI4OjqGh4fv379/9OjRmjni2bNne/To0aFDhxs3blTwKUVFRUlJSa6ursnJyfwnEydO3LBhg7m5udqGCVAehHoAAChPr169iCg+Pl5jR+TH4setIF1d3V69evE1702aNAkJCdm5cyfSFVQh3EUIAADlqREBi4iePn16+/ZtPT29a9eu1a1bVz1DA6gozGABAEB5evbsKZFIkpKSFIuc1C0hIYEqH7Di4+MZYz169EC6guoAAQsAAMpTp06dTp06FRcXJyUlaeBwKSkpjx49MjU1bd++faWeqFwsA1ATBCwAAHgHe3t7IoqLi9PAsRTXByt7659yFxYB1AQBCwAA3kGTy7CUy0klJSXnzp0TiUQfffSResYFUDkIWAAA8A6KgKWBzj7KBawLFy68ePGiffv22GQQqgkELAAAeIdWrVpZWFhkZWWVbZ6uDgUFBZcvX9bS0rK1ta3UE3F9EKobBCwAAHg3nl3UvQzr9OnTMpnM2trayMioUk9EwILqBgELAADerewyrBcvXpw5c0YdR1E6J+EWQqhuELAAAODdys5g/f777x999FHv3r2PHj0q7FGUC1hKd3YAUB8ELAAAeDcbGxsdHZ0bN27s3bu3tLS0Tp06cXFxw4cPt7e3P378uCCL3+/duxcbG0tE2dnZqampFX+i0p0dANQH70WA2ujPP2nyZJo+nZYureqhQC2hra3dvHlzIho/fnxERMThw4e9vb0bNWoUHx/v5OTUtWtXf3//0tJS5YrfvXt3+vTp7du3LyoqIqKZM2daWlo2btx42LBha9asiY2NLS4uLufpWIAF1ZBIA/fcAoBGZWXRrFkUGEhE9N135OBA/fpV9ZigNrhx48aXX36ZkJDw8uVLIho8ePDChQuvXbu2bt06PuHUunXrxYsXT5kyRVtbu4I1L1269OOPP+7bt08ul2tpaTHGSktLHR0dz549m5OTo3iYkZGRnZ2dvb19z549e/XqZWJiUraIjY3N+fPnIyMj+/fvL9irBVANAhZArZOQQGFh9N13RERHj9LDhzRrVlWPCWqPzMzMLVu2eHt78wBkb2+/cOHC3NzcVatW8SYOzZs3nz9/vpubm76+fjl1Lly4sHr16n379jHGdHR0xo4d++233w4ZMiQ5OfnWrVvt2rW7e/dubGxsXFxcbGzsjRs3yn5atWrVyt7evnfv3vb29s2bNzcxMZHL5eHh4QMGDFD3yweoIAQsgFonJYV++IF+/52I6JdfqGlTGjGiqscEtU1ubu7WrVvXrl2bnZ1NRF26dJk/f76ent7y5ctv3LhBRObm5jNnzlywYEGdOnVeeW5sbOyaNWuOHTtGRIaGhlOnTl20aJGlpSURffzxx1FRUeHh4Q4ODmWfkpGRkZCQEBcXFx8fn5SUxKfQOD09Pf5/raysEhMTKz55BqBWCFgAtdHcudSoEZmaUmgo7d1L+MgB9cjPz//vf/+7du3ax48fE1GnTp0WLVqkq6u7Zs2aixcvEpGtre25c+cUj4+NjfX09IyIiCAiY2PjKVOmLFmyxMLCQvEAV1fXHTt2/Pnnn5MnT37bQWUy2aVLl2JjY8+dO7d3716ZTEZEBgYGL168WLdu3VdffaW2lwtQGQwAaqXr11liIouPZ4sWscjIqh4N1GYvX7708fFp2rQp/1hp2bKlt7f38ePHe/bs+euvvzLGSktLjxw5Ymdnxx9Qr149T0/P7Ozs10t9/fXXRLRixYoKHnrFihVEZG5uvmHDBiIyMDBITk4W8rUBKAt3EQLURsnJNHs2LV5MJ0/SunW0b19VDwhqM11dXTc3t+TkZD8/v/bt29+7d2/evHkzZ84cO3bsxIkTg4KCPvzww+HDh587d87c3NzT0zM5Ofn77783NTV9vRRPaQ8fPqzgoT08PD744IOMjIycnJzx48e/ePHiyy+/FPK1ASgLlwgBaqO8PKpXjxijkBCSSqldO7p1q6rHBO+F0tLSvXv3rl69+urVq0RkZmbGF2k1b9588eLFn3/+uZ6eXjlPP378uJOT0+DBg4ODgyt4xNjY2L59+2pra0dERIwcOTIrKysgIGDs2LGqvxYAVWAGC6A2MjamHj1IJqP8fDI1pdu36f79qh4TvBckEsmECRMuX7585MiRNm3aSCSSevXqeXt737p1a9asWeWnK6r8DBYR9e7de9q0acXFxUuXLv3xxx+JaO7cuc+ePVPlVQCoDgELoJaSSomIIiKIdwaKiKjS0cD7RSQSDRs2bPbs2ZmZmRMnTpw7d66urm5FnsgD1oMHDyp1uHXr1jVu3PjUqVMikWjAgAHp6elLlixRZtwAwkHAAqileMAKD///PwBUBZFIVPEHm5qaGhkZ5eXl5ebmVvxZdevWXb9+PRF5eHj88MMPenp6v/32W2RkZKXHCiAcBCyAWqpHD6pbl65fp86diYjCwkguF/wg+fn5P/30k1wNleG9xRtiVeoqIRGNGzfOyckpOzt7y5YtixcvZozNnDmTb7wDUCUQsABqKS2tf3bIuXePmjdnxcW5V68Ke4T8/PxPPvlk0aJFS7HjIQhHiWVY3C+//GJkZLR7925ra+uOHTveunVr3bp1ahggQIUgYAHUWqUDB+Z06RJ//fqvY8YYvHjh89dfAhYvKCgYPnz4qVOnmjZt6ubmJmBleM8pHbCaN2/+3XffEdG8efO8vb1FItHKlStv3rwp/BABKgABC6DWuj1ggMmlS6P9/ExtbF7KZCdPnhSqckFBgZOTU2RkZNOmTSMjI1u3bi1UZQAesB49eqTEc+fPn29tbZ2SkhIRETF58uSioqIZM2agGxFUCQQsgFqrY8eOlpaW6enplpaWYrE4JiamsLBQ9bI8XUVFRSFdgTooPYNFRFpaWj4+PhKJZP369a6urubm5tHR0f7+/kKPEeDdELAAajO+Y25iYqKVlVVhYWFCQoKqFfPzL82YcebMGaQrUBNVAhYR2dnZzZo1SyaTLV68mK/BWrhwYUZGhpBDBKgABCyA2szR0ZGIwsPD+R9UvUqYn0+ffNJr585jffpERUUhXUH5eIOGyl6h4x2zzp8/r/RxV61aZWlpee7cuby8PEdHx6dPny5atEjpagDKQcACqM2kUqlIJIqOju7bty8RhYWFKV+roICGDaNTp6hp0wG//tqqVSvBRgnwr9u3b8+YMUMkEqnSYcHY2Hjz5s1E9PXXX3t6eurr68fFxeXk5Ag3TIB3Q8ACqM0aNmzYuXPngoKCKVOmiESipKSkyMhIZdpWFRSQkxNFRVHTphQZSZi7AjXYsWOHjY3NzZs3LSws/vzzT1VKjRo1auTIkbm5uZs2bTpx4sSVK1fq1q0r1DgBKkKrqgcAAOplZ2d35cqVrKwsImKMDRgwoF69egMGDJBKpY6Oji1btnx3CaQrULPCwsIlS5bwaacxY8b89ttvJiYmKtb8+eefCwoKvvnmGysrKyHGCFA5CFgAtdn9+/f/+usvIvrwww/nzJmTmJgYFhZ27969oKCgoKAgInrm6GjSpg1JpTRgAL3tIy0lha5coWbNKDKScGUQhHb9+vWxY8devXpVX19/9erVc+fOFaSspaVlaGioIKUAlICABVBr3b9//+OPP05NTe3du/eJEyeMjY2/+OILIkpOTg4PDw8LC7t3/rxJRASFhdHWrSSRUNeuJJWSVEp9+tCTJ7R2LYlE1LIlzZ9PERFkZIR0BYLz9/efOXPmixcvOnbsuHfv3g8//LCqRwQgDAQsgNqJp6v79+/37t07ODjYyMhI8Z9at27dunXr6dOnU2kpJSVReDiFhVFCAiUlUVISrVlDq1dTXBz9/js1bEgbN9KePTRxYhW+FqiVcnNzp0+fHhAQQEQuLi7btm0zMDCo6kEBCAaL3AFqob///rtPnz5vTFf/QyKh7t3p668pMpKePqUTJ2jBArKyor59SSymhg2JiEaPppgYTQ4e3geJiYndunULCAioU6fO7t27/f39ka6glsEMFkBtc+fOnQEDBjx69Ogd6eoVhoY0ZAgNGUJEJJdTcfE/P8/LI2NjdY0VarXIyEgiun37dtkfMsY2b968ePHi4uJiGxubgICANm3aVNEAAdQIM1gAtcqdO3c+/vjjSqerV4jF1KYN7d5NN2/SqlU0aZLQw4RaLi0tbfr06YcPHyYiU1NTxc+zsrKGDRs2b968kpISd3f3+Ph4pCuorUTYBROgmti0adPWrVs7d+48c+ZMe3t7PT29ylbg6Yqvalc+XXFyOR06RI8ekaMjdeyofB14zxQVFW3cuHHVqlX5+fm6urp9+/bdv3+/sbExEZ04ccLNzS01NbVevXrbt293cnKq6sECqBECFkC1EBISMmzYMJlMxv+vvr6+vb29VCqVSqXW1tZi8bsnm4VMVwBKOXr06Pz585OTk4nIycnJ29tbsZ/SunXrPDw8GGN9+/bdtWuXpaVllY4UQO0QsACq3l9//TVy5MiXL19269atb9++MTExFy9eVPRbb9CggYODg6Ojo1Qqbdas2RsrIF1B1bp58+aCBQuCg4OJqEOHDhs2bBjC1/P9y87OLjExsUOHDlevXpVIJFU0TADNQcACqGKKdOXm5rZt2za+P25WVlZkZOQ/3aru3VM8uFWrVtJ/KZa2IF1BFXr27Nn333//66+/ymQyU1NTT0/PL7/8Ukvr1Tuoli1b9uOPPy5btqxJkyZRUVEeHh7dunWrkgEDaAYCFkBVUqSr6dOnb926laerV9y5cycsLCw8PDwyMvL58+f8h1paWnZ2do6Ojh06dFi0aFFqamqfPn1OnDiBdAUaI5PJ/vjjj2+++SYzM1NLS+vzzz9fuXJlgwYN3vjgP//88/PPP3d1dS0tLd21a5efn5+rq6uGBwygSWjTAFBlKpKuiKht27Zt27adNWtWaWnpxYsXw8PDw8PDT506lZCQkJCQIBKJGGP9+/c/duyYoaGhhl8CvLciIiLmzZt35coVIhowYIC3t3f5TdibNm1KRA8fPuzRowf/g2bGCVBV0KYBoGpUMF2VJZFIbGxsPDw8wsLCsrKyjh07Nm/ePB6qNm3ahHQFmvH3339/9tlnDg4OV65cadOmTWBg4MmTJ9+5xY0iYPE/PHr0SBNjBag6mMECqAJKpKtXGBkZDR06dOjQoU+ePAkICIiLi7OyslLHUAEUioqKxo4de/z4cZlMZmxsvGzZsnnz5unq6lbkuYpcxe8fxAwW1HqYwQLQNNXTVVlSqZSIwsPDBRodwFvduHHj8OHDpaWlEyZMuH37toeHRwXTFREZGBiYmZm9fPmS98RCwIJaDzNYABolbLoiIkdHRyKKiIgoLS3F3e+gVrz5bb169Xbt2qXE05s2bZqdnc3f8whYUOthBgtAcwRPV0TUrFmztm3bPn/+PDExUfVqAOXQ19cnIqVX+/GrhLm5uQYGBs+ePcvPzxdycADVDAIWgIaEhIQInq44PomFq4RQzSnWuTdp0oSwzh1qu2oUsGQy2aZNm7Zt21ZSUlLVYwEQ2O7du0eNGvXy5csZM2YIm64Iy7CghnjlRkJcJYTarbqswWKMSaXS6OhoIlq4cGGvXr14r+pu3boJ+1EEoHlRUVEuLi5yuXzu3LkbN24U/C398ccfSySS+Pj4goICNGuAaktx/yACFrwPqsUMllwu//zzz6OjoyUSSePGjV+8eBEeHr5kyRJbW9smTZr85z//2bFjR1paWlUPE0BJjx490tXVNTExUUe6IiITExNbW9vi4uJTp04JXhxAKJjBgvdK1QcsuVw+derU7du3GxgYhIaGpqampqenBwYGurm5NWvWLC0tzd/f39XVtXHjxq1bt54+fXpQUFBOTk5VjxqgEkaNGiWXy/Py8hQb3QgOy7Cg+kPAgvdKFQcsPnfF09X+/ft//PHHX375xdzc3NnZ2cfHJyUlJTk52cfHx9nZuW7dunfv3vX19f3ss8/q169va2u7ZMmS8PBwLNiC6s/Q0PCjjz4qLS3lF8HVwcHBgYjCwsLUVB9AdZaWliKRKDU1Fb1G4X1QlQGLpys/Pz+ertauXXvy5Mk1a9bk5eUpHtOqVSs3N7fAwMDMzMyYmJhvv/22Z8+eRJSUlLRmzRpHR8e6dev26tULc1pQzfEApL4Zpl69ehkZGV25cuXJkydqOgQAv8DNGFPu6bq6ug0aNCgpKeHtHhCwoHarsoD1Srpat25dZGRko0aNQkNDeZ/fV2hra/fp02fFihXx8fHPnz8PCwvz8PCwsbEpLCxMSEjYsGGD5l8CQMWp+xKejo5Onz59GGMRERFqOgSA6vjFQR7RELCgdquagPV6uoqIiGjUqFFERETHjh3f+XRDQ0OpVOrl5ZWYmDhx4kQiys7OVv+oAZRnZ2dnamp669at+/fvq+kQaNYA1RxjjIeqiIiIdu3aWVlZFRYWVvWgANSlCgKWIl0ZGhoqka4Udu3a5erqOmXKFMLSE6j2JBJJ//79iSgiIlJNh+ABKzQ0VE31AVSRmZk5dOjQjIwMIurSpcutW7fi4uL4tUKAWknTAUuodEVEv/76644dOwoKCvjEAGaboZobMWJV5855ERGT1VT/ww8/tLCwSE1NvXnzppoOAaCc6Ohoa2vr4ODg+vXrr1271tnZuapHBKB2Gg1Yr6SrtWvXKp2uqMwet/369SNcGYFqr2fPjlevGoWFiZRdIvwOIpHI2tqaiDZu3KiWAwBUnlwuX7NmjYODQ2pqat++fS9evLho0aKqHhSAJmguYAmbrqjMihMsPYEaoV07atGCMjLo8mW11M/JyUlKSiJs8QbVRnp6+uDBg5csWSKXy93d3cPDw/kuhADvAw0FrDemK0tLy1OnTimXrojoo48+qlOnzrVr1zp37kxE4eHhSt88DKAZAwYQEanju0BOTs6gQYPS09NNTU2XL18u/AEAKik8PLxr165hYWHm5uYhISGbNm3S1tau6kEBaI4mApZcLre2tvbz8zMyMjp06NCaNWt4uoqMjGzTpo3SZbW0tPr27UtEKSkpzZs3z8jIuHLlinCjBhCeVEqkhoDF09WZM2eaN2+elJRka2sr8AEAiIiINyksKCgo/2Eymez7778fNGjQkydPHBwcLl26NHDgQI0MEKAa0UTASklJuXz5MhHt2rXrr7/+ioyMVD1dcYqLgwMGDCA130v44MEDf3//rKws9R0Caj1HRxKLKSaGiooEq1k2XUVGRrZs2VKw0gBlREdHjxo1iohkMlk5D3vw4EG/fv2WL18uFos9PT1DQ0MbNWqkqTECVCdMIxo2bEhEZ86cKSgomDBhwp07dwQpe+3aNSJq1KjRzp07iWjw4MGClH3d3bt3+e3EEonExsbGw8MjLCysuLhYTYeDWqxLF0bEIiKEqfb8+fMePXoQUfPmze/evStMUYD/df/+fcV9f3Xr1t28efPbHnngwAFTU1MiatasWWxsrCYHCVDdaChgzZw5k4hWrVoleGW+p1VMTIxIJDIwMHj58qXgh7h3716LFi2ISEtLSyKRKLKpqanpp59+um3btr///lvwg0JtFRDA/PzY06cClEK6AnUrKCjw9PTkXy8NDAw8PT2Dg4N79+6dlpb2yiMLCwvd3d35uXHkyJFPBXmLA9RkGlrkrr4b/fjFwaSkJCsrqxcvXiQkJAhb/8GDBw4ODvfv3+/Zs+fTp0/L7tLz7Nmz/fv3z5gxo02bNo0bN3Z1dQ0KCnr69KmwA4Ba5uhRevCAzMxo82aKilK+Ttkrg1FRUbgyCMJijAUFBX3wwQfLly9/+fKls7NzSEjI5cuXhwwZEhsbu2nTprIPDgkJ6d69++bNm3V1db29veZ1SqkAACAASURBVA8cOGBmZlZVIweoLjST4549eyaRSHR0dPLz84Wt7O/vT0RDhw5duHAhES1btkzA4ikpKa1atSKinj17xsTEODo6ZmdnK/7r48ePAwMD3dzcGjdurPj7FIvFimuIhYWFAg4GaoeJE9mUKezOHbZpE4uMVLJI2bmre/fuCTg8AMZYYmJi7969+TmtW7duYWFhnp6eenp6RGRoaOjp6Vn25Hb79m1+e2D79u0vXLhQhcMGqFY0FLAYY927dyei4OBgYcumpaWJRCJDQ8OjR48SUffu3YWqXDZdRUREmJiYEJGnp+frjywtLU1MTPTy8nJwcODnIM7AwKBx48YHDx4UakhQC0ycyO7fZ2PGsE2bWHg4mzaN7djBXrveUh6kK1Cfx48fu7m58bUQFhYW27ZtCwgIaNasGRGJRCJnZ+eUlJRXnnLq1Cki0tbWLvv9EwA0F7CWLVtGRAsXLhS8Mu+DFRoaqqurK5FIBLn2n5KSwq+58LkrPt09ZsyYkpKS8p/44sULxTVEsVhMRCYmJqqPB2qNiRNZURHbtIk5OLBffmFE//yvVSvm5sYCA9nz5+U9HekK1KS4uNjb27tOnTo8Lbm7u0dFRfXq1Yt/XbS1tY2Li3vbE8VisZaWVmJiYufOnT/99FMNjxygetJcwIqMjCSiLl26CF553rx5RPTdd999/PHHRLR//34VCyrSVa9evSqVrl5x9epVHR0dkUiUnp6u4pCgFrh+nSUk/BOwZDJmbc327WMbN7KhQ5mR0f8nLR0dNm3anpUrV54+fVomk5WtgHQFanLkyJHWrVvzLOXk5BQfH+/m5sa/IjZu3NjHx6e0tLScp1tYWBBRREQEv1CosWEDVGeaC1hFRUWGhoYikej1209UdPz4cbFYPHny5FWrVhHR1KlTVakmSLpKT0/39fXdtWvXoEGDiGjPnj2qDAlqgevXWaNGrE4dFhnJ5HLGGHv2jL148c9/lclYYiLz8mJSKdPWZtbWA/hHnZGRkVQq9fLySkxMzM7O5tfZka5AQNHR0e3bt+fvt06dOgUHB3t7exsbGxORjo6Ou7t7bm7uO4vY2dkR0cmTJ/nSCA0MG6D601zAYowNHjyYiHbv3i1s2ZcvX2ZkZDDGtmzZYmBgIJFIWrVq5ebmFhgYWJFTQ1ll01V0dLTSc1e85amNjc26detUz3xQ0928ySwsGBFzdPz/UPU2z56xgwePzpo1q23btmXvR+HriFu3bv3gwQONjBreC/yMZ2xs7OXldeDAAb7wlM9jJScnV7DI6NGjiSgwMLBu3bpEhB4NAEzDAeunn34ios8//1wdxU+ePGlgYMC/dSk+k/T09BwcHFavXp2YmFj+FDd7LV3xdnnOzs6VTVeMscLCQn19fbFYzOfMmzVrpuzLghpPka4GDnx3unpFWlqa4k7VBg0aGBoaoukaCIu3Ety9e/f27dv5adPKyiqikp1w586dS0Tr16/nK2IvXryoptEC1CAaDViXLl0iIktLS8ErR0dHGxkZ8bmioqIifk+fVCotu7dovXr1nJ2dfXx83tiS8ZUrg6qkK463/goICODbRNy+fVu1lwiatXkzmzaNTZ7Mtm9XpYwq6aqs0tLSqKio1NRUVQYD8Dpra2siOn/+fFFRUffu3b29vV9Z+VcR/MvzvHnzhgwZQkRHjx5Vx1ABahaNBiy5XM7Txs2bNwUsWzZdvTJNlZeXp7inr+zVFsU1RD6Vff/+fWHTFWPMy8uLiNzc3MaPH09EW7ZsUfV1gsZcu8amT//nz+PGsQcP2Pr1bO5cdvQoy8urTJkbHTrIiNigQQw90aB6UgQsVYrs3buXiD799FM3Nzci+vXXX4UaHkDNpaFO7pxIJOKN1wVs6R4TEzN06ND8/PypU6f6+vry214Uyq4RTk5O3rZt25gxY8zMzO7evevr6/vZZ5+Zm5vb2NjY2treu3fP3t7ey8trxIgRz549c3Z23r17t5aWltID4zNYoaGh6utiD+py6RL9e3c6ffQRXb1Kfn60aRMNG0YmJmRrS0uWUHg4FReXU+P69esDBvSXy8ePHl106BCV6Y8GUNs0bdqUiB4+fKj4Q1WPCKDqaTRg0Wt75ty8eVOVauWnq1e0atVq+vTpQUFBmZmZZa8hnj9/3sDAoF27drt37+bpaty4cSqmKyKytrZu0KDB/fv3+VLlyMjI0tJSVQqC5jRoQGlp//w5LY3q1qUNG8jTk3r1IpGIkpJozRpydCRz83kuLlu2bLl9+/YrBW7duiWVStPT01u0yNm5U450BbUbAhbAG2h4xuzRo0dEVKdOneLi4mvXrhFRw4YN+dKohw8fVqpUOVcGK66goGDx4sVENGjQIMbY9u3bJ0yYoMqVwbI+++wzItq2bVu7du2I6PTp04KUBbUrKWEjRrCdO9kff7Bx45i/P9PSYjY2zMODHT3KgoOZhwezsSkqs/1fo0aN+Nv40aNHN27c4G2BBg4c+EKVhVcA6ifIJUKZTKalpSUWi//66y8i6tevn0CjA6jBNB2wGGO850poaGhISAhfksWJRKIuXbosXLgwJCSkoKCg/CLR0dGGhoZENG3aNKXTFffkyRO+2c7Lly9VqfM6X19fIhozZsysWbOI6IcffhC2PqhRURE7eZJFRzOZjC1fziSS/+8EamrKRo9mW7c+iY39448/xo8fb25urngbi8VifX19pCuoKQQJWIwxPnfFG0q3atVKkLEBqGjdunW2trZHjhypkqNXQcDq06cPEenq6vJv/PHx8T4+Ps7OznyLBk5LS0uxZXJxcfErFQRMV5yVlRURRUVFqV6qrPv37xORqanpvn378K2uZsvLY2FhfOJKkbR+7NHDwsKi7NvYwcFBJBIR0eDBg7HVN9QIQgUsvqlOeHi4SCTS1dWV83a6AFVq/vz5RLRu3boqObpKy4yU4+HhkZiYWFhYGBQUFBQUJBaLu3btKpVK9+7dKxaLY2JiwsLCkv61Zs0aMzOzAQMGLFiwoGfPnkQUExPzySefFBQUTJs2zcfHp/x1VxUklUovX74cHh7er18/1aspNG/evHXr1snJyWZmZlpaWgkJCfn5+fyyJtQwRkYklZJUSkSUkkJhYRQeHnj2bFpamuJtbG1tXVJSIhKJTE1NDx48qIeFV/A+4TNY6enp9erVy8rKysjIaNiwYVUPCt53VbsoUNOL3Ilo6NChBQUFt2/f5hNXhoaG58+fX7t27ZAhQ0aOHHnmzJnRo0dHRkaGhoby9grZ2dn79u3Lyckh9aQr+nfpPW+/LixHR0ciOn36tK2tbXFxMd92Hmq25s1p2jQKCLhw925ycrLibZyUlHT58mW5XN6kSROkK6gp+JwrY0zFOljnDtXQexewiEgkErVt25Z3osrOzub39Nnb2xcVFYWHhy9ZsqRv374TJky4e/eum5tbTEyMj49Pnz591JSuiKhfv366urqJiYnPnj0Tqibn4OBAROHh4WjWUCspGqqlp6eHhoYuXLhQW1v76tWrT548qeqhAWgUAhZUQ+9jwCpLsdwqNjY2PT197969X3zxRYsWLbKysoKCgqZPn963b9/169f37dvX0dGxoKBg+vTp7+zIUFkGBgY9e/bkzbIFLEtEDg4OEokkNja2d+/ehIBVe+nr6zs6Ov70008ODg6MMb7UF+D9wT/J7t+/j4AF1cf7HrDKql+//meffebr63vv3j3FxRczM7Pbt2+fP3++pKRk9OjRv/76K5/TFpaaZphMTU27detWWlqqp6dnZGR05cqVNEWDJaiNMFUJ76cTJ04QUVxcXN++fadMmdKhQ4eqHhEANWrUSEdHJyMjo6ioSPNHF6l+6V3dSktLz549u3r16rZt265bt07YuSuFM2fOfPTRR23btn29aaSKrl+/bmFhYWJi0q5du8zMTCMjo6FDh0qlUqlUyvfkgdrk0qVLXbt2tbS0xDd4qBFsbGzOnz+flJTUrVs35SoUFhbOnz/fx8eHiD755JPjx48LOkAAlbRo0SIlJSU5OblVq1YaPnQNCFiaUVpaam5unp2dfe/evRYtWghbnDH25Zdfbt261dDQsKCggP9QIpHY2dk5OjpKpdKePXuW3Zcaai7GWOPGjZ88eXLr1i3eYBagOlMxYN28eXPcuHGXLl3S09P77rvvli5dKvgIAVTRp0+f2NjYqKgoYbsEVET1ukRYhSQSSf/+/Yno5MmTwlZmjLm7u2/dulVXV3fPnj2KXXq0tLROnz79ww8/9OvXz8TExNHRcc2aNUlJSYi8NZpIJPr4449JPTelAggrKytLLpcr/XR/f39bW9tLly61b98+ISEB6QqqIb4Mi+8io2EIWP9PHatnGGNz5sz55ZdfdHV19+/fP2zYMEUD1ezs7LCwMN6KorCwkN8+aWtr26hRI74QDdeYaigsw4LqTyaT+fr6duzYMSMjQ4mn5+Xlubi4/Oc//ykoKHBxcUlMTOzatavggwRQXVWuc6+S9qbVE199Vb9+fUG6wzPG5HL5l19+SUS6urrHjh0r55FpaWk7duxwdXVt3Lhx2X+dzp07S6XSo0ePCjIe0Az+m1y3bl2hNrUEEFZISEjHjh35SaZVq1ZSqfT27dsVf/rVq1c7depERPr6+j4+PuobJ4Dqfv75ZyKaNWuW5g+NgPU/WrZsSf/uGpGTk6NK0lKkK319/bCwsIo/UXH7ZN26dfkZsEmTJjKZTOmRgObxDTcTEhKqeiAA/+P27dvOzs78xNK2bdvdu3f/9NNPlfrs8fPz47ttfvDBB1euXFHfUAEEcejQISIaNmyY5g+NgPU/pk2bRkRr167lf65fvz7fae7evXuVqqN0uiqruLg4JCSEx6zTp08rVwSqBP/XX7FiRVUPBOAf+fn5np6eurq6RGRoaOjp6Xnw4EF+H4ZIJLp69eo7Kzx//vyzzz7j4czFxaWgoEADwwZQUVJSEhF17dpV84dGwPofAQEBRCSVShljr9xx0LZt21mzZh04cODZs2flFxEkXSnMmjWLiFauXKliHdCkgwcPErb3huqhtLTUz8+P7wwoFotdXFzi4uKcnJz4ma1du3YVWYRw9uxZfpd7nTp19uzZo4FhAwiCrzKsV6+e5g+NgPU/Hj9+zE86I0aM+OOPP2JjYxXNThVJSyKRKBaqv3z58pUKwqYrxtiBAweIqH///qqXAo3JycnR1tbW0dHJy8ur6rHAe413+OPnru7du/Mba/g8lomJiZeX1+snsVfI5XJvb2/eR8bW1vbvv//WzMgBBCGXy/lFbc3PuSJgvcrGxqbsxBXfbC4gICAsLGzlypX9+vXT0dFR/FfeNdTb21sulzM1pCvG2LNnzyQSCT6qa5yePXsS0fHjx6t6IPCeevTokYuLC9/3okmTJtu3b9++fbu5ubliHis9Pf2dRW7cuPHhhx/yy4iLFi0qLi7WwMgBhNWmTRsiunXrloaPi4D1BmfPnuUTV2U7rSsmro4dOxYcHMzbK/CTV8eOHZl60hXXo0cPIjpx4oSANUHdvv32WyKaP39+VQ8E3kcymYw3TDYwMPD09AwJCenSpQs/lfXr1+/ixYsVrMNnv4yNjfFVAWou/iVh2bJlGv6GgIBVHplMpugLyifVOUNDQ6lU6uXl9ddff/n7++/YsUMul/PFUoKnK8bYN998Q0QLFiwQtiyoVXR0NG+0UdUDgffL9u3bp06dOmPGjLVr144dO/bMmTOKeSxLS0s/Pz8+3V5BvL37kiVL5HL548ePi4qK1DdyAMHl5uZOmjSJiBo0aEBEjRo18vT0zMzM1MzREbAqKi8v7+jRo3Pnzv3ggw/KXkO0tLScNGkS39lUHemKMRYVFdW+ZUuvsWMFrwzqk5aWpqOjU6dOnY4dO/IVe4WFhVU9KKio69evq+/LblZWVnJysjoqnz17dsqUKXK5PCsrKysry9fXV09Pj38n/OGHH168eFHZgh4eHkT0448/8nn0M2fOqGPYAOpw/vx5fp+skZHRlClTFJ/d+vr6X3zxRUXunFURApYynjx5EhgY6ObmxlvEKq4hqiNdMcZYUREzMmIiEUtLU0t9xhhj+PgX0NOnT62trYmo7Io9fX39gQMHrl279sKFC5WaRQBNSk1NdXBw4CdlwXv8lpaW/vbbb9ra2mKxeOLEiTk5OYKUffLkiYeHx8uXL729vY8cOaL4eVxcnFgsdnZ2vn//vnKVf/nlFyKaOXPmyJEjiWjfvn2CDBjgFWlpaXynOKG80rDt5cuXfn5+J0+edHZ2lkgk/Jxsb28fGBiovjaTCFgqkcvlly9f/vLLL7t37/7f//5XjUcaMoQRsV271FT+/Pnz5ubmOjo6Y8aM8fHxefjwoZoO9D549uyZra0tvwE+OTk5NDSUr9gTi/9/ZyqlW6yB+hQWFq5atcrIyIiv6eb/UkOHDr1586Yg9U+ePMnXgihYWFj8+eefqjQ0Li4u9vb25t3y1qxZ89tvv+3637OEigt7eZNGJyenOXPmENHGjRtVqQbwRidOnODfRbt27ap64nljwzY/Pz9+odDDw+PUqVMeHh4mJib8Ma1bt/by8npnAyYlIGDVEOvXMyI2ZYo6aj969Kh169Zlz/sikcjKymrhwoXBwcFoJ1gpZdNVamqqXC5v2bIlX7EXHh6+d+9eNzc3vvpYgd+pGhgYmJ2dXdXDf38dOXKE93nieeLChQsbN27kwUVbW9vd3V2V8++DBw9cXFx48aZNm/7xxx+nT5/u3bs3/0m3bt1OnTqlRNmwsDDFVQ+pVBoVFXX9+vUBAwacPXs2Li7u0aNHSg9YgTdp7NKly9q1a7ESFARXUlLy9ddf8y+fim817dq1++WXX5S7cf7MmTOKhm0BAQGKn584cULRIkBHR2fSpElRUVHe3t6Ks7GxsbGbm5tQ36Y4BKwa4tIlRsSaNBG8cFpaGt+VzMrK6vjx46/s0kNEWlpair5fuEm7fK+kK8bYjRs3FGcNImrYsOHEiRP//PPPU6dObdmyZdSoUYpvUfyvumfPno6Ojjdu3Kjql/IeuXHjxuDBg/k/QYcOHQ4fPrxixQpfX1/GWFZWlru7O7+gYGZm5u3tXdnv1rx/umIhlKenp+JavFwuDwwMbN68uSLVVXw689atW0OHDlV8Gh06dMjb27tOnTqff/75w4cPt23btn379ufPn1dqqG/EmzSamZnt2bOHiJydnVWvCcA9ePCAf82QSCTfffddZmZm2cRTp06dSiWesg3b7Ozs3rjMMTEx0cXFRUtLix/CxsZm+/btBw8elEql/CdisdjJyUmo1T4IWDWEXM4sLBgRE/SjV5GurK2to6KiQkND+c9LSkoUt0/y9ytnZGTEJ2OuXbsm4DBqh9fTFZeRkcFX7Ck+SstOXAUEBERGRr5yp+qHH35YhS/k/fH06VN3d3d+tuX5ac+ePfyfyczMTPEF+vz583379uX/NB07dgwJCalIcZ6fmjVrxr+aOzs7p6SkvP6wgoICLy8vfl3SwMDAw8Oj/C/uz5498/Dw4NdTeKfQoKAgxQz06NGjhV1QomjSGBYWRkQfffSRgMWhBklPTx8xYsTAgQOFmuM5fPgw7+BtaWkZExPDGAsICFA68WRkZAwZMoT/rrm7u5d/u2tqaqqnp2e9evX4ISwsLDw9PSMiIlxdXRUn4SZNmqxfv17F14iAVXNMnMiI2M8/C1XvlXRVr149PT29s2fPvvKw7Ozs/fv3z5gxg/dqU2jduvWkSZM8PT1rdgfU+Hi2cCFbsYJlZalSJjs7+43p6hU3btz4+eefhw8fXqdOnbITV717916+fHl8fPyKFSt4kC0pKVFlPFC+kpISHx+f+vXr879/Nze3yMhIRYrq2rVrdHT0K085cuQI3wyezzaVfxvguXPnevXqxR9sa2sbFxdX/ngePnxYtinoG5sp8B1vynYKjYuL4x8qRNS+fXs1tcrjv/iRkZH8s1Adh4Bq7vr16x06dODvT9XneIqLiz08PHi1YcOGZf177h00aBB/Mzdv3nzdunXR0dFubm483/PfSh8fnzfeCRsREdG4cWMiatCgQcUbthUWFvr5+XXq1InX19PTc3FxiYmJ8fLysrCw4GcGVdZHMgSsmuTPP5mBAVu+XJBir6Qr/kkzZMiQ8vfNePz4cWBgoIuLiyL7E9H06dMFGVIVuHqVjR3L8vPZjRtsyBCm7J19FUxXZZVtsaa403Dp0qWMMX5fcUJCgnKDgXcKDw9XLDZ3cHCIiYmp4HXAoqIib29vY2NjItLR0XF3d8/NzX3lMampqW5ubnxNSePGjX18fCp+jj59+nTZbW3KvgciIyMVnUL79+/Px8zn3kxNTb29vdWXyD/++GMiCgkJkUgkEokE0b/aUtO9ydu2beMpp0mTJr169VKcr7p16+bn51fZ1mj37t3jb3ItLS1PT8+yvx15eXk+Pj6KZYVlE0+TJk34D83NzT08PBS3YclkMk9PT/7L279//wqefsuSy+UnTpwYOHAgD3wikWjevHk5OTn8JT958qSyBctCwKo5CgtZbCzbsIEdOaJ0FODemK4++eSTd+5KplBaWnru3Dk7OzsisrOzU2UwVWnNGhYR8c+fp01jDx6wyi8zVyJdvSI3N/fIkSNz5sw5ffo0Y4zvB/DDDz8oUQrKd+fOHWdnZ36mbtOmzZ49exS34FV8JfvbIhSPX3xukscvJbowlN2YWSQSubi4XLx48dNPP+VjbtGixd69e318fHjXRD73lpGRoczfRYW5uroS0R9//ME/5B48eKDWw4ES+PIjR0dHmUwm4DXinJyc8ePH8/eei4tLfn7+hQsX4uLiXk88FbyjYt++fXzVafPmzd/2HVIul4eFhTk5OSkSj1QqPXDgwN69exVfP3R0dJydnQ8fPtynTx8ikkgknp6eKr7w27dvu7u76+np9ejRY/fu3fz7zPnz51WpiYBVc4SEsJkz2Z07bOtWtmyZ0mVUT1fcrVu3bt68KRaLDQwMKvvc6qJswPriCxYTw8RiZmPDPDxYWBirQGMw1dPV6w4ePEhE/fr1E6QacGlpaXPmzOELLPhi8xMnTiiuDkil0sp2HXzlIuCaNWvK3oSoYh9RvjSej9bgX56ensHBwVZWVvwoAwYMuHTpkipHqaBly5YR0fLly/nH2zsvd4KGlV1+tGPHDgsLi4onnnKcO3eOr+0zNjZW9P7o169f2cTD288qEk858+6FhYXu7u78waNHj67INxmeeAwNDfmz2rZt6+3tHRISMmrUKD5lxX9BLC0tX7+gr7Qff/yRiObMmePk5EREhw8fVqUaAlbNMXEiUzT4HziQKXVtWKh0xX9VNm7cyGN+hCKm1CyXL7OJE1lxMUtJYUOGsIMHmZ4eI/rnfwYGbPBg9tNPaZcvv3HuXR3pijH2/PlzLS0tbO8tLP69WSwWT506NS4ujp89+Vlb6W6iZZex8/odOnQIDg4WasyK+TYjI6Njx44p5t5at24dGBgo1FHeadu2bUT0xRdf8AGUvfUdqlxUVBSfTKpfv/7Bgwf5jA4PH5MnT75w4YISNfl8GL8UaGNjc+fOHf7zkpKSsouiPvjgg23btoWFhTk7O5e9L8/Pz++V68jXr1/nF+X19PS8vb0rNZisrKzVq1dbWlry+mZmZh4eHnFxcZMnT9bW1q5Xr56w+97s27ePiEaOHDlz5kwi+uWXX1SphoBVc4wezRQtqQYPZgcPskaNmLMz8/FhFfuy8vDhQ75eVbGqXbl0xRjz9/cnoqFDh3711VdE9PXXX1e2QhW7eJFt3coYYydPstmz2ZIljF9rf/GChYUxDw9mY8PEYp60eltaNmjQgPcFVXTEzs7O5l1VhE1XHJ8qwPa6AuJn59DQ0I0bN/IbY01MTDZs2KB655G8vDy+Yd/48ePV0RI6JCRkx44dPMobGxt7eXlpeML4+PHjRDR48OAFCxYQ0bp16zR5dHib0tJSLy8vPpfTt2/f+Ph4vmbDwsLi008/LT/xlCMzM5N3AHnb7XjPnz/39vbm3yuIqG7duu7u7rGxsR4eHqampvyHLVu29PLyevr0KWPMz8+Pz0J16NBB6TnX0tLSI0eOKG4t9PX1zczMJCJTU1PlCr7NmTNn+F8an8ry8PBQpRoCVs3x88/Mz48xxm7eZBMnskWL/n+uRSRiXbuyr75if/1V9Ja+oAKmK8ZYWlqaSCQyNDQ8duwYEXXv3l2VV6ZpFy+y+vWZSMQOHSrvYenpbPfugtmzy26IREQdO3Z0c3PjE4Ht27cXPF0xxr799ltSd1PHY8fYnDls7Vr2fjSS5f92jLGTJ0/yW/BUXL5a1vTp04lo27ZtQhV83cmTJydPnvz48WP1HeJtLl++TESdOnXasGEDEbm7u2t+DPCKJ0+eODo6KmJQUFAQDzfNmjXj13Dv3r37tsRTDsV8WL169cqf2X0l8fBbCw8fPrxp0ybF/eZGRkaKlopTp04VpGf1mTNnPv/8c17KwMCAiISd6X/8+DERmZub79ixg4gmTJigSjUErJqjpIStWsVmzWLz5/9zrTA5mfn4MGdnVqcOT1ryOnWM9PQUfUEV31qETVdc586d+ZSArq6uRCJ5569udcHTFREbMoRV+OUnJyfzFqyKE5a5uXnz5s3Vka4YY9HR0aTWbljHj7MFC1hxMYuLY6qdQWoKRcBijAm+0bIGAlYVevbsGZ88CwoKIqJRo0ZV9Yjed2FhYY0aNeJnoSNHjijWNo0cOfKV3SByc3N9fHz4t0GeeNzc3K5fv/56zbJdOnv06HH37t0KDoa37lS0S+zWrdu2bduOHz+uWKWura29c+dOAV72a9q2bUtEwrZlLi0t1dHREYlEf/31FxH16dNHlWoIWLXCy5csIoItXXp/3DjFNpb8W4izs/OaNWt4/x4B0xVjbN68eUT03Xff8bu4a8YusEqlPYMGzQAAIABJREFUq7JKSkri4uIGDBhARJMnT7548eLixYsPHjwo7DCLioqMjIxEIpG6ZiymT2eKppeffKLcer6apWzAElztDliMMd4H9eTJk0Rka2tb1cN5f5WUlHh6evI7WB0cHE6dOsVXwZa/tqm0tLTsfXlisVgqlR45ckSxtDQ9PX3gwIGK+TAlrpunpaV5enryRb1E1KhRI09PT39//2nTpkVFRSn/gsvFz8OK/thC4a3kIyIiiKhFixaqlELAqm3y8vLCwsI8PDwU3UT4r1/Xrl1jYmJ459yRI0dWtnnJ6/jFwV69eq1atYqIZs6cKcj41UjldKVw+vRpImrXrp2Pjw+pZ/+QTz75hIjU9M0PAUtYtT5g8fnvI0eOEFHDhg2rejjvqZSUFH7jKu8gtX37dr62qX379hVczH7r1i13d3d+ZY2fwdavX+/v76+YD6vgLgVv8+LFC19fX8XNueqe7PzPf/5DRP/973+FLctvFAgNDRWJRNra2qr0GkXAqs2Sk5O3bdvGr6n7+vo+efLkgw8+GDp0qCCLZPPz83V1dbW0tHjSb9Omjeo132bDhg116tSxsbHx9fWt+Nx1WRcvXvxmyBC5SMSGDWMqh0uZTMaj6qlTp/hMoYoNf1/H17tMnjxZyKJxcWz/fsYYO3aMLV7MSktZYuJ7eIlQcLU7YJ05c4bPi/v4+Kxbt27v3r1VPaL30YkTJ/h6phYtWpw8eXLSpEn8Le3i4lLZRUiZmZkrV67krc8VBg4cmJ6eLshQeSOroUOHhoeHC1Lwbb755hsi+v7774UtO2HCBCLy9/fnuyaochkBAav2W79+PRFNmTKFMfbkyRMBN2zmW4scOHCApw3los87BQQEaGlpKW6KoX938QsMDKzgwq+LFy/yuevfZ89WPV1xo0ePJqLff/+ddz9KTEwUpKwCX1ncRMDtvePimLEx09ZmvFfNgQNs9my2ciV7P5pBIGApobS0dPXq1fxXr0GDBqq3VgKlXblyRV9ff+TIkZGRkXztkb6+Pt+SXDnFxcW7du2qX7++WCweO3as4F8RNYBfQJg6daqwZT08PIho1apV/D7xM2fOKF1KTFDb8Rs9QkNDiahhw4ZlN29WkYODAxFFRETwZVh8iYawgoKCJk2aJJPJxo4du3nz5jFjxpiZmd29e9fX1/ezzz4zNzf/6KOPvvnmm+jo6OLi4jdWuHTpklQqzcrKGjJkyKSffqJ/93lQEf9bDQ8P5/fy8K1wBdS5c2cLC4vU1NQbN24IUC4+noYMobw8Gj2a7t6lAwdo1Cj6+WdatoyMjASoD7VOZmamk5PT0qVLS0tL3d3dHz58qGjeDRqTkpIyduzYGTNmbN68+fTp06NGjfrkk0/u3LnTqVOnc+fOffHFF0pX1tbWnjBhQmZmZlpaWkBAAF/XVbPw+7sfPnyoprIC1Bcw90H1JJfL+daVwt5twRiLj48nog4dOmzdupWIxo4dK2z9wMBA/u25bDOS0tJSxS5+enp6ineygYGBVCr18vJKTExULN5UzF29c5vFyrp9+zYR1a9fPzAwkIgcHBwELM7xqwCbN29WtVBc3D/3mY4dy3buZFpaTFubvelOolpMrae72jeDpdzuuSA4V1fXW7duMcb4qtlx48YRkYuLiyAtD2q6K1euEFHHjh2FLXv48GEicnJymjNnDhFt3LhR6VIIWO+FiRMnEtHPP/8sbFmZTMYbWKtjKdIb09UrCgoKgoODFyxYYGVlxW+Q4SwsLFxcXNzc3Pi1S8HTFcdvNomOjpZIJHp6em/c5l0V27dvJ6Lhw4erVKVsutqzh2lpMSIm9KqF6o+/PdS0G+6MGTOIaCtvXVvDqb57LqguLy+P7/kolUrL/vzZs2cqbt5Smzx//pyIjIyMhC17/vx5IurSpcvatWtJtX6ECFjvhT///JOIRowYIXjlESNGENEff/zBO0EkJSUJUrZk715py5ZE5OnpWcGnZGRkBAYGurm5KVoME5GWltbw4cNVv2XyjaZOnUpEa9eu5V22Bbxb+OLFi4yx1NRUIjI2NlZ62dy5+PjSZs0YEZs0iQUEvLfpiiFgVczDhw8F3D0XlHP16tVOnTpZWVkVFhYOHz48u/I70L8/+N7qFdnZsOJ4j3gzM7M9e/aQajeJ17zLrqAEvk4oIiKipKRE2Mp8KdLJkycVa5IEKBoQoDVxYkhR0bbVq7///vsKPkmxm01KSsq1a9d++umnFi1aDBo0KCgoSEegdVeveGUZljCvnWjfvn12dnYLFixo3LixpaVlQUHBqFGj/P39eYvhiouLixswaNAEff2iadPIyYkmTSKZjL7/njw9BRkn1DJHjx7t2rXrqVOnGjZsGBwc/P3335dtqgeasWXLFltb22vXrhHRw4cPFyxYMHXq1J07d65evbqqh1Yd8T0KhV2GVbduXV1d3ezsbL68BGuw4N06dOhARPHx8cKWvXnzJhE1bNgwICCAiBwdHVWtGBhYUyZaMjMzxWKxnp4e36zN2tpa9ZpBQUH8LoTFixfzUyq/CMspbp98/vx5+XXi4uKMjY2JaOzYscf27CnlDcCq/V+p+mAGqxxl21c6OjoKuIkQVFxOTs7YsWP5b7qLi0twcHDjxo0PHjyYlZUVFxeXlpZW1QOsjgYNGkSCbtuqaDa2bt26W7du8byldDUErPcFX6+3fPlywSsvW7bs6NGjGRkZYrFYS0tr1apVSUlJSi7GqjnpirO2tiai4OBgAwMDkUiUkZGhSrWgoCDFsrN169YRkVgsXr169aZNm4YNG8YDE6etrd23b98VK1bEx8e/fh2nbLrauXOnRCIZ27598apVqoytpkPAepv79+/zzcV5+8qaeLt+LXD27Fne8KVOnTo7d+5ctmwZz7tjxoyp6qFVa9OmTSPh7i/Zt28f/0LbvHnziIiIUaNG8ZNwxbfKfgUC1vvi0KFDpPLOSuX46quviIh3Fiai+vXr8wt29+7dq2iJmpauGGOLFi0ioqVLl/KNJgICApQu9Xq6kkgk27dvVzxAJpMpbp8se9HT0NBQcfsk+990tXv3bl5T8F58NQ4C1hvt379f8Yki+PQ2VATfBJD/Rtva2sbGxmIZXMUtX76ciJYtW6ZincLCQsWWjiNGjIiKimrfvj0R6erqLly4UOmyCFjvi9zcXG1tbS0trZycHMGLL126lIh0dHQWLVr0xRdf8NvrFDxGjWKzZrGDB1k5F7ZKS5m9fc1KV4wxviGonZ0dv99k2rRpytV5Y7ry8/N72+NzcnIOHTo0e/ZsfhZQaNKkCb/COGbMmD179iBdKZQNWOfPnxe2eE0MWC9evOjWrRt/2zg7O7/zojOoQ0ZGBt8Ui28CuH//fr5XrKWlZXR0dFWPrgb4448/iMjV1VWVIjdv3uzatSuPU97e3tu3b+e7CX3wwQdXrlxRpTIC1nuEX1o+duyYsGUV6erQoUOKHyYnJ/v4+Dg7O5uZmQX07MmIGBGTSJiNDfPwYGFh/+wGuH07mzGDffste/6cPX/Ofv9d2LGpW2Fhob6+vlgs5vsFNW/eXIkilU1Xr0hLS+O3T5btA9mgQQOkq7IUAYtvqOfk5JScnCxUcR6wrK2tPTw8cnNzhSqrVjt27OCf6+VsEgxqlZ2drWg2dvjwYQ8PD/4uxTK4ivv111/51e39+/crN9vn5+fH9zJv167dqVOnyi6DU73ZGALWe+S7774jonnz5glY843pqiyZTFZ85gz74QfWrx/T0fknaRGxzp3Zrl3sxx8ZY+ziRTZunICj0iTezj4wMNDc3Lxhw4aV3c+rbLri02CVSldlyeXyCxcuLF26VNGUWR1L7moo/heyd+9eX19ffiFbT09v2bJl+fn5KlbmW3zysEJETZs23b17t5quRQooKiqKj/bp06cnT548ffp0VY/oPVJaWnrr1v+1d+9xMab//8Cv6VzoICWlpJJDqa0hqd2NlXMWqQhj2aVWyOETWWGyPCi7SI5hpZwqaVckJKUWRdnFOuRQiuQQOohOM/fvj+u792+2lA530ng9H/7ovueea66ZNPOa677u95X1+PFjb29vBweHtLQ0TINrmqysLLbctKGh4a+//trwkg1v37718PCg93VxcTl37hw7De7w4cOcdA8B6zOSkpJCCDEzM+OqwQ+mq5rKypiEBMbXl+HzmVmzmGnTGHZW+MiRzCf/mfRea9euJYR4enrm5eU19mOVw3QliZ50mDBhQjPbkSbs8N64ceMuXrzo4eFBY6iurm5ISEjTPtIqKip++eUXWomnb9++ycnJAwcOpI/Sv3//T3xKU0lJCSGkffv2R48eJS1TJA/eq7q6evz48evWrVu8eHFiYuKJEyfofyEjI6PLly+3du/anoKCgg0bNtAvOfS/tIeHx82bN+u/182bN83NzQkhysrKmzZtkpwGd//+fa76hoD1GamqqqJ/yXTRVoFAsGTJkoSEhKaVIG90uqrVG+aHHxi2TvSIEU1p5BNARwJkZWXXr1//wb9qSZKl6jlMVwzDbNy4kRAyffr05jclNUpLS/39/en/f3l5eW9v7+TkZHrSnPw7ubhRDSYkJLDv6XTxXV1d3Z07d+7bt09HR4cOaAkEgk/56nr6atAlRK2trVu7O5+LU6dOrVmzht28d++eqqqqs7MzCoo2h0gkSkhIcHJyogPJPB7P0dExNjb2vV96w8LC6Cyr3r17nz9/XnIaHLdVqRGwPi9OTk6EkPDw8MLCQvZEkrKyMnsZWgO/yi9durRZ6Yo6eZJZsIB584aJj2c8PZveTquqqqpqL7FecufOnenlk3Sli7q0XLpiGOb69euEED09PU5akyZPnjzx8PCg9TM7deq0adOmiIgIWvqfx+O5uro+fPjwg41kZWWNHj2a/rpNTU1PnDiRlpZmY2ND99ja2iYnJwuFQnrmol27dkKh8N27dx/h2TWWmZkZIYROH9TS0mrt7ki5ioqK0NBQhmFCQ0MPHDggedO9e/dap0/SKCsry9vbm+Yn+uUnKChIciYALVtITzvQYmP03YDz2ckMAtbnhk7D0tTUPH36dHx8vK+vL5/Pl1xHXUtLa+LEiXv27Knnk4abdEWdPs0sXsxs3840dTWYT8H9+/dXrFghEAjootoUj8fr27fvwoULT548WXuiz/Tp0wkhQqGQ83TFtOTy3tIhMzPzyy+/pL8ma2vrM2fOsHlIRUVFKBTWNab7+vVrX19feipBXV09ICCA/b4rFoujoqIks9r58+ddXV3po5iYmERFRX3Ep9ggI0aMIIScOHFCUVGRx+NxvpgmsB4+fEhPH2/btu3SpUsLFy5s7R5JuaKioqCgIHbZNDU1NW9v79zcXHrrjBkzDh8+zK656eDgQM/qcA4B6/NCywrUGLhKTEyMjIz08PCg6wmy2LrhkmPXbLrCmqPvxV4+qaamxr6ScnJyfD7f19c3ISGBLixYXV0dExPTEumKost7BwcHc9usNImNje3WrRv9BTk5OV28eFEgENDzC127dg0LC5M8uSASicLCwrS1tQkhMjIyAoHgvVczvHnzpkZWi4+P79u3L32Ub7755tq1ax/xKX7ArFmzCCE7d+6kf/gYR5EkFou5+tA9evSohoYGIcTAwODMmTP37t3z8fH5/vvvPT09MemqRVVXV8fGxtI1zehfrpOTU0JCQl5e3tdff01avtgYAtbnRSwWh4aGuru7W1lZ0c8S9sTW5MmT9+7de+HCBba8AnvrlClT6N2RrhquoqLi/Pnzy5cvt7W1lVzTTUNDw9nZOTg4eOrUqS2UrhiG2bdvn7y8/E+zZnHesjQpKysLCAigZ3hVVFR8fX3j4uIsLCwIIdra2mzBhaSkJEtLS/rrGzRoEF2Kux55eXkCgYAer6+vv3fv3uDgYPoHJScnt3379pZ/Zg3y888/E0L8/Pzoh825c+dau0efisLCwu7du/N4PDMzs7rm8TSEZPnK8ePHJyUlmZiY9OjRo63U8pAaaWlpkyZNojUCadIihOjp6SUnJ7fo4yJgfb5evHhB6yfVqAtKB64iIiISEhJWr17t4OCwf/9+BumqGUpLSxMSEugJWcmXWkZGhr62nCvLzy9t145RVWWausjD5+PRo0fs2JWenl5oaOiOHTsOHjzI1IpKjYrCNWJZSkqKt7e3oqLitWvXYmNj165de+nSpRZ7Tg0SGhpKCJk2bRod72yJoN8WpaSk6OvrS/6d9u3bd8+ePY2dSHf79m0a1pWUlOh1aoqKioQQa2vr+idoQgspKCgQCoX0t2BsbPwRLkBBwAKGkTixRYeyKVlZWXpi6/jx47Nnz0a64sTDhw93795tY2Ojpqa2aNGiFnyk3r0ZQpgLF1rwIaRIeno6rUVECLGxsYmOjl6xYoXkyb4mTFSvrq7evn17p06d6F/Tjz/+eOfOnV9//XX16tX0v0HrVjw6e/YsIWTw4MH0u5PkpW2fJ7pqDR3nsLGxiY+PDwgIYMOWurq6t7d3A7NRWFgYLbfWs2fPP//8k52NJxAIMNetdb18+TI+Pv7j/OkhYMF/VFVVXbhwYdWqVV9++SW9zI3i8XgKCgrHjx9v7Q5Cg82bxxDCoNZog4lEot27d9OJVux/e4FAkM8WE2kSdmq8hobGixcvHB0dJd/cHz58ePfu3Wb3/f+UlZWtXLmSrkr5QVlZWYQQExOTbdu2EUI829qVvFu2bJk+fXozFzNhPXv2bPjw4eTfy/Ur/73spqKiIioqiq3oIS8v7+rqeqHu7y0lJSV0RJDGqeTkZDrFTU1NLTIykpOuQluBgAV1KikpiY2N9fb2VldXl5OTw6IrbcyxYwwhTIst7y2tioqKxo0bJysrq6mpyWF981u3btHR3yFDhkjuHzNmDK3L1czVAMVi8aFDh7p27UoavKZ7WVkZIURRUfHYsWOEkNGjRzenAx9TeXn5nDlz6FldOTm5+hNPQyQmJtILb7W0tOLj4997TEZGhkAgYOfx8Pn8sLCwyv9e/pyZmUmLorVv3z48PFxyPCw7O7s5PYS2CAELGqS0tLS1uwCNVFLCyMszcnJMCyzvLfWePHnSQsvdLFiwgH6El5aWlpWVff/993TKbefOnffs2dO0MxeZmZlfffUV/eC3trZOSUlp4B3p0sK0FJaFhUUTHvrju3PnDp3cpqCgoKenxw6029nZRUVFVTVy0mFVVZVQKKS/gsGDB39wtPLJkydCoZC+boSQLl26CIXCwsJChmHCwsKUlZUJIWZmZikpKbQKRo3xMPisIGABSC97e4YQBid2PyVv3rxZvnz57NmzV65caWVl5eHhkZiYyMYjKyur8+fPN7y1wsJCb29vepmqpqZmUFBQo645/+KLL9iApaGh0fhn8wGlpaWNTTz1k1ya9+rVq0VFRenp6XUlng/Ky8uzt7cnjV8E8N27d2FhYXStFToKOGbMGPpbmDNnTnx8PDsedvLkyWY8XWjbELAApJdQyBDCLFvW2v2A90hMTKQfyVpaWiEhIX/88Qd7Pa+Tk9MHzyhVVlYGBQXRcmtNPsk4ZswYQsgff/xBK19zO1AdExOjpqamrKy8evXq5q8D8/btW7bkgYuLC32yAQEBtLhRXFxcWFgYrU1PE49AIKh/etYff/xBa2fo6+unpqY2oUtisTg+Pn7EiBHs8iwBAQGNGg8D6YaABSC9Hj9m/vyTSU5mWqZOMTTTrVu36MRqQkjv3r1jY2PZulzKysq+vr51FUySXAnR0dGxUYtgSvLy8iKEbNmyxdTUlHBX+l8sFgcGBrLTlQgh7dq1mz17dpPbl1yaNygoiN2/ZMkSWlifnhsNCws7d+4cuyAdIcTe3r52Iavy8nJvb296zLffftvA4a563Lt3z9vb28bGZuHChXQ8bM2aNa17iSh8ChCwAKRXairj7s5ERTEzZjDNX9cIWkZsbCy7iEKNmvJ6eno1asrfuXOHrk1LCOnZs2dcXFxzHnrdunWEkCVLlvz0008eHh6cTMR+/vw5u3quh4dHZGTkBxNP/SSX5r1+/XqNW58+fRoQEKCnp0fb19bW9vX1TU1N9fb2poUSCCEmJibsgnRZWVlWVlZ02C8gIIDDmXZisfjp06f9+vVr+Bw4kG4IWADSy9mZoaeNKiqYESNauzdQp4qKiqCgoA4dOtC5297e3klJSXT1OkJI//79L168+OrVK3YlRA0NDcmVEJuM1hodMmQIVzkjKSmpxuq5oaGhP//886VLl+pKPPUoLi6eNGkSvYtAICgrK6vrSFpMYcCAAfRgBQUFV1fX06dPBwYGsoWsNDQ0zM3NaR9MTEwaWMwCoMkQsACk1/Dh//n5+HHGxYUJCWEePGi9PkGd8vPzPTw86AweXV3dnTt37tmzR0dHhw5WfXAlxMbKycmxtLSkzfbo0aMhiace1dXVtVfPFYlExsbGbOI5e/ZsUFAQuwSkqqqqt7d3XevKX7lyhd5XVVX10KFDDexGamqqq6sre2khn8/ft2/f77//zi5IRwhxdnZ+/fp1k58pQAMhYAFILy8vhs7zzc9nJk9mZs1iCPm/f0ZGjIcHExXFvHzZ2r2E/7hy5Qpb1rJfv35nzpxxcHCgmxwuF33o0CFVVVVCSOfOnemYEyGkY8eOS5Ysyc3NbWxrjx49ogsaysjI+Pr6Sl7JWCPx2NvbR0ZGSiYedgle9i60ojodq+vXr18TVqHOzs729fVl16Xo3r17QEDA/v37hwwZ4uvr29jWAJoGAQtAej19ykybxnh5MVOnMtnZzMOHTEgI4+rKaGqySUukpTXAxmbZsmXnzp0rLy9v7R4DwzCMWCwOCwujuYfH423dutXW1vbo0aOcNC65ArGzs/OrV69EIlFsbGw9iad+Z86c6dy5M81qp0+ffu8xDx48qJ14kpKSPDw86HpEhBArK6uQkJDs7OzRo0eTfytINec0aElJyebNm01MTGj7+vr6japhAdBMCFgAnx+RiLlyhVm3jvnmm5yvv2ZPnSgrKzs6OgYEBGRkZLRQmU1ouLKyMqFQqK2tzeGqtLdu3WJXIJa8HI/KzMyUTDzW1tYhISH1LMIoWaXT0dHxg/0sKSkJCQnp3bs3bb99+/YeHh7Jycl+fn5aWlp0J50Oz07haj6RSJSQkODk5OTj48NJgwANhIAF8FkrKys7deqUj4+PpaUle6kXIeTq1aut3TVgGIbhcG1gdgXiXr161XOqsaCgQCgU0jWq6biUr69v7ZJODx8+pNPwG1ulk0089P+bjIyMo6Pj0aNH9+3bR+tz6ujoPG6BwiKomwAfGY9hGAIAQMjz58/Pnj179uzZv//+OyMjg45MgBQoLS398ccfDx06RAgRCAQ7duxgL+irS0VFRWRk5IYNG65fv04IUVRUdHNz8/HxoQNgMTExM2fOfP36tYGBweHDh9lJY41y48aNzZs3Hzx4sLy8nBBibm7u6+trZGQ0YMAAOlkeoE1DwAIAkGZXr16dOHHi/fv3O3TosGPHjilTpjTq7omJiZs3b46LixOLxYSQQYMGtWvXLi4ujhDi4uKye/dudXX15nSvqKgoLCxsw4YNjx49mjdvXnBwcHNaA/h0IGABAEgnhmGCg4N9fX0rKiqsra0jIiJ69OjRtKays7N37doVEhJSVFSkra1dXFwcGBg4f/58rrpaWVkZHR09YMAAWpoBQAogYAEASKHi4uKZM2dGR0cTQgQCQUhIiLKycvPbHD9+fFJS0vz584OCgrjoJoDUkmvtDgAAAMcuXbo0ZcqUnJwcNTW1Xbt2ubm5NbPBd+/eRUdHl5WV0YBVVVXFST8BpBgCFgCAVFmwYEFwcDDDMHZ2docPHzYwMGh+m9XV1dOmTVNRUTl48CAh5NGjR81vE0C6IWABAEiVyMhIhmEcHBwSEhLk5eU5abNDhw5qamrFxcVqamoEAQugAXAZNgCAVKGVPOfMmcNVuqLYVZMJAhZAAyBgAQBIFTMzM0LIkydPuG2WBqw3b94oKSm9fPny7du33LYPIGUQsAAApApNQo8fP26hZvX09FqifQApg4AFACBVaBLi/Cwe22wLBTgAKYOABQAgVVo0YD1+/LiF2geQMghYAABS5eOMYCFgAdQPAQsAQKro6urKyso+efKkurqaw2YRsAAaBQELAECqyMvLd+7cWSQSPX36lMNm9fX1eTze48ePu3btShCwAD4EAQsAQNq0RAZSUlLS1NSsqKho164d540DSB8ELAAAadOi07AoBCyA+iFgAQBImxYNWMXFxe3bty8uLi4pKeG2fQBpgoAFACBtWvpCQtQaBfggBCwAAGnTQgFr9OjR/v7+tra2Xbp0IYT8888/3LYPIE0QsAAApE0LFVsfOXKkUCjU1tZOS0sjhGRkZHDbPoA0QcACAJA2kiNYYrE4JiamqqqKk5YjIiIsLCzKy8tVVFSmTp3KSZsAUgkBCwBA2ujo6MjLyz979qyysvLEiRMTJkwwMDDw9/cvLCxscpvl5eXz5893d3cvKSkZP358Xl6ehYUFh30GkDIIWAAA0kZWVrZdu3ZisXjjxo3y8vJmZmZPnz5dtWqVgYHBrFmzmjB36s6dOwMGDAgODlZSUgoKCoqJidHU1GyJngNIDR7DMK3dBwAA4JiZmdmtW7cIIZaWll5eXkZGRrt27YqJiRGJRIQQe3v7+fPnOzs7y8rKfrCp8PBwLy+vsrKyXr16RUREWFpatnjvAdo+BCwAACmUl5fn4+OTmppKF8zR1tb29PQcOXLksWPHQkJCioqKCCHGxsazZs3y9PRUV1d/byOlpaWzZ88+ePAgIUQgEOzYsYOWcQeAD0LAAgCQWpWVlceOHdu0adOlS5cIIQoKCmPHjvX09Hzw4MHGjRuzsrIIIR06dHB3d1+4cGGvXr0k73v16tVJkybdu3evQ4cO27dvx5R2gEZBwAIAkH6ZmZmbN28+fPghT/nLAAAUdUlEQVRwdXU1IYTP58+dO7ddu3bbtm07f/48IURWVnbu3LlBQUH0+F27dnl7e1dUVFhZWUVGRvbo0aM1ew/QBmGSOwCA9OPz+eHh4bm5uUKhsGPHjpmZmTNmzJg/f/6gQYOSk5M9PDwUFBSMjIwIIcXFxW5ubp6enhUVFQKB4MKFC0hXAE2AESwAgM9LeXl5VFTU+vXrb968SQhRUlJydXWdOXOmtbX1rVu3Jk2alJOTo6amtmvXLjc3t9buLEBbhYAFAPA5Yhjm1KlTQUFBCQkJDMPweLyOHTsWFRWJRCI7O7tDhw5169attfsI0IYhYAEAfNbu3bu3devW3bt3v3v3jsfjeXl5bdq0SV5evrX7BdC2IWABAADJyckJCgoaMGDA5MmTW7svANIAAQsAAACAY7iKEAAAAIBjCFgAAAAAHEPAAgAAAOAYAhYAAAAAxxCwAAAAADiGgAUAAADAMQQsAAAAAI4hYAEAAABwDAELAAAAgGMIWAAAAAAcQ8ACAAAA4BgCFgAAAADHELAAAAAAOIaABQAAAMAxBCwAAAAAjiFgAQAAAHAMAQsAAACAYwhYAAAAABxDwAIAAADgGAIWAAAAAMcQsAAAAAA4hoAFAAAAwDEELAAAAACOIWABAAAAcAwBCwAAAIBjCFgAAAAAHEPAAgAAAOAYAhYAAAAAxxCwAAAAADiGgAUAAADAMQQsAAAAAI4hYAEAAABwDAELAAAAgGMIWAAAAAAcQ8ACAAAA4BgCFgAAAADHELAAAAAAOIaABQAAAMAxBCwAAAAAjiFgAQAAAHAMAQsAAACAYwhYAAAAABxDwAIAAADgGAIWAAAAAMcQsAAAAAA4hoAFAAAAwDEELAAAAACOIWABAAAAcAwBCwAAAIBjCFgAAAAAHEPAAgAAAOAYAhYAAAAAxxCwAAAAADiGgAUAAADAMQQsAAAAAI4hYAEAAABwDAELAAAAgGMIWAAAAAAcQ8ACAAAA4BgCFgAAAADHELAAAAAAOIaABQAAAMAxBCwAAAAAjiFgAQAAAHAMAQsAAACAYwhYAAAAABxDwAIAAADgGAIWAAAAAMcQsAAAAAA4hoAFAAAAwDEELAAAAACOIWABAAAAcAwBCwAAAIBjCFgAAAAAHEPAAgAAAOAYAhYAAAAAxxCwAAAAADiGgAUAAADAMQQsaFmlpaWt3YUGaSv9BACANgEBC1rQ7du3e/To8e7du9buyAekp6dbWVlVVFS0dkcAAEBKIGB9ugoKCrrV4cqVK+xhv//+u7Gx8YgRIyTvu3fvXuN/8fl8d3f3S5cuSR6QkZExYcIEbW1tHo+npaU1YcKEzMxMetPs2bNrP2JJScnJkyfr6k91dfV7n4KPj8+4ceOUlZXpZlxcnKOjo7q6upycnJGRkZ+fX1lZGZcvWYOVlZVdv36d3ezfv7+CgsKWLVtapTMAACB95Fq7A1AnVVVVPz8/+vP+/fuvXr26adMmuqmvr88eFh4e/uTJk+zs7Js3b5qZmdGdxcXF2dnZ69evV1NTe/HiRVhYmIODw7lz57788ktCyIkTJyZMmNC9e3ehUGhoaJifnx8aGmpnZ5eZmWlubv7ixYvy8nKhUCjZGSUlpd69e7P9WbVqlaam5ty5c+mmrKxs7f5fu3YtPj4+KyuLbm7YsMHHx8fGxiYgIEBTU/P69evBwcH37t2Liori6hVruJSUlO+//76goIBuysjILFiwwN/f39vbW0FB4eP3BwAApA0DbcEPP/ygoaFRe39xcbGSktKKFSs0NTWXLVvG7t+4cSMh5MmTJ3Tz2bNnysrK3333HcMwb9++1dLS6t27d3FxMXu8SCQ6deoU/XnChAmmpqb198fMzGzUqFH1HzNr1iw7Ozv68927d+Xk5IYNG1ZVVcUecOPGjfz8/PobaSG7du3S0dGR3PP69WslJaXDhw+3Sn8AAEDKYASrbTt69Gh5efnUqVOfPXt24MCBNWvW8Hi82odpa2vr6+s/fPiQEBIXF/fixYtNmzapqqqyB8jIyAwfPpzbvsXFxXl5edGfDxw4UF1dHRgYKCf3///LmZubsz/n5uYKhcLU1NSKigoLCws/Pz97e3tCSEBAQGFh4RdffLFu3brKysqrV686OjquWrUqOjr69OnTbm5uGzZsEIvFGzduPHjw4NOnT/X19RcsWDB58mTa7J07d/z9/dPT00Ui0VdffbV27VodHR17e/tnz569fPmyX79+hJAlS5a4ubmpq6sPHDgwLi5u0qRJ3L4OAFDDuXPnzpw5Y29vP2bMGHbnwYMHc3Nzly1bVvv4ly9f/vLLL/RnDQ0NQ0NDR0dHTU1NQsipU6eSk5MJIbKysrq6un369HFwcJCRkSGE7N+///79+zWa8vf3v3z58u+//04327dvr6en5+joyJ4WyM/P37hx4/Tp0/v27Uv3iESi48eP//nnn6WlpZqamt9++62trS0hJCYm5tq1a7V7a2pqKhaL8/Pzly5dyu4sKCiIjo6+c+eOSCQyMjIaN26cqakpvSkrKys0NNTS0tLd3Z09Pj8/f8uWLd99913v3r0b+KrCJ6e1Ex40SF0jWMOGDbO2tmYY5vz584SQlJQUur/GCFZOTo6CgsIPP/zAMAz9m799+3Zdj0XPHt6VUFhYWOOYD45g3bt3jxBy+vRpuunk5KSgoCASid578IsXL/T09Lp3775nz57IyMjBgwfLy8unpqYyDOPl5aWqqmpubr5u3bpVq1YxDCMnJ6euri4QCAIDA2NjYxmGmTNnjry8/MqVK48dOyYQCHg8Hr1vbm5ux44de/bsuXfv3vDwcD6fP3Xq1KqqqpCQkK+//lpVVTUkJCQkJOSff/6h3fD19dXX16/nSQEAJxwcHAghvXr1ktzp7u5uaGj43uNpTurWrRufz+/bt6+ioqKamlpaWhrDMD/99BMhhM/n8/l8Q0NDQsjgwYMrKysZhhk6dKiKikq//xKLxTt27CCEmJub03t16NBBXl5+6dKl9A2KzkaNjo6mD/348eMvvviCEGJhYeHk5NSnTx9CyLx588Risb+/v62tra2trZWVFSHEyMiIbi5atMjV1dXExITt/4EDB1RUVDQ0NJycnMaNG6erqysrKysUCumtJ0+eJIQoKCjcunWLvUtGRgYh5NixYxy+7PCRIWC1De8NWM+fP5eTkwsMDGQYRiwWGxoaenp60ptowPLz8wsICJg7d662tra6uvrNmzcZhvHw8CCElJSUSDYlEokePHjw+vVrhmEmTJhQI4UvWbKkxkN/MGAlJiYSQtj3i4EDB9b11skwzJo1a2RkZNjMV1FRYWBgMGzYMIZhvLy8VFRUXr58yR4sJyc3bdo0drOgoEBWVnbp0qV0s6qqqmvXrpMmTWIYZtGiRYqKinl5efSmt2/fVldX0589PT1rnCJkGGbr1q0yMjLsMQDQEvLz82VkZJycnAghGRkZ7P4PBqyQkBC6mZubq6enZ2lpyfwbsNgvbzt37iSEBAcHMwwzdOjQfv361W6NBqzs7Gy6WV5evnz5ckLImjVrmFoBy97eXllZOS4ujr17WFiYUCgUi8V1dY9hGMmAdeXKFTk5uXHjxrHvupWVlfPnzyeEhIeHM/8GrE6dOg0ePJhtFgFLCuAqwjYsOjq6urq6X79+2dnZOTk533zzzZEjRyorK9kDjh8/HhUVtW3bNiMjo6tXr9LvXoqKioSQGpfvlZWVGRsbb926lW4aGhr+I2HRokWN7durV68IIexZSEVFxTdv3tR18PXr101MTHr16kU3FRQUhg8f/vfff9PNdu3adezYUfJ4yTn+N2/eFIlE169f9/T09PT0nDNnDiGEzqy/du2atbU1e7CysvJ7J+OzNDQ0xGIx7TkAtJDIyEhCyPbt23V0dA4ePNiEFgwMDFxcXG7cuPH27dsaN3l4eLRv3z49Pb3hrSkqKq5evXrs2LGBgYElJSWSN6Wmpl64cGHJkiWjRo1id06bNs3f3/+9kzHea926dR06dAgLC+vQoQPdIy8vv3Hjxr59+65Zs4Y9zM/PLykp6cCBAw3vOXziELDasMOHDxNChgwZQssx7N2799WrV/Hx8ewBp06dyszMdHd3v3XrlpKSEt3ZrVs3Qgidj1UXBQUFMwmdO3dubN/oXCuRSMQ+6MuXL+vKWC9fvlRTU5Pco6Gh8erVK7FY/MEHKiwsJIRoa2tr/GvKlCmurq6EkFevXtVotn5VVVWEEHl5+YbfBQAai04D0NfXd3Nzi4iIYN8lGqWwsFBJSYl+XZRUVlZWXl7Ovt01nJubW2lp6YULFyR3JiUlEULGjx/fhB5SYrH47NmzgwcPlpzzSgiRkZEZO3bs3bt3c3Nz6Z5BgwY5OzsvWrSIvqeBFEDAaqvy8/MvXLiwZs2aBxIsLCxqfwFav369WCxesWIF3XR0dCSEtHRxBDr/tKioiH1QhmGOHDny3oMNDQ3z8vIk41R2drahoSGdqVo/Outi/PjxARLoWQNDQ8OcnJyG97moqEhOTq5RmQwAGiUnJ+fy5csTJ04khEyZMqWgoODcuXMNvG9hYWF2dvbt27c3b94cGRk5atQodkw6Ozs7Ozs7PT3d3d29urqatk8IuXPnjr2E8PDwuhrv3r07qfXN89GjR+TfN5mmef36dUlJCf1aW0Pt77obNmx4+/bte6f5Q1uEgNVWRUREEEJmzJhhJGHq1KknTpxgYw2lp6e3bNmy0NBQOmxuaWnp5OS0detWySjWkLGiRjExMSGE0KnuhBAXFxdTU1MfH5/U1FT2mLt379IANGTIkGfPnrHvfbdu3Tp+/DgNgh9kYWGhpaW1adMmtg57WVkZPXEwZMiQrKys6Oho9mD2hIKmpmZRUVGN86R3797t0aNHw0f+AaCxIiIi5OTknJ2dCSE2NjampqYNP0vo5+dnbGzcp0+fxYsXu7i47N69m72pR48exsbGtra2f/311969e4cOHUr3t2/f3k5C165d62qc1kOuUTOZjq7VHidrONrCe6vr1X5EQ0PDZcuW/fbbbzXqQkMbhYDVVkVGRg4aNEhXV1dy55QpU6qqqtgrkFn/+9//TExM5syZQ4PUvn377OzsBAKBiYnJqFGjvv76627duikoKLDTlbKzs+upHd8QXbp0MTU1ZWdCKCkpxcbGdurUycHBwdLScuTIkWZmZr169dq3bx8hZNKkSW5ubj/88IOjo6Ozs7Otra2Jicnq1asb8kDKysqhoaFpaWl9+vSZPHnyyJEjdXR0Dh06RAjx9PQcOXLkxIkThwwZMnny5D59+ixYsIDey8XFRSQS2djYDBs2jEZVQkhaWhq9uAkAWkhkZKSsrOzw4cPpNX2FhYUxMTG1p1K9l5+fX0ZGRnZ29tu3bw8fPqyurs7edPny5bS0tC5duvTt23fGjBns/q5du/4i4ZtvvqmrcTqSpKOjI7mzU6dO5N9xrKbR0NCQl5d//Phx7ZvoycEaj7h48WJTU9O5c+dy/qUXPj7UwWob3NzcrK2t2c3KykoXFxdaKUqSrq7uzp07O3XqZG5uHhAQwM6pVFBQ2L9/f1JSUkFBgZ6enqamZlJSUlJSUkpKyqtXr7S0tGbNmjVq1Ch6Xu+7774bOHBg7ZYlNxcvXlxjSkFtEyZMiIqKCgwMpGNCPXv2/Oeff06cOJGRkfHmzZuhQ4f279//q6++IoTweLyIiIiEhISUlJSKiorx48e7ubnRb41jx45lJ79Ta9euHTBggOSe0aNH37lz58iRI8+ePbOysvL396cHyMnJxcXFnTlzJi0tjcfjjRs3ji26Y2VllZmZGRsbq6CgQKthZWdn//XXX4GBgfU/KQBostu3b1+7dm3GjBk9e/ake8rLy1etWhUbG9uQ+nMGBgZ8Pv+9N/H5fBkZmXXr1k2fPv348eOS5bUaKCUlRVZW1s7O7tmzZ+xOGxsbQkhaWlqPHj0a2yAlLy/P5/MvX77MMEyN0fH09HR1dfVevXrl5eWxO+maXUOHDm3a9H/4tLTyVYwgvR49eiQnJ5eUlNTaHWmQFStW9OnTR/LSawDg1sqVK5WUlIqKiiR3DhgwwMnJiWlMmQZJkmUaxGLxV199ZWRk9O7dO6bBZRoYhrl48aKSktLkyZOZ/5ZpePfunY6OjrGx8YsXLyRbqPFGUX+ZBjpOv23bNsm7JCYmysjI+Pr6Mv+Wafjrr7/YWydOnEjPKqJMQ5uGESxoKV27dp05c+b69esHDRrU2n35gLKysh07dmzbtg0TsABazpEjR0aNGlXjOpIpU6YsWrTo+fPnzW+fx+MFBQX1799/w4YNdOHUJ0+e1FhWlV1eYufOnR07dqyurr5x40ZMTIylpWXt5d6VlJT27ds3btw4KysrDw+Pnj17Pn369MyZMwYGBtu3b29gr6ZNm3by5Ml58+alp6ePHDlSUVExNTV1x44d/fr1W7ly5XvvEhQUdOrUKcmaO9AWIWBBC/r555+dnZ3fvXtHp3N+su7fv29nZ0eLOwBAS3j48KGKisr06dNr7J84cWJ4eHhaWpqBgcHr16/fe19FRUU+n6+lpVX7Jl1dXcnzhtbW1kuWLDl9+vS8efO6d+9eUFAQExMjefyUKVO0tLT4fD4thszj8bp27bp169bp06fTQSNlZWVzc3M2BQ4fPvzy5cvr16//7bffiouLdXR0LCws3NzcanTP3NxcslyfgYEBO7GMx+MdOnRoyJAhe/fujYmJEYlExsbGy5cvX7hwoYqKCiFETU2Nz+fTnykdHZ3AwMDdu3dLzjODNofHMExr9wEAAABAquAqQgAAAACOIWABAAAAcAwBCwAAAIBjCFgAAAAAHEPAAgAAAOAYAhYAAAAAxxCwAAAAADiGgAUAAADAMQQsAAAAAI4hYAEAAABwDAELAAAAgGMIWAAAAAAcQ8ACAAAA4BgCFgAAAADHELAAAAAAOIaABQAAAMAxBCwAAAAAjiFgAQAAAHAMAQsAAACAY/8Pv5KmMJtX8tEAAAL3elRYdHJka2l0UEtMIHJka2l0IDIwMjUuMDkuMwAAeJylkktMU0EUhudO20spfdE3UNqLLW1peZiCqAvtXAMaETe6MWrg+ljcBAhKgqjByELjTiBhQVADiahBIz6AGCKWXtHEV2xEWYgJQVjgI0qMgAQBvT0VojXRBZNM/u+c/OfMmclMBq+PIHFpxY1RdGWL2y/uOopGnKiURI48okowHVXpckyiMU34iEqo3yDGCZ3wsq6gw38r/uVYIfxqHPMMcVGVxF4yqmJdjP/PK1F/1y0dmIAoRGEGSzwYSxmpjMcymqPjeBwnZ+TxKF6BFAkoQckoVTxWqTm1hscaLaNNRIk6pNPzWG/gDEYeG02Mycxjs4WzJPE4KZlLThGTVs6aKvptnM0uNmQ4Jo1JW+XBaQ7G4UTOdJTuQi43cns82J3BZHiRy4d8mYjJ4nGWlLMruVQdl2LiMh1ok0yclpZmMXG0jFapNTa7ktYbjNZUHW22JCWnmGhnusuX6TD7xR9GoaVf1lrmY99fPReKBMENBja8rh64QqFmv3axwAtSOzvf/b0/wpbeTyQY9kB+duApye0oB87nL5DFBzuBm6hG0qH+DP72bYUksLkP+MA+ijTNp4Gn9oouUGqJB1ZK5oKvvhQDP5v81t+qygP2PjSEPrZ2Qm1n7eEQvnEMuKarJZRfiMGzXvM4RN0fhPyb6qFQ+ZqiYITD7yhhXFWwMcKMVS5oF14EIkwV2YV7YxSJcGPBaoFJPwr5qqotwtzwdeCBS9sF97Vc8LS/PCjsrS8GznteLUz0yIHHWyqFqdEc8OOKXQJpa4azSKJX0FychhlKx3XCnv0lMNtQ8dnQKC6FmV1jgZBxoga4e+Y2OXGnGDy6hsvEF3BBzzb+EPkxcQrYf/oROTk4DDw1OkNMig5gRZmSrSuxwDy9a2nWa60G3uo3s+Hj54FnF7Vs++snwF4um+2RNwDf3R1gK6dvAd/Uj5AcdAa4wdZHPriKgI/seEs8zmaYzfQTsyvhauQ54xQAAAQQelRYdE1PTCByZGtpdCAyMDI1LjA5LjMAAHicfVZbbiM3EPz3KXgBE+wH+/G5thdBEKwMJE7ukP/cH6mmbI4WISJ5GtagxClWFbv11Or1+9tvf//T9ovfnp5aG//zl5ntLxljPP1o9U97+f7Lr7f2+vHt5evO6/uft48/mlqbo633z9hvH+8/vu5Qe200upO4z/asPUyNpI0+1uv6Lrf3lp1tDtcCqk8LPQAFS0YPrAcGz9IHm404AHWtmBJO0Z6pp9WzD8CJFb0rRyg4cjdxi3EAGoDWxdyc69GWHuwHoAOoPSnSqYAyIiDsf4GxgFNIVYpjzAzmAzCxGewVj1MuoPqIPMlDMKgxVKFpWbvB4kNP26YyZ/TpHBaFJGU6ekNlzvPoGiFDa0M8mO0kEZU9Jbb4GEvNIBE5Pl8Lyl1Yp2h9SUx5zhO0LMJj3YVlFpX0KUfpqUyqoInq8Fp/yFQ/SuW1LesckGhFBAGpVB2gZRSg5gPpKAKUrnaylLKg0YlSpHJJSTpPSB535BwWZq0eP6ac9s9l1TPCPOc0X/am2mlPCE/pT32QuZZDaSl0UorvVnGvTSMh0H8GvDhB9RMak7EeCMSAFSdXeVlFisyDAY4r9m88T1nhZRVN+M6B/COBYjRO8Wf/XLV8p9q4hePkn6DLKpI+0RhWnDlj5KlD8LIK/ck4Ef0VcU8/BlvG3QKKMXHGAMWpsTgtK3SPoCpIZuVW0VmOEgjfz0BmkEdBwwUcTtCya/aZbLSOgGean9wSXUgec0QZb5o6Tl4ha9VV0GklKn9DQuRI09oNnQ+CMpJM3UdOPZL01UudNKEiEsXD+LhirDbuDovKeUGg7ChmLqBEGFoP9IGbekq+lkEJ0dPd0VSr4+Zp14h6DQYQY+wB/VxyHB+t1faQjsATYQ769CAc/xOyvMGhy5k2qVwyYjrtW3XNkJTMIViSCZPsuORcJoYjEFSnaAbEPwFtSY4eL+h9aGJUUp2W/H57+2k438f1y/vt7RrX9eZrJuNDk2vy4kPTa76CV5vXFFVcds1KxeXXRFRccc09wpXXdKO6HocYrUIPw4pW2eSo2FXZ/HDYtQptiugUWoU2SyqaKLSJQjRaZXNFt6RVNl0qvlU2Y3R5rcKP3Vyr8OaM/qhV+BJ0KcpovQ9NWKvw5szFGYU3Zy7OKLw5c6mLwpszl8AovDmjD+p61ubMxRlFHpua1t5lc5biXIHbnNGiFF7JFQMp4wVt5qHhcBXZlNFZaN3ZlKVkRpFNWUpmFNmUpWRGkU1ZKhko+njMuYpuyjW61p1NWUtmFN2cVT7XudJb8UXRzVlLZuxLN2ctmet37r5RKstj5rRUXnnYd5bK8eDxrDRj9UuMOomP564+f/3Exv9P/wL81itnql/SqgAAAfp6VFh0U01JTEVTIHJka2l0IDIwMjUuMDkuMwAAeJwlUkuu4zAMu8osXwHH0P+DYFbZTw+Ra7zDD5UUaGvQlETSur7Xz9/v5+b7vn++1/W95T3dOv/Hbfd87HPfiq987p+n4J9c1/Uev9fnunDBf35/mHay1jpsV1jwOntLOOUglh61ztqV6b4O3STxclorZR28O1C2ztwmVbYO2aE5VbE1MnKqorN0nbabqx9EqXoAVzabNuVdsk5MKMkHsXw4gpnsOY1BJlsnbU8pjADEJgw9B22rUtyit5AIFI02TeKhFatCAE4q5jrtNexh6c5UAUS701USGLJQs6eS1G2g2FKKUaiEX4QxUCSFTSV3WkDsUZu51dZAbP5CTlG5ppAc+g+k5w5LY63tkcqbGFlBQ7TyDGTZmGMoUy/xFyloxy9B8QCGHEFWDIuX4vAlBQ7rvNNQxhUaQwK9seh2vCIg6aKugXAtLePEsjMejVzkPBCijqonFrM5ITMrezKQ3V38QJVqYPn2lnh6ZXekDyTk1CPC2vAieGZsljbMkda8DOKNgDzeST2dE2tpPYAKoR2WMDN47Cv85wBaFTBf8GGzk2bYs4XHq+4eAIWIBYuojYU+YbO6HAwnJrhE7O1YSGgOFh4ZrSi1LVwzw+EKviZhr9CRRcb6LD7P9M/vf6oXuzUd22zMAAADv3pUWHRyZGtpdFBLTDEgcmRraXQgMjAyNS4wOS4zAAB4nJ3TbUxbVRgH8NPn3t4WChulb3QdcCh0PYUOKDDaUmZ7N5xo6geZEzTOQEwEI5Nh9AOTGRdj2KbiwphGZMTM6eIMH5CMRd3ouYma4Mt0IntxxizGwZSZMeeHZXHGe5+bJjMjTrzJyfn133Oec++TnMuToz8S9clRBxD9uUMd69TxvEEirepsECTSoc6CoAZMWyjqsyDe9Me/QCuSngEWKXabvdqe9LyUc9Nr/7HnliJaQFWIi7zg/0O6/H8ot9Q2Ltq+W9q4hINvW/TmhQbBQgzEAAxAYCCIVDQyMEpUMjEwmYk5g2RkkkwLA0sWg6xsmr2MwbLlZHkOybESay6DXBu12RnYHdThZOB0UVceA5ebulcwcHuIZyVZmU/yCxgUFNJCyoAW0SIvA28xLS5hUOIjvlVklZ+Bn1EWYBAopaVlDMqCNLiawepyUl5BKioZVIYYhKoYVFWT6hpSs4bBmlpaG2YQjpBIlETrGNTFGMTqGdSvJWaBeKzE4ySBElofpuuM6pdKgtkkGUVTrtXjdNhtosvtcUo+9ehAiRSJ1sXqw67f1ftiIOk78/DAcb7l08e49qO7aI43tr+Enn3IrIyf1nO7L185db4WPXIppFz7Rc+/nNigeK7qedeHG5TI7GhK889XQsqFQx70UX++MqroebDVrHzziJ7fKJvj3v16fnH4OP/IoefT2YM8EbNrd5m4bYO8ve/buOYM8yZ+KulOaN6Xu4nPJHvQT/xh4H8eHkRf+0vzJ2hlRwv/oucseuLiFt7gFWXNNmcV751zolnwYKrwV917Xx+bDDF9zdMn+uOhQzdw75HDeYkDQxbMH9zYGF/x/QnMr7d9Ht8ceRfd4ihPJV/Qz/rhEqT8J99Db79wLpWVfAUdv++z1NfVMbS/tZK/EQC0+TUfX+uw4De++tVTPOz/YFLzVqGDtzXNY0969w/x9vFy7HNnZB8/Y+lDV373Mb+652007Zrgeeun0fc+uZt7t86ibe/cxU3zU+iqrib+U2QI/X7YyPvbn0Vbz9SkTp7fg86WSLzzaAf6fmkhznfqZ43V3Zm42/QAOjjSn9jc1oteiB5J+JvXo8dnziWuJDPRxyIG+ZkXD+D7+6IueTh6GW3qK5Xb7tmG7u5LyC1np9BNMxvl07sq0MdmHpdtPQfRDTs75em3QlizuWGHPHe9Gd04v1seWyhAiwO75OHQc7h+pHi7/ObAFPbQ2b1N/u3Rl7G3zr8BGgErtzT2AtIAAAUSelRYdE1PTDEgcmRraXQgMjAyNS4wOS4zAAB4nH1Xy24jNxC8+yv4AybYT5KHHNb2ZhEEawOJs/+Qe/4fqSYlzmjDiWQ1pEFNT3VXP+inFK8/3n7/+5+0Xvz29JRS+Z+/3nv6IaWUp+8pvqSXr99+e0+vn19e7ldeP/56//wzOSd33IP3I/bL58f3+xVKr+nZcmcl8vQs2Yu1gltyGa/jXg5kzczchNOzZjEXaxukBLJlY6bO06d3sg1SA9lzYzIDknOzqlU2SAskUSYu7G06bUV2RH1AOSt15za94r1jWu9QeK0gQAjqAtoWAXBFpsokQBtov0eF+H06vYiKyj1Voq1MpxepIrrnX5GD6fUi/8R3UU2LTq8XotLQSrOz+wibVKvVHVLvyCqImrJfIodWkoWb4ztn6Sb7mPyOBNWeJLdL5FCKMxQ3lQQaag2KbZDtjiRqNTkiukL229MbHo+yybU70S4iLhNZqYJqapmtSfTWf5E0n14bAqqpZ0U8SN0GOUSizIFsA4nK2snJQ6OSRb15GU/vjXblxKERSpTMq6WKAnBHXjfAkAjCNEOTwGNlw3sHDIXKCAeEPauj+rbh1PFoa4SnJ8vUVdp2jLQZjaMgEURkvXLdxt0n0ogU7i1L8dp2DSdl5hL6dCVUh6JHZZchoYk07WKKFKDwiuw0F74pada8pygoqW2XTJGJjDJrkURUKPft0/VWR11q96lp7bYreLk1kSPxNNq9SdOtRuKzM5tQhTbBmXvdtrvUWxO7kLT4xlxiSG2gbQ4RBO8xmyEpdNCd9tIntBYqbunZMW+g/46AlklAUMMBxTwr6NOd/EozAwRXVsKrYPntvfKEcmshZuQCN2256k2s0ghDN27qDPB2Oemsv1pceWw846q6E1ZttInh4m3jiV849dEnrRqhREIA1OHeZ2glQbSWOuaU6oXPkApFb+ipsRrRexc+QykL+YUHz4Lq6rs8WQjloT4mXSSMvHTdFYqFTi2Xim1EY6KhYvpOUQuZMOgwECeyebvwGSoRhoShT0aj9Fq97/rUxswb7Vl8bFsDUnd5slCJUCZYSJgjYwjZhdchExqFMHRnVE0vvNYJrTgW+Thu1ApJd3PKQijCSor10YemFf53Q8VCKapZXFuT8KpN9l69TGgcd+ZU6Z37tvo9tCKLAxfdBlCXfQacJxTND+VRsqV77TuqX9/fHo6U85D58vH+dhwy483HSTLechwX8SPpcSbEBE92HPwYHz8Od4RPPQ5wjE87DmmETz8OYhSf83GLhqHTqYrD0CJHwQ2GFj9sKRpmUcRC5zBx0vn12y/HvRZImEWXgm+YxRgHDBpmkcaJhcPQ4k1BHIbPJxAeZjHnkVL8HWkdecXfYs7BHIYXcw7mYVZ+OTjHlcUZO5+HWZw5OMeVxZkj0zC8OHNwhpHzVuZ027nnLMXpMYwcV0Y98NikD8jgDyOLvwR/GPkp8xJRwMiKQiIKmFh5D8iIBUZWLBKxwMiKBQuNw+h5b1EYXfnXqJw4bCzP2ELxEyvmtGzi59gkZw46alyxNk4LhMPEenhARv3A6NJCgz+MLv4a9QOji78Gfxg7j3MOY4u/hQowtp5lwR/GFn8L/jD2E38L/jB2dOloUxtz8wEZ/GFs8bfgD2OLvwV/GFv8LfjD+HnGcRhf/D34w/h6lvNo91UrLrNGj05zvVXtitBtaiaLn/v0a6uSYr6dp1n8vv+7je9P/wIiL+vBj3WP+wAAAmF6VFh0U01JTEVTMSByZGtpdCAyMDI1LjA5LjMAAHicRVRLruMwDLvKLPsA17D+Moqucq13+KHkZAZoEZehJZGSen0vul7f6+fi13V9/3/k3/Mg+H7v5/P224ef6/Wg9frhS//8ecIR//l98ZQ05ZEz2NjGhyaReY6Ym919fNaMxCsePtWVuCiWhINN2io5Pu81PdciASd28IGMSDnAkuVREM2g2Dp0KplJI6ZbbDCS2VIgPMMsA8gSid0IfiMkAqrwrmuCLMiLHKKxzRpyFEkDIVNSC1KcKHhUTN7RiLuQDhyYF5S8baZvt4EDoVxtKBatGG+fviCw7wnb9vGOyYtodz48kQYsWbtZMjlzdfTEu6NmJRlKEAhMbltiucJNsIyjWBC21trNcvE2OMOQpuoUK45UoKCiiGpVABPRpkbQhOJYqZMOs2AFOF7qUssB8rVhSs4VDsNhE3t5shGGtIH0LAqhmdbm7ojmUDejvTUgxYFWKUOq8XZIMJc0OnJqGQIkUqT9j1ZBPsmqHqgIdVRIMcV1F0ezVQCh7dL5N7eNhC4x5q36veVkg1hjHzTX9jxOS3UGGbTm5yDLatr8RpCVMdRDJoRCz0ESjmMLbgRsDhRNuKXhN+KodSGyhkWPCMZVW/2yyo654FoNiBdDDwEl2iuah+SbaiI3dPDNCq1IhHVjOkauXF0C7FZecmhp+4ao/OaZD1Q3iyXn5h0ei3VYJ35VUaPQLN9818opNVxPraWny5dHEIxJeNX77LXztXNUxuRkSzkG44/BByZIyLpwdP4GStOn99PT+87GrP78/gUzdPrsL8lqagAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Breaking the Plateau: Usually, when you double the data, the loss jumps up initially because the model is confused by new information. Your model handled the new 5,000 reactions so well that the loss continued to drop steadily. This means its \"brain\" was already prepared for complex chemistry.\n",
        "\n",
        "Loss < 1.53: In molecular generation, every decimal point below 1.6 is a massive victory. A loss of 1.52 suggests the model isn't just guessing atoms; it is beginning to understand reaction centers—the specific places where bonds break and form.\n",
        "\n",
        "Blind Test Success (#5005): Since index 5005 was not in your first 5,000 rows, the model had never seen this specific reaction until the last 10 minutes. If the Visual Audit showed a valid molecule, your AI has successfully learned to \"reason\" about chemicals it has never encountered."
      ],
      "metadata": {
        "id": "7W9zO1Vci9R6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at your Visual Audit image for #5005. Check for these three \"Chemical IQ\" markers:\n",
        "\n",
        "The Br (Bromine) Factor: The reactant sequence starts with [Br]. Did the AI include Bromine in its prediction? If so, it has mastered Atom Conservation.\n",
        "\n",
        "Ring Stability: Does the predicted image show clean hexagonal or pentagonal rings? This proves the AI understands Aromaticity (the way stable molecules like Benzene are shaped).\n",
        "\n",
        "The \"Impossible\" Check: If the AI prediction image appeared, the molecule is \"chemically valid\" (meaning it doesn't have 10 bonds on a Carbon). This is the hardest part for an AI to learn!"
      ],
      "metadata": {
        "id": "jG35WGujjTZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('chemist_model_sprint.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2SowU-DKjWPM",
        "outputId": "93abd19e-594c-4c06-ae09-aae869483974"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f5bb7b46-5192-4e23-ab29-bb7cc8a19f0a\", \"chemist_model_sprint.pth\", 46742018)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "Ce3u8ZUlkThV"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}